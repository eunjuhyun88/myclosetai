#!/usr/bin/env python3
"""
ìƒì„¸ AI ëª¨ë¸ ê²€ì¦ ë° ë¬¸ì œ í•´ê²° ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

import torch
from pathlib import Path
import logging
import json
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def analyze_ootd_model():
    """OOTD ëª¨ë¸ í¬ê¸° ë¶ˆì¼ì¹˜ ë¬¸ì œ ë¶„ì„"""
    print("\nğŸ­ OOTD ëª¨ë¸ ë¶„ì„:")
    print("=" * 50)
    
    ootd_paths = [
        "ai_models/OOTD/diffusion_pytorch_model.bin",
        "ai_models/OOTD/diffusion_pytorch_model.safetensors",
        "ai_models/OOTD/pytorch_model.bin"
    ]
    
    for path in ootd_paths:
        full_path = Path(path)
        if full_path.exists():
            size_mb = full_path.stat().st_size / (1024 * 1024)
            print(f"ğŸ“ {path} ({size_mb:.1f}MB)")
            
            try:
                if path.endswith('.safetensors'):
                    from safetensors import safe_open
                    with safe_open(full_path, framework="pt", device="cpu") as f:
                        keys = f.keys()
                        print(f"   ğŸ”‘ í‚¤ ìˆ˜: {len(keys)}")
                        print(f"   ğŸ“Š ìƒ˜í”Œ í‚¤: {list(keys)[:3]}")
                else:
                    checkpoint = torch.load(full_path, map_location='cpu', weights_only=True)
                    if isinstance(checkpoint, dict):
                        print(f"   ğŸ”‘ í‚¤ ìˆ˜: {len(checkpoint)}")
                        print(f"   ğŸ“Š ìƒ˜í”Œ í‚¤: {list(checkpoint.keys())[:3]}")
                        
                        # í¬ê¸° ì •ë³´ ë¶„ì„
                        for key, tensor in list(checkpoint.items())[:5]:
                            if hasattr(tensor, 'shape'):
                                print(f"   ğŸ“ {key}: {tensor.shape}")
            except Exception as e:
                print(f"   âŒ ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
        else:
            print(f"âŒ {path} (ì—†ìŒ)")

def analyze_gmm_model():
    """GMM ëª¨ë¸ íƒ€ì… ë¶ˆì¼ì¹˜ ë¬¸ì œ ë¶„ì„"""
    print("\nğŸ¯ GMM ëª¨ë¸ ë¶„ì„:")
    print("=" * 50)
    
    gmm_path = "ai_models/GMM/gmm_final.pth"
    if Path(gmm_path).exists():
        size_mb = Path(gmm_path).stat().st_size / (1024 * 1024)
        print(f"ğŸ“ {gmm_path} ({size_mb:.1f}MB)")
        
        try:
            checkpoint = torch.load(gmm_path, map_location='cpu', weights_only=True)
            if isinstance(checkpoint, dict):
                print(f"ğŸ”‘ í‚¤ ìˆ˜: {len(checkpoint)}")
                
                # í…ì„œ íƒ€ì… ë¶„ì„
                for key, tensor in list(checkpoint.items())[:5]:
                    if hasattr(tensor, 'dtype'):
                        print(f"ğŸ“Š {key}: {tensor.dtype} - {tensor.shape}")
                        
                        # MPS í˜¸í™˜ì„± í™•ì¸
                        if tensor.dtype == torch.float32:
                            try:
                                mps_tensor = tensor.to('mps')
                                print(f"   âœ… MPS ë³€í™˜ ê°€ëŠ¥")
                            except Exception as e:
                                print(f"   âŒ MPS ë³€í™˜ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
    else:
        print(f"âŒ {gmm_path} (ì—†ìŒ)")

def check_model_compatibility():
    """ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬"""
    print("\nğŸ”§ ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬:")
    print("=" * 50)
    
    # PyTorch ë²„ì „ í™•ì¸
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    print(f"ğŸ“± CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    print(f"ğŸ MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}")
    
    if torch.backends.mps.is_available():
        print(f"ğŸ MPS ë””ë°”ì´ìŠ¤: {torch.device('mps')}")
        
        # MPS í…ì„œ í…ŒìŠ¤íŠ¸
        try:
            test_tensor = torch.randn(2, 3).to('mps')
            print(f"âœ… MPS í…ì„œ ìƒì„± ì„±ê³µ: {test_tensor.dtype}")
        except Exception as e:
            print(f"âŒ MPS í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")

def suggest_fixes():
    """ë¬¸ì œ í•´ê²° ë°©ì•ˆ ì œì‹œ"""
    print("\nğŸ’¡ ë¬¸ì œ í•´ê²° ë°©ì•ˆ:")
    print("=" * 50)
    
    print("1. ğŸ­ OOTD ëª¨ë¸ ë¬¸ì œ:")
    print("   - OOTD ì²´í¬í¬ì¸íŠ¸ì™€ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶ˆì¼ì¹˜")
    print("   - í•´ê²°ë°©ì•ˆ:")
    print("     a) ì˜¬ë°”ë¥¸ OOTD ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ")
    print("     b) ëª¨ë¸ ì•„í‚¤í…ì²˜ ìˆ˜ì •")
    print("     c) ë‹¤ë¥¸ ê°€ìƒ í”¼íŒ… ëª¨ë¸ ì‚¬ìš© (HR-VITON, VITON HD)")
    
    print("\n2. ğŸ¯ GMM ëª¨ë¸ ë¬¸ì œ:")
    print("   - MPS ë°±ì—”ë“œì™€ PyTorch í…ì„œ íƒ€ì… ë¶ˆì¼ì¹˜")
    print("   - í•´ê²°ë°©ì•ˆ:")
    print("     a) ëª¨ë¸ì„ CPUì—ì„œ ë¡œë“œ í›„ MPSë¡œ ë³€í™˜")
    print("     b) í…ì„œ íƒ€ì… ê°•ì œ ë³€í™˜")
    print("     c) MPS í˜¸í™˜ ëª¨ë¸ ì‚¬ìš©")
    
    print("\n3. ğŸŒ DPT ëª¨ë¸ ë¬¸ì œ:")
    print("   - Hugging Face ì—°ê²° ì‹¤íŒ¨")
    print("   - í•´ê²°ë°©ì•ˆ:")
    print("     a) ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
    print("     b) í”„ë¡ì‹œ ì„¤ì •")
    print("     c) ë¡œì»¬ ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©")

def generate_model_report():
    """ëª¨ë¸ ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\nğŸ“‹ ëª¨ë¸ ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±:")
    print("=" * 50)
    
    report = {
        "timestamp": str(datetime.now()),
        "pytorch_version": torch.__version__,
        "mps_available": torch.backends.mps.is_available(),
        "cuda_available": torch.cuda.is_available(),
        "model_status": {}
    }
    
    # ê° ìŠ¤í…ë³„ ëª¨ë¸ ìƒíƒœ í™•ì¸
    step_modules = [
        "step_01_human_parsing",
        "step_02_pose_estimation", 
        "step_03_cloth_segmentation",
        "step_04_geometric_matching",
        "step_05_cloth_warping",
        "step_06_virtual_fitting",
        "step_07_post_processing",
        "step_08_quality_assessment"
    ]
    
    for module in step_modules:
        try:
            module_path = f"app.ai_pipeline.steps.{module}"
            step_class = getattr(__import__(module_path, fromlist=[module.split('.')[-1]]), 
                               module.split('.')[-1].replace('step_', '').title().replace('_', '') + 'Step')
            
            step = step_class()
            if hasattr(step, '_load_ai_models_via_central_hub'):
                result = step._load_ai_models_via_central_hub()
                report["model_status"][module] = "success" if result else "failed"
            else:
                report["model_status"][module] = "no_load_method"
        except Exception as e:
            report["model_status"][module] = f"error: {str(e)[:100]}"
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_path = "model_status_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_path}")

if __name__ == "__main__":
    print("ğŸ” MyCloset AI ìƒì„¸ ëª¨ë¸ ê²€ì¦")
    print("=" * 60)
    
    check_model_compatibility()
    analyze_ootd_model()
    analyze_gmm_model()
    suggest_fixes()
    generate_model_report()
    
    print("\nâœ… ìƒì„¸ ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!") 