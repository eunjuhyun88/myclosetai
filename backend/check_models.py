#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ëª¨ë¸ ë¡œë”© ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ëª¨ë¸ì´ ì œëŒ€ë¡œ ë¡œë”©ë˜ëŠ”ì§€ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
"""

import sys
import os
from pathlib import Path
import logging
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
backend_dir = current_dir.parent.parent.parent  # backend/
sys.path.insert(0, str(backend_dir))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)

def check_model_loading():
    """ëª¨ë¸ ë¡œë”© ì²´í¬"""
    print("ğŸ” MyCloset AI ëª¨ë¸ ë¡œë”© ì²´í¬ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ModelLoader ì„í¬íŠ¸ ë° ì´ˆê¸°í™”
        print("ğŸ“¦ ModelLoader ì„í¬íŠ¸ ì¤‘...")
        from app.ai_pipeline.utils.model_loader import get_global_model_loader, initialize_global_model_loader
        
        print("ğŸ”„ ModelLoader ì´ˆê¸°í™” ì¤‘...")
        success = initialize_global_model_loader()
        
        if not success:
            print("âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
        
        loader = get_global_model_loader()
        print(f"âœ… ModelLoader ì´ˆê¸°í™” ì„±ê³µ")
        print(f"   ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {loader.model_cache_dir}")
        print(f"   ğŸ”§ ë””ë°”ì´ìŠ¤: {loader.device}")
        print(f"   ğŸ§  ë©”ëª¨ë¦¬: {loader.memory_gb:.1f}GB")
        print(f"   ğŸ¯ ìµœì†Œ ëª¨ë¸ í¬ê¸°: {loader.min_model_size_mb}MB")
        
        # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸
        print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
        available_models = loader.list_available_models()
        
        if not available_models:
            print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # í¬ê¸°ë³„ ë¶„ë¥˜
        gb_models = [m for m in available_models if m["size_mb"] > 1000]
        large_models = [m for m in available_models if 500 <= m["size_mb"] <= 1000] 
        medium_models = [m for m in available_models if 100 <= m["size_mb"] < 500]
        small_models = [m for m in available_models if 50 <= m["size_mb"] < 100]
        
        print(f"   ğŸ”¥ GBê¸‰ ëª¨ë¸(1GB+): {len(gb_models)}ê°œ")
        print(f"   ğŸ“¦ ëŒ€í˜• ëª¨ë¸(500MB-1GB): {len(large_models)}ê°œ")
        print(f"   ğŸ“ ì¤‘í˜• ëª¨ë¸(100-500MB): {len(medium_models)}ê°œ")
        print(f"   ğŸ“„ ì†Œí˜• ëª¨ë¸(50-100MB): {len(small_models)}ê°œ")
        print(f"   âœ… ì´ ëª¨ë¸: {len(available_models)}ê°œ")
        
        # ìƒìœ„ 5ê°œ ëª¨ë¸ ì¶œë ¥
        print(f"\nğŸ† ìƒìœ„ 5ê°œ ëª¨ë¸ (í¬ê¸°ìˆœ):")
        for i, model in enumerate(available_models[:5]):
            size_gb = model["size_mb"] / 1024 if model["size_mb"] > 1000 else None
            size_str = f"{size_gb:.1f}GB" if size_gb else f"{model['size_mb']:.0f}MB"
            status = "ğŸ”¥" if model["size_mb"] > 1000 else "ğŸ“¦" if model["size_mb"] > 500 else "ğŸ“"
            print(f"   {i+1}. {status} {model['name']}: {size_str} ({model['model_type']})")
        
        # 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸
        print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        metrics = loader.get_performance_metrics()
        
        if "error" not in metrics:
            print(f"   ğŸ“ ë“±ë¡ëœ ëª¨ë¸: {metrics['model_counts']['registered']}ê°œ")
            print(f"   ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥: {metrics['model_counts']['available']}ê°œ")
            print(f"   ğŸ”„ ë¡œë“œëœ ëª¨ë¸: {metrics['model_counts']['loaded']}ê°œ")
            print(f"   ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {metrics['memory_usage']['total_mb']:.1f}MB")
            print(f"   ğŸ¯ ê²€ì¦ë¥ : {metrics['performance_stats']['validation_rate']:.1%}")
            print(f"   ğŸš€ ìºì‹œ íˆíŠ¸ìœ¨: {metrics['performance_stats']['cache_hit_rate']:.1%}")
        
        # 3. ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (ìƒìœ„ 3ê°œ)
        print(f"\nğŸ§ª ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸:")
        test_models = available_models[:3]  # ìƒìœ„ 3ê°œë§Œ í…ŒìŠ¤íŠ¸
        
        loading_results = []
        for i, model in enumerate(test_models):
            model_name = model["name"]
            print(f"   {i+1}. í…ŒìŠ¤íŠ¸ ì¤‘: {model_name} ({model['size_mb']:.0f}MB)...")
            
            start_time = time.time()
            try:
                checkpoint = loader.load_model(model_name)
                load_time = time.time() - start_time
                
                if checkpoint is not None:
                    # ì²´í¬í¬ì¸íŠ¸ íƒ€ì… í™•ì¸
                    checkpoint_type = type(checkpoint).__name__
                    is_dict = isinstance(checkpoint, dict)
                    param_count = len(checkpoint) if is_dict else "N/A"
                    
                    print(f"      âœ… ì„±ê³µ: {load_time:.1f}ì´ˆ, íƒ€ì…: {checkpoint_type}, íŒŒë¼ë¯¸í„°: {param_count}")
                    loading_results.append((model_name, True, load_time, checkpoint_type))
                else:
                    print(f"      âŒ ì‹¤íŒ¨: None ë°˜í™˜")
                    loading_results.append((model_name, False, load_time, "None"))
                    
            except Exception as e:
                load_time = time.time() - start_time
                print(f"      âŒ ì˜¤ë¥˜: {str(e)[:50]}...")
                loading_results.append((model_name, False, load_time, "Error"))
        
        # 4. ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        successful_loads = [r for r in loading_results if r[1]]
        failed_loads = [r for r in loading_results if not r[1]]
        
        print(f"   âœ… ì„±ê³µ: {len(successful_loads)}/{len(loading_results)}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_loads)}/{len(loading_results)}ê°œ")
        
        if successful_loads:
            avg_load_time = sum(r[2] for r in successful_loads) / len(successful_loads)
            print(f"   â±ï¸ í‰ê·  ë¡œë”© ì‹œê°„: {avg_load_time:.1f}ì´ˆ")
        
        # 5. Stepë³„ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ”— Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸:")
        step_names = [
            "HumanParsingStep",
            "PoseEstimationStep", 
            "ClothSegmentationStep",
            "VirtualFittingStep"
        ]
        
        interface_results = []
        for step_name in step_names:
            try:
                interface = loader.create_step_interface(step_name)
                available_models_for_step = interface.list_available_models()
                
                print(f"   âœ… {step_name}: {len(available_models_for_step)}ê°œ ëª¨ë¸")
                interface_results.append((step_name, True, len(available_models_for_step)))
            except Exception as e:
                print(f"   âŒ {step_name}: ì˜¤ë¥˜ - {str(e)[:30]}...")
                interface_results.append((step_name, False, 0))
        
        # ìµœì¢… íŒì •
        success_rate = len(successful_loads) / len(loading_results) if loading_results else 0
        interface_success_rate = len([r for r in interface_results if r[1]]) / len(interface_results)
        
        print(f"\nğŸ¯ ìµœì¢… íŒì •:")
        print(f"   ğŸ“¦ ëª¨ë¸ ë¡œë”© ì„±ê³µë¥ : {success_rate:.1%}")
        print(f"   ğŸ”— ì¸í„°í˜ì´ìŠ¤ ì„±ê³µë¥ : {interface_success_rate:.1%}")
        
        overall_success = success_rate >= 0.5 and interface_success_rate >= 0.75
        
        if overall_success:
            print(f"   ğŸ‰ ì „ì²´ í‰ê°€: ì„±ê³µ! ëª¨ë¸ ë¡œë”©ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
            return True
        else:
            print(f"   âš ï¸ ì „ì²´ í‰ê°€: ë¶€ë¶„ ì‹¤íŒ¨. ì¼ë¶€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            return False
            
    except ImportError as e:
        print(f"âŒ ì„í¬íŠ¸ ì˜¤ë¥˜: {e}")
        print("   ğŸ’¡ conda í™˜ê²½ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:")
        print("   conda activate mycloset-ai-clean")
        return False
        
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        print("ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ:")
        traceback.print_exc()
        return False

def quick_model_count():
    """ë¹ ë¥¸ ëª¨ë¸ íŒŒì¼ ê°œìˆ˜ ì²´í¬"""
    print("ğŸ” ë¹ ë¥¸ ëª¨ë¸ íŒŒì¼ ê°œìˆ˜ ì²´í¬")
    print("=" * 40)
    
    # ê¸°ë³¸ AI ëª¨ë¸ ê²½ë¡œë“¤
    potential_paths = [
        Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models"),
        Path("./ai_models"),
        Path("../ai_models"),
        Path("backend/ai_models")
    ]
    
    ai_models_path = None
    for path in potential_paths:
        if path.exists():
            ai_models_path = path
            break
    
    if not ai_models_path:
        print("âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return
    
    print(f"ğŸ“ ê²€ìƒ‰ ê²½ë¡œ: {ai_models_path}")
    
    # ëª¨ë¸ íŒŒì¼ í™•ì¥ì
    extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt"]
    
    total_files = 0
    total_size_gb = 0
    large_files = []
    
    for ext in extensions:
        files = list(ai_models_path.rglob(f"*{ext}"))
        for file_path in files:
            if file_path.is_file():
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    if size_mb >= 50:  # 50MB ì´ìƒë§Œ
                        total_files += 1
                        total_size_gb += size_mb / 1024
                        
                        if size_mb > 500:  # 500MB ì´ìƒì€ ëŒ€í˜• íŒŒì¼
                            large_files.append((file_path.name, size_mb))
                except:
                    continue
    
    print(f"ğŸ“Š ì´ ëª¨ë¸ íŒŒì¼: {total_files}ê°œ (50MB ì´ìƒ)")
    print(f"ğŸ’¾ ì´ í¬ê¸°: {total_size_gb:.1f}GB")
    print(f"ğŸ”¥ ëŒ€í˜• íŒŒì¼: {len(large_files)}ê°œ (500MB+)")
    
    # ìƒìœ„ 5ê°œ ëŒ€í˜• íŒŒì¼
    if large_files:
        large_files.sort(key=lambda x: x[1], reverse=True)
        print(f"\nğŸ† ìƒìœ„ 5ê°œ ëŒ€í˜• íŒŒì¼:")
        for i, (name, size_mb) in enumerate(large_files[:5]):
            size_gb = size_mb / 1024 if size_mb > 1000 else None
            size_str = f"{size_gb:.1f}GB" if size_gb else f"{size_mb:.0f}MB"
            print(f"   {i+1}. {name}: {size_str}")

if __name__ == "__main__":
    print("ğŸš€ MyCloset AI ëª¨ë¸ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    
    # ì¸ìì— ë”°ë¼ ë‹¤ë¥¸ ì²´í¬ ì‹¤í–‰
    if len(sys.argv) > 1:
        if sys.argv[1] == "quick":
            quick_model_count()
        elif sys.argv[1] == "full":
            check_model_loading()
        else:
            print("â“ ì‚¬ìš©ë²•:")
            print("   python check_models.py quick  # ë¹ ë¥¸ íŒŒì¼ ê°œìˆ˜ ì²´í¬")
            print("   python check_models.py full   # ì „ì²´ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    else:
        # ê¸°ë³¸: ì „ì²´ ì²´í¬
        success = check_model_loading()
        sys.exit(0 if success else 1)