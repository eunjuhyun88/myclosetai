#!/usr/bin/env python3
"""
M3 Max ìµœì í™” Virtual Try-On ì—”ì§„
Apple Siliconì˜ Neural Engineê³¼ MPSë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ êµ¬í˜„
"""

import torch
import torch.nn as nn
import numpy as np
import cv2
from PIL import Image
import coremltools as ct
from typing import Dict, Tuple, Optional
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class M3OptimizedVirtualTryOn:
    """Apple M3 Maxì— ìµœì í™”ëœ Virtual Try-On ì—”ì§„"""
    
    def __init__(self):
        # MPS (Metal Performance Shaders) ë””ë°”ì´ìŠ¤ ì‚¬ìš©
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            logger.info("âœ… Apple M3 Max GPU (MPS) ì‚¬ìš©")
        else:
            self.device = torch.device("cpu")
            logger.info("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.setup_models()
        
    def setup_models(self):
        """ëª¨ë¸ ì„¤ì • ë° ìµœì í™”"""
        
        # 1. Human Parsing Network (ê²½ëŸ‰í™” ë²„ì „)
        self.human_parser = self.create_efficient_parser()
        
        # 2. Cloth Warping Network
        self.cloth_warper = self.create_warping_network()
        
        # 3. ëª¨ë¸ì„ MPSë¡œ ì´ë™
        self.human_parser = self.human_parser.to(self.device)
        self.cloth_warper = self.cloth_warper.to(self.device)
        
        # 4. ëª¨ë¸ ìµœì í™”
        self.optimize_for_m3()
        
    def create_efficient_parser(self):
        """M3ì— ìµœì í™”ëœ ê²½ëŸ‰ íŒŒì„œ"""
        
        class EfficientParser(nn.Module):
            def __init__(self):
                super().__init__()
                # MobileNetV3 ë°±ë³¸ ì‚¬ìš© (ê²½ëŸ‰í™”)
                self.backbone = nn.Sequential(
                    # Depthwise Separable Convolutions
                    self._conv_block(3, 16, stride=2),
                    self._inverted_residual(16, 24, stride=2),
                    self._inverted_residual(24, 32, stride=2),
                    self._inverted_residual(32, 64, stride=2),
                    self._inverted_residual(64, 96, stride=1),
                    self._inverted_residual(96, 160, stride=2),
                )
                
                # Decoder
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(160, 96, 4, stride=2, padding=1),
                    nn.BatchNorm2d(96),
                    nn.ReLU6(inplace=True),
                    nn.ConvTranspose2d(96, 64, 4, stride=2, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU6(inplace=True),
                    nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
                    nn.BatchNorm2d(32),
                    nn.ReLU6(inplace=True),
                    nn.ConvTranspose2d(32, 20, 4, stride=2, padding=1),  # 20 classes
                )
                
            def _conv_block(self, in_channels, out_channels, kernel_size=3, stride=1):
                return nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size, 
                             stride=stride, padding=kernel_size//2, bias=False),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU6(inplace=True)
                )
            
            def _inverted_residual(self, in_channels, out_channels, stride=1, expand_ratio=6):
                hidden_dim = in_channels * expand_ratio
                return nn.Sequential(
                    # Expand
                    nn.Conv2d(in_channels, hidden_dim, 1, bias=False),
                    nn.BatchNorm2d(hidden_dim),
                    nn.ReLU6(inplace=True),
                    # Depthwise
                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride=stride, 
                             padding=1, groups=hidden_dim, bias=False),
                    nn.BatchNorm2d(hidden_dim),
                    nn.ReLU6(inplace=True),
                    # Project
                    nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
                    nn.BatchNorm2d(out_channels),
                )
            
            def forward(self, x):
                features = self.backbone(x)
                output = self.decoder(features)
                return output
        
        return EfficientParser()
    
    def create_warping_network(self):
        """íš¨ìœ¨ì ì¸ ì›Œí•‘ ë„¤íŠ¸ì›Œí¬"""
        
        class WarpingNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # ê²½ëŸ‰ íŠ¹ì§• ì¶”ì¶œê¸°
                self.feature_extractor = nn.Sequential(
                    nn.Conv2d(6, 32, 3, padding=1),
                    nn.BatchNorm2d(32),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    
                    nn.Conv2d(32, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.AdaptiveAvgPool2d((8, 6))
                )
                
                # TPS íŒŒë¼ë¯¸í„° ì˜ˆì¸¡
                self.tps_predictor = nn.Sequential(
                    nn.Linear(128 * 8 * 6, 256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Linear(256, 50)  # 5x5 grid = 25 points * 2 (x,y)
                )
                
            def forward(self, cloth, person_features):
                x = torch.cat([cloth, person_features], dim=1)
                features = self.feature_extractor(x)
                features = features.view(features.size(0), -1)
                tps_params = self.tps_predictor(features)
                return tps_params
        
        return WarpingNetwork()
    
    def optimize_for_m3(self):
        """M3 Max ìµœì í™”"""
        
        # 1. í˜¼í•© ì •ë°€ë„ ì‚¬ìš©
        self.scaler = torch.cuda.amp.GradScaler('mps' if self.device.type == 'mps' else 'cpu')
        
        # 2. ëª¨ë¸ ì»´íŒŒì¼ (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            self.human_parser = torch.compile(self.human_parser)
            self.cloth_warper = torch.compile(self.cloth_warper)
            logger.info("âœ… ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ (torch.compile)")
        
        # 3. ë©”ëª¨ë¦¬ ìµœì í™”
        torch.mps.empty_cache() if self.device.type == 'mps' else None
    
    async def process_image(self, person_image: np.ndarray, 
                          clothing_image: np.ndarray) -> Dict:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        
        start_time = time.time()
        
        try:
            # 1. ì „ì²˜ë¦¬
            person_tensor = self.preprocess_image(person_image, size=(192, 256))
            cloth_tensor = self.preprocess_image(clothing_image, size=(192, 256))
            
            # 2. ì¶”ë¡  (í˜¼í•© ì •ë°€ë„ ì‚¬ìš©)
            with torch.no_grad():
                with torch.autocast(device_type=self.device.type, dtype=torch.float16):
                    # ì¸ì²´ íŒŒì‹±
                    parsing_output = self.human_parser(person_tensor)
                    parsing_mask = torch.argmax(parsing_output, dim=1)
                    
                    # í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ
                    pose_features = self.extract_pose_features(person_tensor)
                    
                    # ì›Œí•‘ íŒŒë¼ë¯¸í„° ì˜ˆì¸¡
                    warp_params = self.cloth_warper(cloth_tensor, pose_features)
            
            # 3. CPUì—ì„œ í›„ì²˜ë¦¬
            parsing_mask_np = parsing_mask.cpu().numpy()[0]
            warp_params_np = warp_params.cpu().numpy()[0]
            
            # 4. ì˜ë¥˜ ì›Œí•‘
            warped_cloth = self.apply_tps_transform(
                clothing_image, warp_params_np, person_image.shape[:2]
            )
            
            # 5. í•©ì„±
            result_image = self.composite_images(
                person_image, warped_cloth, parsing_mask_np
            )
            
            processing_time = time.time() - start_time
            
            return {
                'success': True,
                'result_image': result_image,
                'processing_time': processing_time,
                'device': str(self.device),
                'optimization': 'M3 Max Optimized'
            }
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def preprocess_image(self, image: np.ndarray, size: Tuple[int, int]) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        resized = cv2.resize(image, size)
        
        # ì •ê·œí™”
        normalized = resized.astype(np.float32) / 255.0
        
        # í…ì„œ ë³€í™˜
        tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)
        
        return tensor.to(self.device)
    
    def extract_pose_features(self, person_tensor: torch.Tensor) -> torch.Tensor:
        """í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ (ê°„ì†Œí™”)"""
        
        # ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œë¡œ í¬ì¦ˆ íŠ¹ì§• ê·¼ì‚¬
        edges = torch.nn.functional.conv2d(
            person_tensor.mean(dim=1, keepdim=True),
            torch.tensor([[[[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]]]).to(self.device),
            padding=1
        )
        
        return edges.repeat(1, 3, 1, 1)  # 3ì±„ë„ë¡œ í™•ì¥
    
    def apply_tps_transform(self, image: np.ndarray, params: np.ndarray, 
                           target_shape: Tuple[int, int]) -> np.ndarray:
        """TPS ë³€í™˜ ì ìš©"""
        
        h, w = image.shape[:2]
        target_h, target_w = target_shape
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid_size = 5
        src_points = []
        dst_points = []
        
        for i in range(grid_size):
            for j in range(grid_size):
                x = j * w / (grid_size - 1)
                y = i * h / (grid_size - 1)
                src_points.append([x, y])
                
                # íŒŒë¼ë¯¸í„° ì ìš©
                idx = (i * grid_size + j) * 2
                if idx + 1 < len(params):
                    dx = params[idx] * 20  # ìŠ¤ì¼€ì¼ ì¡°ì •
                    dy = params[idx + 1] * 20
                    dst_x = x + dx
                    dst_y = y + dy
                    dst_points.append([dst_x, dst_y])
                else:
                    dst_points.append([x, y])
        
        # OpenCV TPS
        src_points = np.array(src_points, dtype=np.float32)
        dst_points = np.array(dst_points, dtype=np.float32)
        
        matches = [cv2.DMatch(i, i, 0) for i in range(len(src_points))]
        
        tps = cv2.createThinPlateSplineShapeTransformer()
        tps.estimateTransformation(dst_points.reshape(1, -1, 2), 
                                   src_points.reshape(1, -1, 2), matches)
        
        warped = tps.warpImage(image)
        
        # íƒ€ê²Ÿ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        warped_resized = cv2.resize(warped, (target_w, target_h))
        
        return warped_resized
    
    def composite_images(self, person: np.ndarray, warped_cloth: np.ndarray, 
                        mask: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ í•©ì„±"""
        
        # ìƒì˜ ì˜ì—­ ë§ˆìŠ¤í¬ (label 5, 6, 7)
        upper_body_mask = np.isin(mask, [5, 6, 7]).astype(np.uint8) * 255
        
        # ë§ˆìŠ¤í¬ ë¦¬ì‚¬ì´ì¦ˆ
        h, w = person.shape[:2]
        mask_resized = cv2.resize(upper_body_mask, (w, h))
        
        # ë¶€ë“œëŸ¬ìš´ ê²½ê³„ë¥¼ ìœ„í•œ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
        mask_blurred = cv2.GaussianBlur(mask_resized, (21, 21), 0)
        mask_3ch = cv2.cvtColor(mask_blurred, cv2.COLOR_GRAY2BGR) / 255.0
        
        # ì•ŒíŒŒ ë¸”ë Œë”©
        result = person * (1 - mask_3ch) + warped_cloth * mask_3ch
        
        return result.astype(np.uint8)
    
    def export_to_coreml(self, save_path: str = "virtual_tryon_m3.mlmodel"):
        """Core ML ëª¨ë¸ë¡œ ë³€í™˜ (ì˜µì…˜)"""
        
        try:
            # ì˜ˆì‹œ ì…ë ¥
            example_input = torch.rand(1, 3, 256, 192).to(self.device)
            
            # ëª¨ë¸ ì¶”ì 
            traced_model = torch.jit.trace(self.human_parser, example_input)
            
            # Core ML ë³€í™˜
            ml_model = ct.convert(
                traced_model,
                inputs=[ct.TensorType(shape=(1, 3, 256, 192))],
                compute_units=ct.ComputeUnit.ALL,  # Neural Engine ì‚¬ìš©
                convert_to="mlprogram"  # ìµœì‹  í¬ë§·
            )
            
            ml_model.save(save_path)
            logger.info(f"âœ… Core ML ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
            
        except Exception as e:
            logger.error(f"Core ML ë³€í™˜ ì‹¤íŒ¨: {e}")


# ì‚¬ìš© ì˜ˆì œ
async def main():
    # ì—”ì§„ ì´ˆê¸°í™”
    engine = M3OptimizedVirtualTryOn()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
    person_img = cv2.imread("person.jpg")
    cloth_img = cv2.imread("cloth.jpg")
    
    # ì²˜ë¦¬
    result = await engine.process_image(person_img, cloth_img)
    
    if result['success']:
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"â±ï¸ ì†Œìš” ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {result['device']}")
        
        # ê²°ê³¼ ì €ì¥
        cv2.imwrite("result.jpg", result['result_image'])
    else:
        print(f"âŒ ì˜¤ë¥˜: {result['error']}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())