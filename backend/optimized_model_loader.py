"""
MyCloset AI - ìµœì í™”ëœ í†µí•© ëª¨ë¸ ë¡œë”
M3 Max 128GB ì „ìš© ìµœì í™” ë²„ì „
"""

import json
import torch
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedModelLoader:
    """M3 Max ìµœì í™” ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, config_path: str = "model_scan_results.json"):
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        self.loaded_models = {}
        self.load_times = {}
        
        logger.info(f"ğŸ M3 Max ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”: {self.device}")
        logger.info(f"ğŸ“¦ ê´€ë¦¬ ëŒ€ìƒ ëª¨ë¸: {len(self.config['path_mappings'])}ê°œ")
    
    async def load_model_async(self, model_name: str) -> Optional[torch.nn.Module]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©"""
        if model_name in self.loaded_models:
            logger.info(f"ğŸ”„ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
            return self.loaded_models[model_name]
        
        if model_name not in self.config['path_mappings']:
            logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {model_name}")
            return None
        
        model_path = self.config['path_mappings'][model_name]
        
        try:
            start_time = time.time()
            logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            
            # M3 Max MPS ìµœì í™” ë¡œë”©
            if self.device.type == 'mps':
                # CPUì—ì„œ ë¡œë“œ í›„ MPSë¡œ ì´ë™ (ì•ˆì „í•œ ë°©ë²•)
                model = torch.load(model_path, map_location='cpu', weights_only=False)
                if hasattr(model, 'to'):
                    model = model.to(self.device)
            else:
                model = torch.load(model_path, map_location=self.device, weights_only=False)
            
            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            if hasattr(model, 'eval'):
                model.eval()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if hasattr(model, 'half') and self.device.type == 'mps':
                # MPSì—ì„œ float16 ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
                try:
                    model = model.half()
                except:
                    logger.warning(f"âš ï¸ float16 ë³€í™˜ ì‹¤íŒ¨, float32 ìœ ì§€: {model_name}")
            
            load_time = time.time() - start_time
            self.loaded_models[model_name] = model
            self.load_times[model_name] = load_time
            
            logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} ({load_time:.2f}ì´ˆ)")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def unload_model(self, model_name: str):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)"""
        if model_name in self.loaded_models:
            model = self.loaded_models[model_name]
            if hasattr(model, 'cpu'):
                model.cpu()
            del self.loaded_models[model_name]
            logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {model_name}")
    
    def get_step_model(self, step: str) -> Optional[str]:
        """ë‹¨ê³„ë³„ ê¶Œì¥ ëª¨ë¸ ë°˜í™˜"""
        for model_name, model_info in self.config['selected_models'].items():
            if model_info['step'] == step:
                return model_name
        return None
    
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        if self.device.type == 'mps':
            try:
                total_memory = torch.mps.current_allocated_memory() / (1024**2)  # MB
                return {"mps_memory_mb": total_memory}
            except:
                return {"mps_memory_mb": 0}
        return {"cpu_memory": "N/A"}
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë¡œë” í†µê³„"""
        return {
            "device": str(self.device),
            "loaded_models": len(self.loaded_models),
            "available_models": len(self.config['path_mappings']),
            "load_times": self.load_times,
            "memory_usage": self.get_memory_usage()
        }

# ì „ì—­ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
_global_loader = None

def get_model_loader() -> OptimizedModelLoader:
    """ì „ì—­ ëª¨ë¸ ë¡œë” ë°˜í™˜"""
    global _global_loader
    if _global_loader is None:
        _global_loader = OptimizedModelLoader()
    return _global_loader

async def load_essential_models():
    """í•„ìˆ˜ ëª¨ë¸ë“¤ ì‚¬ì „ ë¡œë”©"""
    loader = get_model_loader()
    
    essential_steps = [
        'step_01_human_parsing',
        'step_02_pose_estimation', 
        'step_03_cloth_segmentation',
        'step_06_virtual_fitting'
    ]
    
    loaded_count = 0
    for step in essential_steps:
        model_name = loader.get_step_model(step)
        if model_name:
            model = await loader.load_model_async(model_name)
            if model:
                loaded_count += 1
    
    logger.info(f"âœ… í•„ìˆ˜ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{len(essential_steps)}")
    return loaded_count

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def test_loader():
        loader = get_model_loader()
        print("ğŸ“Š ëª¨ë¸ ë¡œë” í†µê³„:")
        stats = loader.get_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        print("\nğŸ”„ í•„ìˆ˜ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸:")
        loaded_count = await load_essential_models()
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {loaded_count}ê°œ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
    
    asyncio.run(test_loader())
