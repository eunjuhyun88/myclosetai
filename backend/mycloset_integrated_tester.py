#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - GitHub ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤í„° v4.0
================================================================================
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ í˜¸í™˜ (backend/app/ai_pipeline/)
âœ… StepFactory v11.0 + BaseStepMixin v19.2 íŒ¨í„´ ì‚¬ìš©
âœ… DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì…
âœ… ì‹¤ì œ 229GB AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í™œìš©
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ mycloset-ai-clean ìë™ ê°ì§€
âœ… ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰ â†’ ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„± â†’ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
âœ… GitHub Actions CI/CD ì¤€ë¹„
================================================================================
"""

import os
import sys
import time
import gc
import warnings
import asyncio
import base64
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
import logging
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import json

# GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ìë™ ê°ì§€
PROJECT_ROOT = Path(__file__).resolve()

# mycloset-ai/backend êµ¬ì¡° ê°ì§€
while PROJECT_ROOT.name != 'mycloset-ai' and PROJECT_ROOT.parent != PROJECT_ROOT:
    PROJECT_ROOT = PROJECT_ROOT.parent

if PROJECT_ROOT.name == 'mycloset-ai':
    BACKEND_ROOT = PROJECT_ROOT / "backend"
    if BACKEND_ROOT.exists() and (BACKEND_ROOT / "app").exists():
        sys.path.insert(0, str(BACKEND_ROOT))
        print(f"âœ… GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê°ì§€: {PROJECT_ROOT}")
        print(f"âœ… Backend ë£¨íŠ¸ ì„¤ì •: {BACKEND_ROOT}")
    else:
        print("âŒ GitHub backend/app êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        sys.exit(1)
else:
    print("âŒ mycloset-ai í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    sys.exit(1)

# í™˜ê²½ ìµœì í™”
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class GitHubInferenceTestType(Enum):
    QUICK = "quick"          # 1ê°œ Step (Human Parsing)
    STANDARD = "standard"    # 3ê°œ í•µì‹¬ Step
    FULL_PIPELINE = "full"   # ì „ì²´ íŒŒì´í”„ë¼ì¸
    STRESS_TEST = "stress"   # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸

@dataclass
class GitHubInferenceResult:
    step_name: str
    success: bool
    inference_time: float
    memory_used_mb: float
    model_loaded: bool = False
    ai_inference_executed: bool = False
    result_saved: bool = False
    output_path: Optional[str] = None
    confidence_score: Optional[float] = None
    error_message: Optional[str] = None
    step_factory_used: bool = False
    di_container_used: bool = False
    basestepmixin_compatible: bool = False

class GitHubAIInferenceTester:
    """GitHub êµ¬ì¡° ê¸°ë°˜ ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤í„°"""
    
    def __init__(self, test_type: GitHubInferenceTestType = GitHubInferenceTestType.STANDARD):
        self.test_type = test_type
        self.project_root = PROJECT_ROOT
        self.backend_root = BACKEND_ROOT
        self.results_dir = self.backend_root / "test_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # GitHub ì»´í¬ë„ŒíŠ¸ë“¤
        self.step_factory = None
        self.di_container = None
        self.model_loader = None
        self.pipeline_manager = None
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼
        self.inference_results: List[GitHubInferenceResult] = []
        self.total_inference_time = 0.0
        self.peak_memory_mb = 0.0
        
        # conda í™˜ê²½ í™•ì¸
        self._check_conda_environment()
        
        print(f"ğŸš€ GitHub MyCloset AI ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤í„° v4.0 ì‹œì‘")
        print(f"ğŸ“ í”„ë¡œì íŠ¸: {self.project_root}")
        print(f"ğŸ“ ë°±ì—”ë“œ: {self.backend_root}")
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {test_type.value}")
        
        self._initialize_github_components()
        self._prepare_test_images()
    
    def _check_conda_environment(self):
        """conda í™˜ê²½ í™•ì¸"""
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'none')
        if conda_env == 'mycloset-ai-clean':
            print(f"âœ… conda í™˜ê²½ í™•ì¸: {conda_env}")
        else:
            print(f"âš ï¸ conda í™˜ê²½ í™•ì¸: {conda_env} (ê¶Œì¥: mycloset-ai-clean)")
    
    def _initialize_github_components(self):
        """GitHub í”„ë¡œì íŠ¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            print("ğŸ”§ GitHub ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
            
            # StepFactory ì´ˆê¸°í™” (GitHub íŒ¨í„´)
            try:
                from app.ai_pipeline.factories.step_factory import get_global_step_factory
                self.step_factory = get_global_step_factory()
                if self.step_factory:
                    print("âœ… StepFactory v11.0 ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    print("âš ï¸ StepFactory ì´ˆê¸°í™” ì‹¤íŒ¨")
            except ImportError as e:
                print(f"âš ï¸ StepFactory import ì‹¤íŒ¨: {e}")
            
            # DI Container ì´ˆê¸°í™” (GitHub íŒ¨í„´)
            try:
                from app.core.di_container import get_global_di_container
                self.di_container = get_global_di_container()
                if self.di_container:
                    print("âœ… DI Container ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    print("âš ï¸ DI Container ì´ˆê¸°í™” ì‹¤íŒ¨")
            except ImportError as e:
                print(f"âš ï¸ DI Container import ì‹¤íŒ¨: {e}")
            
            # ModelLoader ì´ˆê¸°í™” (GitHub íŒ¨í„´)
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                self.model_loader = get_global_model_loader()
                if self.model_loader:
                    print("âœ… ModelLoader v5.1 ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    print("âš ï¸ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
            except ImportError as e:
                print(f"âš ï¸ ModelLoader import ì‹¤íŒ¨: {e}")
            
            # PipelineManager ì´ˆê¸°í™” (ì„ íƒì )
            try:
                from app.ai_pipeline.pipeline_manager import PipelineManager
                self.pipeline_manager = PipelineManager()
                print("âœ… PipelineManager ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError as e:
                print(f"âš ï¸ PipelineManager import ì‹¤íŒ¨: {e}")
            
            if not any([self.step_factory, self.model_loader]):
                raise Exception("í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ GitHub ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _prepare_test_images(self):
        """í…ŒìŠ¤íŠ¸ìš© ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± (GitHub í‘œì¤€)"""
        print("ğŸ–¼ï¸ GitHub í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„ ì¤‘...")
        
        # 512x512 ê³ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì¸ë¬¼ ì´ë¯¸ì§€
        person_img = Image.new('RGB', (512, 512), color='lightsteelblue')
        draw = ImageDraw.Draw(person_img)
        
        # ì‚¬ì‹¤ì ì¸ ì¸ë¬¼ í˜•íƒœ
        # ë¨¸ë¦¬
        draw.ellipse([180, 30, 332, 180], fill='peachpuff', outline='saddlebrown', width=2)
        # ëª©
        draw.rectangle([236, 180, 276, 210], fill='peachpuff', outline='saddlebrown')
        # ìƒì²´ (í‹°ì…”ì¸  ì˜ì—­)
        draw.rectangle([200, 210, 312, 380], fill='lightcoral', outline='darkred', width=2)
        # íŒ”
        draw.rectangle([160, 230, 200, 320], fill='peachpuff', outline='saddlebrown', width=2)
        draw.rectangle([312, 230, 352, 320], fill='peachpuff', outline='saddlebrown', width=2)
        # í•˜ì²´ (ë°”ì§€ ì˜ì—­)
        draw.rectangle([220, 380, 292, 480], fill='navy', outline='darkblue', width=2)
        
        # GitHub í…ŒìŠ¤íŠ¸ ë¼ë²¨
        try:
            font = ImageFont.load_default()
            draw.text((160, 10), "GitHub Test Person", fill='black', font=font)
            draw.text((200, 490), "MyCloset AI", fill='black', font=font)
        except:
            draw.text((160, 10), "GitHub Test Person", fill='black')
            draw.text((200, 490), "MyCloset AI", fill='black')
        
        person_path = self.results_dir / "github_test_person.jpg"
        person_img.save(person_path, quality=95)
        
        # 512x512 ê³ í’ˆì§ˆ ì˜ë¥˜ ì´ë¯¸ì§€
        cloth_img = Image.new('RGB', (512, 512), color='white')
        draw = ImageDraw.Draw(cloth_img)
        
        # ìƒì„¸í•œ í‹°ì…”ì¸  ë””ìì¸
        # ë©”ì¸ ëª¸í†µ
        draw.rectangle([130, 80, 382, 420], fill='crimson', outline='darkred', width=3)
        # ì†Œë§¤ (ë” ì‚¬ì‹¤ì )
        draw.rectangle([80, 100, 130, 220], fill='crimson', outline='darkred', width=2)
        draw.rectangle([382, 100, 432, 220], fill='crimson', outline='darkred', width=2)
        # ëª©ì„  (ë¼ìš´ë“œ ë„¥)
        draw.arc([180, 60, 332, 120], 0, 180, fill='darkred', width=4)
        # ë””ìì¸ ìš”ì†Œ
        draw.rectangle([180, 200, 332, 240], fill='white', outline='darkred', width=2)
        
        try:
            font = ImageFont.load_default()
            draw.text((180, 30), "GitHub Test Cloth", fill='black', font=font)
            draw.text((210, 210), "AI Fashion", fill='darkred', font=font)
            draw.text((200, 460), "MyCloset", fill='darkred', font=font)
        except:
            draw.text((180, 30), "GitHub Test Cloth", fill='black')
            draw.text((210, 210), "AI Fashion", fill='darkred')
            draw.text((200, 460), "MyCloset", fill='darkred')
        
        cloth_path = self.results_dir / "github_test_cloth.jpg"
        cloth_img.save(cloth_path, quality=95)
        
        self.test_person_path = person_path
        self.test_cloth_path = cloth_path
        
        print(f"âœ… GitHub í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„ ì™„ë£Œ:")
        print(f"   ì¸ë¬¼: {person_path}")
        print(f"   ì˜ë¥˜: {cloth_path}")
    
    def _monitor_memory(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (M3 Max ìµœì í™”)"""
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / (1024 * 1024)
            self.peak_memory_mb = max(self.peak_memory_mb, memory_mb)
            return memory_mb
        except:
            return 0.0
    
    def _load_image_data(self, image_path: Path) -> Dict[str, Any]:
        """GitHub í‘œì¤€ ì´ë¯¸ì§€ ë°ì´í„° ë¡œë”©"""
        try:
            pil_image = Image.open(image_path).convert('RGB')
            np_array = np.array(pil_image)
            
            # Base64 ì¸ì½”ë”©
            import io
            buffer = io.BytesIO()
            pil_image.save(buffer, format='JPEG', quality=95)
            base64_str = base64.b64encode(buffer.getvalue()).decode()
            
            return {
                'pil_image': pil_image,
                'numpy_array': np_array,
                'base64_string': base64_str,
                'image_path': str(image_path),
                'width': pil_image.width,
                'height': pil_image.height,
                'shape': np_array.shape,
                'format': 'RGB',
                'quality': 'high'
            }
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ {image_path}: {e}")
            return {}
    
    def test_github_step_inference(self, step_name: str) -> GitHubInferenceResult:
        """GitHub Step ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ§  GitHub {step_name} ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        start_time = time.time()
        memory_before = self._monitor_memory()
        
        try:
            # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (GitHub StepFactory íŒ¨í„´)
            step_instance = None
            step_factory_used = False
            di_container_used = False
            
            if self.step_factory:
                try:
                    # StepFactoryë¥¼ í†µí•œ ìƒì„±
                    step_instance = self.step_factory.create_step(step_name)
                    step_factory_used = True
                    print(f"  âœ… StepFactoryë¡œ {step_name} ìƒì„± ì„±ê³µ")
                except Exception as e:
                    print(f"  âš ï¸ StepFactory ìƒì„± ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì§ì ‘ import
            if not step_instance:
                step_instance = self._create_step_directly(step_name)
                if step_instance:
                    print(f"  âœ… ì§ì ‘ importë¡œ {step_name} ìƒì„± ì„±ê³µ")
            
            if not step_instance:
                return GitHubInferenceResult(
                    step_name=step_name,
                    success=False,
                    inference_time=time.time() - start_time,
                    memory_used_mb=0.0,
                    error_message="Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨"
                )
            
            # BaseStepMixin í˜¸í™˜ì„± ê²€ì¦
            basestepmixin_compatible = self._check_basestepmixin_compatibility(step_instance)
            
            # DI Container ì˜ì¡´ì„± ì£¼ì… (ì„ íƒì )
            if self.di_container and hasattr(step_instance, 'inject_dependencies'):
                try:
                    step_instance.inject_dependencies(self.di_container)
                    di_container_used = True
                    print(f"  âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    print(f"  âš ï¸ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            person_data = self._load_image_data(self.test_person_path)
            cloth_data = self._load_image_data(self.test_cloth_path)
            
            if not person_data or not cloth_data:
                return GitHubInferenceResult(
                    step_name=step_name,
                    success=False,
                    inference_time=time.time() - start_time,
                    memory_used_mb=0.0,
                    error_message="í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨"
                )
            
            # GitHub Stepë³„ ì…ë ¥ ë°ì´í„° êµ¬ì„±
            input_data = self._prepare_github_step_input(step_name, person_data, cloth_data)
            
            print(f"  ğŸ“Š ì…ë ¥ ë°ì´í„°: {list(input_data.keys())}")
            
            # ëª¨ë¸ ë¡œë”© í™•ì¸
            model_loaded = self._check_model_loading(step_instance)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            result = None
            ai_inference_executed = False
            
            if hasattr(step_instance, 'process'):
                try:
                    if asyncio.iscoroutinefunction(step_instance.process):
                        # ë¹„ë™ê¸° ì‹¤í–‰
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        result = loop.run_until_complete(step_instance.process(**input_data))
                        loop.close()
                    else:
                        # ë™ê¸° ì‹¤í–‰
                        result = step_instance.process(**input_data)
                    
                    ai_inference_executed = True
                    print(f"  ğŸ§  AI ì¶”ë¡  ì‹¤í–‰ ì™„ë£Œ")
                    
                except Exception as process_e:
                    print(f"  âš ï¸ process ì‹¤í–‰ ì‹¤íŒ¨: {process_e}")
                    # í´ë°±: ë‹¤ë¥¸ ë©”ì„œë“œ ì‹œë„
                    if hasattr(step_instance, 'run_inference'):
                        try:
                            result = step_instance.run_inference(input_data)
                            ai_inference_executed = True
                            print(f"  ğŸ§  run_inferenceë¡œ AI ì¶”ë¡  ì‹¤í–‰ ì™„ë£Œ")
                        except Exception as run_e:
                            print(f"  âŒ run_inference ì‹¤í–‰ ì‹¤íŒ¨: {run_e}")
            
            inference_time = time.time() - start_time
            memory_after = self._monitor_memory()
            memory_used = memory_after - memory_before
            
            # ê²°ê³¼ ë¶„ì„
            success = result is not None and ai_inference_executed
            result_saved = False
            output_path = None
            confidence_score = None
            
            if success and isinstance(result, dict):
                # ê²°ê³¼ ì €ì¥
                output_path = self._save_github_result(step_name, result)
                result_saved = output_path is not None
                
                # ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ì¶œ
                confidence_score = result.get('confidence', result.get('confidence_score'))
                
                print(f"  âœ… GitHub ì¶”ë¡  ì„±ê³µ: {inference_time:.2f}s, {memory_used:.1f}MB")
                if confidence_score:
                    print(f"  ğŸ“Š ì‹ ë¢°ë„: {confidence_score:.3f}")
            else:
                print(f"  âŒ GitHub ì¶”ë¡  ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ")
            
            return GitHubInferenceResult(
                step_name=step_name,
                success=success,
                inference_time=inference_time,
                memory_used_mb=memory_used,
                model_loaded=model_loaded,
                ai_inference_executed=ai_inference_executed,
                result_saved=result_saved,
                output_path=output_path,
                confidence_score=confidence_score,
                step_factory_used=step_factory_used,
                di_container_used=di_container_used,
                basestepmixin_compatible=basestepmixin_compatible
            )
            
        except Exception as e:
            inference_time = time.time() - start_time
            memory_after = self._monitor_memory()
            
            print(f"  âŒ {step_name} GitHub ì¶”ë¡  ì˜¤ë¥˜: {e}")
            
            return GitHubInferenceResult(
                step_name=step_name,
                success=False,
                inference_time=inference_time,
                memory_used_mb=memory_after - memory_before,
                error_message=str(e)
            )
    
    def _create_step_directly(self, step_name: str):
        """ì§ì ‘ Step ìƒì„± (í´ë°±)"""
        try:
            # GitHub Step ë§¤í•‘
            step_mappings = {
                "step_01_human_parsing": ("app.ai_pipeline.steps.step_01_human_parsing", "HumanParsingStep"),
                "step_02_pose_estimation": ("app.ai_pipeline.steps.step_02_pose_estimation", "PoseEstimationStep"),
                "step_03_cloth_segmentation": ("app.ai_pipeline.steps.step_03_cloth_segmentation", "ClothSegmentationStep"),
                "step_04_geometric_matching": ("app.ai_pipeline.steps.step_04_geometric_matching", "GeometricMatchingStep"),
                "step_05_cloth_warping": ("app.ai_pipeline.steps.step_05_cloth_warping", "ClothWarpingStep"),
                "step_06_virtual_fitting": ("app.ai_pipeline.steps.step_06_virtual_fitting", "VirtualFittingStep"),
                "step_07_post_processing": ("app.ai_pipeline.steps.step_07_post_processing", "PostProcessingStep"),
                "step_08_quality_assessment": ("app.ai_pipeline.steps.step_08_quality_assessment", "QualityAssessmentStep")
            }
            
            if step_name not in step_mappings:
                return None
            
            module_path, class_name = step_mappings[step_name]
            
            import importlib
            module = importlib.import_module(module_path)
            step_class = getattr(module, class_name)
            
            # BaseStepMixin í˜¸í™˜ kwargs (ì•ˆì „í•œ ì´ˆê¸°í™”)
            step_kwargs = {
                'step_name': step_name,
                'device': 'cpu'
            }
            
            # ì„ íƒì  íŒŒë¼ë¯¸í„° ì¶”ê°€ (ì•ˆì „í•˜ê²Œ)
            if self.model_loader:
                try:
                    step_instance = step_class(**step_kwargs)
                    if hasattr(step_instance, 'set_model_loader'):
                        step_instance.set_model_loader(self.model_loader)
                    return step_instance
                except Exception as e:
                    print(f"  âš ï¸ ModelLoader ì£¼ì… ì‹¤íŒ¨, ê¸°ë³¸ ìƒì„± ì‹œë„: {e}")
            
            return step_class(**step_kwargs)
            
        except Exception as e:
            print(f"  âŒ ì§ì ‘ Step ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _check_basestepmixin_compatibility(self, step_instance) -> bool:
        """BaseStepMixin v19.2 í˜¸í™˜ì„± ê²€ì¦"""
        try:
            mro_names = [cls.__name__ for cls in step_instance.__class__.__mro__]
            return 'BaseStepMixin' in mro_names
        except:
            return False
    
    def _check_model_loading(self, step_instance) -> bool:
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸"""
        try:
            # ë‹¤ì–‘í•œ ëª¨ë¸ ë¡œë”© í™•ì¸ ë°©ë²•
            if hasattr(step_instance, 'models_loaded'):
                return getattr(step_instance, 'models_loaded', False)
            elif hasattr(step_instance, 'model_loader') and step_instance.model_loader:
                return True
            elif hasattr(step_instance, '_models') and step_instance._models:
                return len(step_instance._models) > 0
            else:
                return False
        except:
            return False
    
    def _prepare_github_step_input(self, step_name: str, person_data: Dict, cloth_data: Dict) -> Dict[str, Any]:
        """GitHub Stepë³„ ì…ë ¥ ë°ì´í„° ì¤€ë¹„"""
        base_input = {
            'person_image': person_data['pil_image'],
            'clothing_image': cloth_data['pil_image'],
            'user_id': 'github_test_user',
            'session_id': f'github_test_session_{int(time.time())}',
            'github_test_mode': True
        }
        
        # Stepë³„ íŠ¹í™” ì…ë ¥ (GitHub í‘œì¤€)
        if 'human_parsing' in step_name:
            return {
                **base_input,
                'image': person_data['pil_image'],
                'parsing_type': 'full_body',
                'output_format': 'mask'
            }
        elif 'pose' in step_name:
            return {
                **base_input,
                'image': person_data['pil_image'],
                'keypoint_format': 'coco_17',
                'confidence_threshold': 0.3
            }
        elif 'segmentation' in step_name:
            return {
                **base_input,
                'target_image': cloth_data['pil_image'],
                'segmentation_type': 'cloth',
                'use_sam': True
            }
        elif 'geometric' in step_name:
            return {
                **base_input,
                'source_cloth': cloth_data['pil_image'],
                'person_mask': person_data['numpy_array'],
                'matching_algorithm': 'tps'
            }
        elif 'warping' in step_name:
            return {
                **base_input,
                'cloth_image': cloth_data['pil_image'],
                'pose_keypoints': person_data['numpy_array'],
                'warping_method': 'dense_flow'
            }
        elif 'virtual_fitting' in step_name:
            return {
                **base_input,
                'cloth_type': 'upper_body',
                'fitting_mode': 'realistic',
                'quality_level': 'high'
            }
        elif 'post_processing' in step_name:
            return {
                **base_input,
                'enhance_quality': True,
                'remove_artifacts': True
            }
        elif 'quality' in step_name:
            return {
                **base_input,
                'assessment_metrics': ['fid', 'lpips', 'ssim'],
                'reference_image': person_data['pil_image']
            }
        else:
            return base_input
    
    def _save_github_result(self, step_name: str, result: Dict[str, Any]) -> Optional[str]:
        """GitHub ì¶”ë¡  ê²°ê³¼ ì €ì¥"""
        try:
            output_dir = self.results_dir / f"github_{step_name}"
            output_dir.mkdir(exist_ok=True)
            
            timestamp = int(time.time())
            
            # ì´ë¯¸ì§€ ê²°ê³¼ ì €ì¥
            if 'result' in result:
                result_data = result['result']
                
                # Base64 ì´ë¯¸ì§€
                if isinstance(result_data, str) and ('base64' in result_data or result_data.startswith('data:image')):
                    try:
                        if ',' in result_data:
                            result_data = result_data.split(',')[1]
                        image_data = base64.b64decode(result_data)
                        
                        output_path = output_dir / f"github_result_{timestamp}.jpg"
                        with open(output_path, 'wb') as f:
                            f.write(image_data)
                        
                        return str(output_path)
                    except Exception as e:
                        print(f"  âš ï¸ Base64 ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                # NumPy ë°°ì—´
                elif hasattr(result_data, 'shape'):
                    try:
                        if len(result_data.shape) >= 2:
                            if result_data.max() <= 1.0:
                                result_data = (result_data * 255).astype(np.uint8)
                            
                            if len(result_data.shape) == 3:
                                image = Image.fromarray(result_data)
                            elif len(result_data.shape) == 2:
                                image = Image.fromarray(result_data, mode='L')
                            else:
                                return None
                            
                            output_path = output_dir / f"github_result_{timestamp}.jpg"
                            image.save(output_path, quality=95)
                            
                            return str(output_path)
                    except Exception as e:
                        print(f"  âš ï¸ NumPy ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # JSON ë©”íƒ€ë°ì´í„° ì €ì¥
            json_path = output_dir / f"github_result_{timestamp}.json"
            try:
                json_result = {}
                for key, value in result.items():
                    if hasattr(value, 'tolist'):
                        json_result[key] = value.tolist()
                    elif hasattr(value, 'shape'):
                        json_result[key] = f"tensor_shape_{value.shape}"
                    elif isinstance(value, (str, int, float, bool, list, dict)):
                        json_result[key] = value
                    else:
                        json_result[key] = str(value)
                
                json_result['github_test_metadata'] = {
                    'timestamp': timestamp,
                    'step_name': step_name,
                    'test_version': '4.0'
                }
                
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(json_result, f, indent=2, ensure_ascii=False)
                
                return str(json_path)
            except Exception as e:
                print(f"  âš ï¸ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            
            return None
            
        except Exception as e:
            print(f"  âš ï¸ GitHub ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def run_github_inference_tests(self) -> List[GitHubInferenceResult]:
        """GitHub ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\nğŸ§  GitHub ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘ (ëª¨ë“œ: {self.test_type.value})")
        print("=" * 60)
        
        # í…ŒìŠ¤íŠ¸í•  Stepë“¤ ì„ íƒ (GitHub í‘œì¤€)
        if self.test_type == GitHubInferenceTestType.QUICK:
            test_steps = ["step_01_human_parsing"]
        elif self.test_type == GitHubInferenceTestType.STANDARD:
            test_steps = [
                "step_01_human_parsing",
                "step_03_cloth_segmentation", 
                "step_06_virtual_fitting"
            ]
        elif self.test_type == GitHubInferenceTestType.FULL_PIPELINE:
            test_steps = [
                "step_01_human_parsing",
                "step_02_pose_estimation",
                "step_03_cloth_segmentation",
                "step_04_geometric_matching",
                "step_05_cloth_warping",
                "step_06_virtual_fitting"
            ]
        else:  # STRESS_TEST
            test_steps = [
                "step_01_human_parsing",
                "step_02_pose_estimation",
                "step_03_cloth_segmentation",
                "step_04_geometric_matching",
                "step_05_cloth_warping",
                "step_06_virtual_fitting",
                "step_07_post_processing",
                "step_08_quality_assessment"
            ]
        
        results = []
        
        for i, step_name in enumerate(test_steps, 1):
            print(f"\n[{i}/{len(test_steps)}] GitHub {step_name} ì¶”ë¡  í…ŒìŠ¤íŠ¸")
            
            result = self.test_github_step_inference(step_name)
            results.append(result)
            self.inference_results.append(result)
            
            if result.success:
                self.total_inference_time += result.inference_time
                print(f"  âœ… ì„±ê³µ: {result.inference_time:.2f}s")
                if result.result_saved:
                    print(f"  ğŸ’¾ ê²°ê³¼ ì €ì¥: {result.output_path}")
            else:
                print(f"  âŒ ì‹¤íŒ¨: {result.error_message}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max ìµœì í™”)
            gc.collect()
            if hasattr(gc, 'set_debug'):
                gc.set_debug(0)
        
        return results
    
    def generate_github_report(self):
        """GitHub ì¶”ë¡  í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 80)
        print("ğŸ§  GitHub MyCloset AI ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("=" * 80)
        
        if not self.inference_results:
            print("âŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í†µê³„ ê³„ì‚°
        total_tests = len(self.inference_results)
        successful_tests = sum(1 for r in self.inference_results if r.success)
        success_rate = (successful_tests / total_tests) * 100
        
        avg_inference_time = self.total_inference_time / successful_tests if successful_tests > 0 else 0
        total_memory_used = sum(r.memory_used_mb for r in self.inference_results)
        
        # GitHub íŠ¹í™” í†µê³„
        step_factory_used_count = sum(1 for r in self.inference_results if r.step_factory_used)
        di_container_used_count = sum(1 for r in self.inference_results if r.di_container_used)
        basestepmixin_compatible_count = sum(1 for r in self.inference_results if r.basestepmixin_compatible)
        model_loaded_count = sum(1 for r in self.inference_results if r.model_loaded)
        ai_inference_count = sum(1 for r in self.inference_results if r.ai_inference_executed)
        
        print(f"ğŸ“Š GitHub MyCloset AI ì „ì²´ í†µê³„:")
        print(f"   í”„ë¡œì íŠ¸: {self.project_root}")
        print(f"   ë°±ì—”ë“œ: {self.backend_root}")
        print(f"   í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {self.test_type.value.upper()}")
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"   ì„±ê³µ: {successful_tests}ê°œ")
        print(f"   ì‹¤íŒ¨: {total_tests - successful_tests}ê°œ")
        print(f"   ì„±ê³µë¥ : {success_rate:.1f}%")
        print(f"   ì´ ì¶”ë¡  ì‹œê°„: {self.total_inference_time:.2f}ì´ˆ")
        print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.2f}ì´ˆ/Step")
        print(f"   ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©: {total_memory_used:.1f}MB")
        print(f"   í”¼í¬ ë©”ëª¨ë¦¬: {self.peak_memory_mb:.1f}MB")
        
        print(f"\nğŸ”§ GitHub ì»´í¬ë„ŒíŠ¸ ì‚¬ìš© í†µê³„:")
        print(f"   StepFactory ì‚¬ìš©: {step_factory_used_count}/{total_tests}ê°œ")
        print(f"   DI Container ì‚¬ìš©: {di_container_used_count}/{total_tests}ê°œ")
        print(f"   BaseStepMixin í˜¸í™˜: {basestepmixin_compatible_count}/{total_tests}ê°œ")
        print(f"   ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_loaded_count}/{total_tests}ê°œ")
        print(f"   AI ì¶”ë¡  ì‹¤í–‰: {ai_inference_count}/{total_tests}ê°œ")
        
        # ìƒì„¸ ê²°ê³¼
        print(f"\nğŸ“‹ GitHub Stepë³„ ìƒì„¸ ê²°ê³¼:")
        for result in self.inference_results:
            status = "âœ…" if result.success else "âŒ"
            time_info = f"({result.inference_time:.2f}s)" if result.inference_time > 0 else ""
            memory_info = f"[{result.memory_used_mb:.1f}MB]" if result.memory_used_mb > 0 else ""
            
            components = []
            if result.step_factory_used:
                components.append("StepFactory")
            if result.di_container_used:
                components.append("DI")
            if result.basestepmixin_compatible:
                components.append("BaseStepMixin")
            if result.model_loaded:
                components.append("Model")
            if result.ai_inference_executed:
                components.append("AI")
            
            component_info = f"[{'/'.join(components)}]" if components else "[ì§ì ‘ìƒì„±]"
            
            print(f"  {status} {result.step_name}: ", end="")
            
            if result.success:
                confidence_info = f"ì‹ ë¢°ë„: {result.confidence_score:.3f}" if result.confidence_score else "ì¶”ë¡  ì™„ë£Œ"
                output_info = "ê²°ê³¼ ì €ì¥ë¨" if result.result_saved else "ë©”íƒ€ë°ì´í„°ë§Œ"
                print(f"{confidence_info}, {output_info} {component_info} {time_info} {memory_info}")
            else:
                print(f"{result.error_message} {component_info} {time_info}")
        
        # ì €ì¥ëœ ê²°ê³¼ íŒŒì¼ë“¤
        saved_results = [r for r in self.inference_results if r.result_saved]
        if saved_results:
            print(f"\nğŸ’¾ GitHub ì €ì¥ëœ ê²°ê³¼ íŒŒì¼:")
            for result in saved_results:
                if result.output_path:
                    print(f"   {result.step_name}: {result.output_path}")
        
        # GitHub CI/CD í˜¸í™˜ì„± ê²€ì¦
        github_compatibility_score = self._calculate_github_compatibility()
        
        # ìµœì¢… ê²°ë¡ 
        print(f"\nğŸ¯ GitHub MyCloset AI ìµœì¢… ê²°ë¡ :")
        if success_rate >= 90 and github_compatibility_score >= 80:
            print("   ğŸš€ MyCloset AIê°€ GitHub í™˜ê²½ì—ì„œ ì™„ë²½í•˜ê²Œ ì‹¤ì œ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤!")
            print("   ğŸŒŸ ëª¨ë“  Stepì´ GitHub í‘œì¤€ì— ë§ì¶° ì •ìƒì ìœ¼ë¡œ AI ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            print("   ğŸ”§ StepFactory, DI Container, BaseStepMixin íŒ¨í„´ì´ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.")
        elif success_rate >= 70 and github_compatibility_score >= 60:
            print("   âœ… MyCloset AIê°€ GitHub í™˜ê²½ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ì¶”ë¡ ì„ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤!")
            print("   ğŸ”§ ì¼ë¶€ Stepì— ë¬¸ì œê°€ ìˆì§€ë§Œ í•µì‹¬ ê¸°ëŠ¥ì€ GitHub í‘œì¤€ì— ë§ì¶° ì‘ë™í•©ë‹ˆë‹¤.")
        elif success_rate >= 50:
            print("   âš ï¸ MyCloset AI GitHub ì¶”ë¡ ì— ë¶€ë¶„ì  ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("   ğŸ› ï¸ ì‹¤íŒ¨í•œ Stepë“¤ì˜ GitHub í˜¸í™˜ì„±ê³¼ ëª¨ë¸ ë¡œë”© ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            print("   âŒ MyCloset AI GitHub ì¶”ë¡ ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("   ğŸš¨ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡°, ëª¨ë¸ íŒŒì¼, conda í™˜ê²½ì„ ì „ë©´ ì ê²€í•˜ì„¸ìš”.")
        
        print(f"\nğŸ“ˆ GitHub í˜¸í™˜ì„± ì ìˆ˜: {github_compatibility_score:.1f}/100")
        print("=" * 80)
    
    def _calculate_github_compatibility(self) -> float:
        """GitHub í˜¸í™˜ì„± ì ìˆ˜ ê³„ì‚°"""
        if not self.inference_results:
            return 0.0
        
        total_tests = len(self.inference_results)
        scores = []
        
        for result in self.inference_results:
            score = 0
            
            # ê¸°ë³¸ ì„±ê³µ (40ì )
            if result.success:
                score += 40
            
            # GitHub ì»´í¬ë„ŒíŠ¸ ì‚¬ìš© (ê° 10ì )
            if result.step_factory_used:
                score += 10
            if result.di_container_used:
                score += 10
            if result.basestepmixin_compatible:
                score += 10
            
            # AI ê¸°ëŠ¥ (ê° 10ì )
            if result.model_loaded:
                score += 10
            if result.ai_inference_executed:
                score += 10
            
            # ê²°ê³¼ ì €ì¥ (10ì )
            if result.result_saved:
                score += 10
            
            scores.append(score)
        
        return sum(scores) / total_tests

def quick_github_test():
    """ë¹ ë¥¸ GitHub AI ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("âš¡ GitHub MyCloset AI ë¹ ë¥¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
    tester = GitHubAIInferenceTester(GitHubInferenceTestType.QUICK)
    results = tester.run_github_inference_tests()
    tester.generate_github_report()
    return len([r for r in results if r.success]) > 0

def standard_github_test():
    """í‘œì¤€ GitHub AI ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("ğŸ” GitHub MyCloset AI í‘œì¤€ ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
    tester = GitHubAIInferenceTester(GitHubInferenceTestType.STANDARD)
    results = tester.run_github_inference_tests()
    tester.generate_github_report()
    return results

def full_pipeline_github_test():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ GitHub ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ GitHub MyCloset AI ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
    tester = GitHubAIInferenceTester(GitHubInferenceTestType.FULL_PIPELINE)
    results = tester.run_github_inference_tests()
    tester.generate_github_report()
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¥ GitHub MyCloset AI ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤í„° v4.0")
    print("=" * 60)
    print("GitHub í˜¸í™˜ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (1ê°œ Step - Human Parsing)")
    print("2. í‘œì¤€ í…ŒìŠ¤íŠ¸ (3ê°œ í•µì‹¬ Step)")
    print("3. ì „ì²´ íŒŒì´í”„ë¼ì¸ (6ê°œ Step)")
    print("4. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (8ê°œ ì „ì²´ Step)")
    
    choice = input("ì„ íƒ (1/2/3/4): ").strip()
    
    if choice == "1":
        return quick_github_test()
    elif choice == "3":
        return full_pipeline_github_test()
    elif choice == "4":
        tester = GitHubAIInferenceTester(GitHubInferenceTestType.STRESS_TEST)
        results = tester.run_github_inference_tests()
        tester.generate_github_report()
        return results
    else:
        return standard_github_test()

if __name__ == "__main__":
    main()