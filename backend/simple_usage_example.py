#!/usr/bin/env python3
"""
MyCloset-AI ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì‹œ
==========================================

ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë“¤ì…ë‹ˆë‹¤.
"""

import sys
import os
sys.path.append('.')

def example_1_direct_model_usage():
    """ì˜ˆì‹œ 1: ì§ì ‘ ëª¨ë¸ ì‚¬ìš© (ê°€ì¥ ê°„ë‹¨)"""
    print("ğŸ”§ ì˜ˆì‹œ 1: ì§ì ‘ ëª¨ë¸ ì‚¬ìš©")
    print("=" * 40)
    
    try:
        # ì„±ê³µí•œ ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©
        from app.ai_pipeline.utils.model_architectures import (
            OpenPoseModel, RAFTModel, SAMModel, U2NetModel,
            RealESRGANModel, LPIPSModel, DeepLabV3PlusModel,
            VITONHDModel, GFPGANModel
        )
        import torch
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤ (ì„±ê³µí•œ ê²ƒë“¤ë§Œ)
        working_models = [
            ('OpenPoseModel', OpenPoseModel()),
            ('RAFTModel', RAFTModel()),
            ('SAMModel', SAMModel()),
            ('U2NetModel', U2NetModel()),
            ('RealESRGANModel', RealESRGANModel()),
            ('LPIPSModel', LPIPSModel()),
            ('DeepLabV3PlusModel', DeepLabV3PlusModel()),
            ('VITONHDModel', VITONHDModel()),
            ('GFPGANModel', GFPGANModel())
        ]
        
        for model_name, model in working_models:
            try:
                print(f"\nğŸ” {model_name}:")
                
                # ëª¨ë¸ ì •ë³´
                total_params = sum(p.numel() for p in model.parameters())
                print(f"   ğŸ“Š íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
                
                # Forward pass í…ŒìŠ¤íŠ¸
                if model_name == 'LPIPSModel':
                    x = torch.randn(1, 3, 256, 256)
                    y = torch.randn(1, 3, 256, 256)
                    output = model(x, y)
                    print(f"   âœ… ì¶œë ¥: {output.shape}")
                elif model_name == 'VITONHDModel':
                    person = torch.randn(1, 3, 256, 256)
                    clothing = torch.randn(1, 3, 256, 256)
                    output = model(person, clothing)
                    print(f"   âœ… ì¶œë ¥: {type(output)} (ë”•ì…”ë„ˆë¦¬)")
                else:
                    x = torch.randn(1, 3, 256, 256)
                    output = model(x)
                    print(f"   âœ… ì¶œë ¥: {output.shape}")
                    
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
                
    except Exception as e:
        print(f"âŒ ì§ì ‘ ëª¨ë¸ ì‚¬ìš© ì‹¤íŒ¨: {e}")

def example_2_step_integration():
    """ì˜ˆì‹œ 2: Step í´ë˜ìŠ¤ì— í†µí•©"""
    print("\nğŸ”§ ì˜ˆì‹œ 2: Step í´ë˜ìŠ¤ì— í†µí•©")
    print("=" * 40)
    
    try:
        # Step í´ë˜ìŠ¤ì—ì„œ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
        from app.ai_pipeline.utils.model_architectures import (
            OpenPoseModel, SAMModel, U2NetModel, RealESRGANModel
        )
        import torch
        
        class ExampleStep:
            def __init__(self):
                # ëª¨ë¸ ì´ˆê¸°í™”
                self.pose_model = OpenPoseModel()
                self.segmentation_model = SAMModel()
                self.enhancement_model = RealESRGANModel()
                
            def process_image(self, image):
                """ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
                print("ğŸ”„ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
                
                # 1. í¬ì¦ˆ ì¶”ì •
                pose_result = self.pose_model(image)
                print(f"   ğŸ“ í¬ì¦ˆ ì¶”ì • ì™„ë£Œ: {pose_result.shape}")
                
                # 2. ì„¸ê·¸ë©˜í…Œì´ì…˜
                seg_result = self.segmentation_model(image)
                print(f"   ğŸ¯ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ: {seg_result.shape}")
                
                # 3. ì´ë¯¸ì§€ í–¥ìƒ
                enhanced_result = self.enhancement_model(image)
                print(f"   âœ¨ ì´ë¯¸ì§€ í–¥ìƒ ì™„ë£Œ: {enhanced_result.shape}")
                
                return {
                    'pose': pose_result,
                    'segmentation': seg_result,
                    'enhanced': enhanced_result
                }
        
        # ì‚¬ìš© ì˜ˆì‹œ
        step = ExampleStep()
        test_image = torch.randn(1, 3, 256, 256)
        result = step.process_image(test_image)
        
        print(f"\nâœ… Step ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ğŸ“Š ê²°ê³¼ í‚¤: {list(result.keys())}")
        
    except Exception as e:
        print(f"âŒ Step í†µí•© ì‹¤íŒ¨: {e}")

def example_3_checkpoint_loading():
    """ì˜ˆì‹œ 3: ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
    print("\nğŸ”§ ì˜ˆì‹œ 3: ì²´í¬í¬ì¸íŠ¸ ë¡œë”©")
    print("=" * 40)
    
    try:
        from app.ai_pipeline.utils.model_architectures import OpenPoseModel
        import torch
        
        # ëª¨ë¸ ìƒì„±
        model = OpenPoseModel()
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        checkpoint_path = "ai_models/step_02/openpose.pth"
        
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # state_dict ì¶”ì¶œ
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            # ê°€ì¤‘ì¹˜ ë¡œë”©
            try:
                model.load_state_dict(state_dict, strict=False)
                print("   âœ… ê°€ì¤‘ì¹˜ ë¡œë”© ì„±ê³µ!")
                
                # í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
                test_input = torch.randn(1, 3, 256, 256)
                with torch.no_grad():
                    output = model(test_input)
                print(f"   âœ… ì¶”ë¡  ì„±ê³µ: {output.shape}")
                
            except Exception as e:
                print(f"   âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {e}")
        else:
            print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
            
    except Exception as e:
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")

def example_4_api_integration():
    """ì˜ˆì‹œ 4: APIì— í†µí•©"""
    print("\nğŸ”§ ì˜ˆì‹œ 4: APIì— í†µí•©")
    print("=" * 40)
    
    try:
        from app.ai_pipeline.utils.model_architectures import (
            OpenPoseModel, SAMModel, RealESRGANModel
        )
        import torch
        
        class AIProcessor:
            def __init__(self):
                self.models = {
                    'pose': OpenPoseModel(),
                    'segmentation': SAMModel(),
                    'enhancement': RealESRGANModel()
                }
                
            def process_request(self, request_type, image):
                """API ìš”ì²­ ì²˜ë¦¬"""
                if request_type == 'pose':
                    return self.models['pose'](image)
                elif request_type == 'segmentation':
                    return self.models['segmentation'](image)
                elif request_type == 'enhancement':
                    return self.models['enhancement'](image)
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìš”ì²­ íƒ€ì…: {request_type}")
        
        # API ì‚¬ìš© ì˜ˆì‹œ
        processor = AIProcessor()
        test_image = torch.randn(1, 3, 256, 256)
        
        # í¬ì¦ˆ ì¶”ì • ìš”ì²­
        pose_result = processor.process_request('pose', test_image)
        print(f"ğŸ“ í¬ì¦ˆ ì¶”ì • ê²°ê³¼: {pose_result.shape}")
        
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ìš”ì²­
        seg_result = processor.process_request('segmentation', test_image)
        print(f"ğŸ¯ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼: {seg_result.shape}")
        
        # ì´ë¯¸ì§€ í–¥ìƒ ìš”ì²­
        enhanced_result = processor.process_request('enhancement', test_image)
        print(f"âœ¨ ì´ë¯¸ì§€ í–¥ìƒ ê²°ê³¼: {enhanced_result.shape}")
        
    except Exception as e:
        print(f"âŒ API í†µí•© ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ MyCloset-AI ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì‹œ")
    print("=" * 60)
    
    # ì˜ˆì‹œ ì‹¤í–‰
    example_1_direct_model_usage()
    example_2_step_integration()
    example_3_checkpoint_loading()
    example_4_api_integration()
    
    print("\nğŸ‰ ëª¨ë“  ì˜ˆì‹œ ì‹¤í–‰ ì™„ë£Œ!")
    print("\nğŸ’¡ ì‹¤ì œ ì‚¬ìš© ë°©ë²•:")
    print("   1. from app.ai_pipeline.utils.model_architectures import ëª¨ë¸ëª…")
    print("   2. model = ëª¨ë¸ëª…()")
    print("   3. output = model(input)")
    print("   4. ì²´í¬í¬ì¸íŠ¸ê°€ ìˆë‹¤ë©´ model.load_state_dict() ì‚¬ìš©")

if __name__ == "__main__":
    main()
