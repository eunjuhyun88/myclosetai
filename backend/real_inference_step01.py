#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 01: ì‹¤ì œ AI ì¶”ë¡  ê°€ëŠ¥í•œ Human Parsing
===============================================================================

ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ë° ì •í™•í•œ forward pass êµ¬í˜„
- ATR ëª¨ë¸ êµ¬ì¡° ì—­ì¶”ì 
- ì˜¬ë°”ë¥¸ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬
- ì‹¤ì œ AI ì¶”ë¡  ê²°ê³¼

Author: MyCloset AI Team
Date: 2025-07-25
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
from PIL import Image
import cv2
from pathlib import Path
import logging

class ModelStructureAnalyzer:
    """ë¡œë”©ëœ ëª¨ë¸ì˜ ì‹¤ì œ êµ¬ì¡° ë¶„ì„"""
    
    def __init__(self, checkpoint_path: str):
        self.checkpoint_path = Path(checkpoint_path)
        self.checkpoint = torch.load(checkpoint_path, map_location='cpu')
        self.state_dict = self._extract_state_dict()
    
    def _extract_state_dict(self):
        """state_dict ì¶”ì¶œ"""
        if isinstance(self.checkpoint, dict):
            if 'state_dict' in self.checkpoint:
                return self.checkpoint['state_dict']
            else:
                return self.checkpoint
        return self.checkpoint
    
    def analyze_architecture(self):
        """ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„"""
        layers = list(self.state_dict.keys())
        
        print("ğŸ” ëª¨ë¸ êµ¬ì¡° ë¶„ì„:")
        print(f"ì´ ë ˆì´ì–´: {len(layers)}ê°œ")
        
        # ë ˆì´ì–´ íŒ¨í„´ ë¶„ì„
        patterns = {
            'backbone': [],
            'classifier': [],
            'decoder': [],
            'others': []
        }
        
        for layer in layers:
            if any(x in layer for x in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'backbone']):
                patterns['backbone'].append(layer)
            elif any(x in layer for x in ['classifier', 'head', 'cls']):
                patterns['classifier'].append(layer)
            elif any(x in layer for x in ['decoder', 'upconv', 'deconv']):
                patterns['decoder'].append(layer)
            else:
                patterns['others'].append(layer)
        
        for pattern_name, pattern_layers in patterns.items():
            if pattern_layers:
                print(f"\n{pattern_name.upper()}:")
                for layer in pattern_layers[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                    shape = self.state_dict[layer].shape
                    print(f"  {layer}: {shape}")
                if len(pattern_layers) > 5:
                    print(f"  ... ì´ {len(pattern_layers)}ê°œ")
        
        return patterns

class ATRModelReconstructor:
    """ATR ëª¨ë¸ êµ¬ì¡° ì¬êµ¬ì„±"""
    
    def __init__(self, state_dict):
        self.state_dict = state_dict
        self.num_classes = self._infer_num_classes()
    
    def _infer_num_classes(self):
        """ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜ ì¶”ë¡ """
        # ë§ˆì§€ë§‰ classifier ë ˆì´ì–´ì—ì„œ í´ë˜ìŠ¤ ìˆ˜ ì¶”ì¶œ
        for key, tensor in self.state_dict.items():
            if 'classifier' in key and 'weight' in key:
                if len(tensor.shape) == 4:  # Conv2d weight
                    return tensor.shape[0]  # output channels
                elif len(tensor.shape) == 2:  # Linear weight  
                    return tensor.shape[0]  # output features
        
        # ê¸°ë³¸ê°’
        return 20
    
    def build_model(self):
        """ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ë¹Œë“œ"""
        
        class ATRNet(nn.Module):
            def __init__(self, num_classes=20):
                super().__init__()
                
                # ResNet50 ê¸°ë°˜ ë°±ë³¸ (ì¼ë°˜ì ì¸ ATR êµ¬ì¡°)
                self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
                self.bn1 = nn.BatchNorm2d(64)
                self.relu = nn.ReLU(inplace=True)
                self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
                
                # ResNet ë ˆì´ì–´ë“¤ (ê°„ì†Œí™”)
                self.layer1 = self._make_layer(64, 64, 3)
                self.layer2 = self._make_layer(64, 128, 4, stride=2)
                self.layer3 = self._make_layer(128, 256, 6, stride=2)  
                self.layer4 = self._make_layer(256, 512, 3, stride=2)
                
                # ì„¸ê·¸ë©˜í…Œì´ì…˜ í—¤ë“œ
                self.classifier = nn.Conv2d(512, num_classes, kernel_size=1)
                
            def _make_layer(self, inplanes, planes, blocks, stride=1):
                layers = []
                
                # ì²« ë²ˆì§¸ ë¸”ë¡
                layers.append(nn.Conv2d(inplanes, planes, 3, stride=stride, padding=1, bias=False))
                layers.append(nn.BatchNorm2d(planes))
                layers.append(nn.ReLU(inplace=True))
                
                # ë‚˜ë¨¸ì§€ ë¸”ë¡ë“¤
                for _ in range(1, blocks):
                    layers.append(nn.Conv2d(planes, planes, 3, padding=1, bias=False))
                    layers.append(nn.BatchNorm2d(planes))
                    layers.append(nn.ReLU(inplace=True))
                
                return nn.Sequential(*layers)
            
            def forward(self, x):
                # ì…ë ¥ í¬ê¸° ì €ì¥
                input_size = x.shape[2:]
                
                # ë°±ë³¸ í†µê³¼
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.maxpool(x)
                
                x = self.layer1(x)
                x = self.layer2(x)
                x = self.layer3(x)
                x = self.layer4(x)
                
                # ë¶„ë¥˜
                x = self.classifier(x)
                
                # ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
                x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=False)
                
                return x
        
        return ATRNet(self.num_classes)

class RealHumanParsingInference:
    """ì‹¤ì œ AI ì¶”ë¡  ê°€ëŠ¥í•œ Human Parsing"""
    
    def __init__(self, model_path: str):
        self.model_path = Path(model_path)
        self.device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.num_classes = 20
        
        # ì „ì²˜ë¦¬ ì„¤ì • (ATR í‘œì¤€)
        self.transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def load_model(self):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë° êµ¬ì¡° ë³µì›"""
        try:
            # 1. ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            self.logger.info("ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(self.model_path, map_location='cpu')
            
            # 2. state_dict ì¶”ì¶œ
            if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                self.logger.info("âœ… state_dict ë°œê²¬")
            else:
                state_dict = checkpoint
                self.logger.info("âœ… ì§ì ‘ state_dict ì‚¬ìš©")
            
            # 3. ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ë° ì¬êµ¬ì„±
            self.logger.info("ğŸ” ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            reconstructor = ATRModelReconstructor(state_dict)
            self.model = reconstructor.build_model()
            self.num_classes = reconstructor.num_classes
            
            # 4. ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œë„
            self.logger.info("âš–ï¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì¤‘...")
            
            # í‚¤ ë§¤í•‘ (module. ì œê±°)
            new_state_dict = {}
            for key, value in state_dict.items():
                new_key = key.replace('module.', '') if key.startswith('module.') else key
                new_state_dict[new_key] = value
            
            # ëª¨ë¸ì— ë¡œë”©
            missing_keys, unexpected_keys = self.model.load_state_dict(new_state_dict, strict=False)
            
            self.logger.info(f"âœ… ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ë£Œ")
            self.logger.info(f"  ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
            self.logger.info(f"  ì˜ˆìƒì™¸ í‚¤: {len(unexpected_keys)}ê°œ")
            
            # 5. ëª¨ë¸ ì„¤ì •
            self.model.to(self.device)
            self.model.eval()
            
            self.logger.info(f"ğŸ¯ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {self.num_classes}ê°œ í´ë˜ìŠ¤")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def parse_image(self, image_input):
        """ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì‹± ìˆ˜í–‰"""
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # 1. ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
            if isinstance(image_input, str):
                image = Image.open(image_input).convert('RGB')
            elif isinstance(image_input, np.ndarray):
                image = Image.fromarray(image_input)
            elif isinstance(image_input, Image.Image):
                image = image.convert('RGB')
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
            
            original_size = image.size  # (W, H)
            
            # ì „ì²˜ë¦¬
            input_tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            # 2. ì‹¤ì œ AI ì¶”ë¡ 
            self.logger.info("ğŸ§  AI ì¶”ë¡  ì‹œì‘...")
            
            with torch.no_grad():
                output = self.model(input_tensor)  # [B, num_classes, H, W]
            
            # 3. í›„ì²˜ë¦¬
            # Softmax ì ìš©
            output_prob = F.softmax(output, dim=1)
            
            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ
            parsing_map = torch.argmax(output_prob, dim=1)  # [B, H, W]
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            parsing_map_resized = F.interpolate(
                parsing_map.float().unsqueeze(1), 
                size=original_size[::-1],  # (H, W)
                mode='nearest'
            ).squeeze().long()
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = torch.max(output_prob, dim=1)[0].mean().item()
            
            # CPUë¡œ ì´ë™
            parsing_map_cpu = parsing_map_resized.cpu().numpy()
            
            self.logger.info(f"âœ… ì¶”ë¡  ì™„ë£Œ: ì‹ ë¢°ë„ {confidence:.3f}")
            
            return {
                'parsing_map': parsing_map_cpu,
                'confidence': confidence,
                'num_classes': self.num_classes,
                'original_size': original_size
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def visualize_result(self, parsing_map, save_path=None):
        """íŒŒì‹± ê²°ê³¼ ì‹œê°í™”"""
        
        # ATR í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ë§¤í•‘
        colors = [
            [0, 0, 0],       # 0: background
            [128, 0, 0],     # 1: hat
            [255, 0, 0],     # 2: hair  
            [0, 85, 0],      # 3: glove
            [170, 0, 51],    # 4: sunglasses
            [255, 85, 0],    # 5: upper_clothes
            [0, 0, 85],      # 6: dress
            [0, 119, 221],   # 7: coat
            [85, 85, 0],     # 8: socks
            [0, 85, 85],     # 9: pants
            [85, 51, 0],     # 10: jumpsuits
            [52, 86, 128],   # 11: scarf
            [0, 128, 0],     # 12: skirt
            [0, 0, 255],     # 13: face
            [51, 170, 221],  # 14: left_arm
            [0, 255, 255],   # 15: right_arm
            [85, 255, 170],  # 16: left_leg
            [170, 255, 85],  # 17: right_leg
            [255, 255, 0],   # 18: left_foot
            [255, 170, 0]    # 19: right_foot
        ]
        
        # ìƒ‰ìƒ ë§¤í•‘ ì ìš©
        h, w = parsing_map.shape
        colored_map = np.zeros((h, w, 3), dtype=np.uint8)
        
        for class_id in range(min(self.num_classes, len(colors))):
            mask = (parsing_map == class_id)
            colored_map[mask] = colors[class_id]
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        result_image = Image.fromarray(colored_map)
        
        if save_path:
            result_image.save(save_path)
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {save_path}")
        
        return result_image

# ==============================================
# ğŸ”§ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ==============================================

def test_real_inference():
    """ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # ATR ëª¨ë¸ ë¡œë”©
    model_path = "ai_models/step_01_human_parsing/atr_model.pth"
    parser = RealHumanParsingInference(model_path)
    
    if not parser.load_model():
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        return
    
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    print("\nğŸ–¼ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±...")
    test_image = np.random.randint(0, 255, (512, 384, 3), dtype=np.uint8)
    
    # ì‹¤ì œ ì¶”ë¡ 
    print("\nğŸ§  ì‹¤ì œ AI ì¶”ë¡  ìˆ˜í–‰...")
    result = parser.parse_image(test_image)
    
    if result:
        print("âœ… ì¶”ë¡  ì„±ê³µ!")
        print(f"  ì‹ ë¢°ë„: {result['confidence']:.3f}")
        print(f"  í´ë˜ìŠ¤ ìˆ˜: {result['num_classes']}")
        print(f"  ê²°ê³¼ í¬ê¸°: {result['parsing_map'].shape}")
        print(f"  ê²€ì¶œëœ í´ë˜ìŠ¤: {np.unique(result['parsing_map'])}")
        
        # ì‹œê°í™”
        print("\nğŸ¨ ê²°ê³¼ ì‹œê°í™”...")
        vis_image = parser.visualize_result(result['parsing_map'], "parsing_result.png")
        print("âœ… ì‹œê°í™” ì™„ë£Œ!")
        
        return True
    else:
        print("âŒ ì¶”ë¡  ì‹¤íŒ¨")
        return False

if __name__ == "__main__":
    test_real_inference()