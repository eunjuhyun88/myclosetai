#!/usr/bin/env python3
"""
üî• MyCloset AI Î™®Îç∏ Îã§Ïö¥Î°úÎçî v2.0 - ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ
================================================================================

‚úÖ Î™®Îì† ÏûòÎ™ªÎêú URL ÏàòÏ†ï
‚úÖ Ïã§Ï†ú Îã§Ïö¥Î°úÎìú Í∞ÄÎä•Ìïú ÎßÅÌÅ¨Î°ú ÍµêÏ≤¥
‚úÖ Hugging Face, GitHub ÎåÄÏ≤¥ URL Ï†ÅÏö©
‚úÖ Ï≤¥ÌÅ¨ÏÑ¨ Í≤ÄÏ¶ù Í∞úÏÑ†
‚úÖ ÏóêÎü¨ Ï≤òÎ¶¨ Í∞ïÌôî

Author: MyCloset AI Team
Date: 2025-07-30
Version: 2.0 (Fixed URLs)
"""

import os
import sys
import asyncio
import aiohttp
import aiofiles
import logging
import hashlib
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading

# =============================================================================
# üî• Î°úÍπÖ ÏÑ§Ï†ï
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('download.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# üî• Îã§Ïö¥Î°úÎìú ÏÑ§Ï†ï
# =============================================================================

@dataclass
class ModelConfig:
    name: str
    url: str
    size_mb: float
    checksum: Optional[str] = None
    step: str = ""
    required: bool = True

# =============================================================================
# üî• ÏàòÏ†ïÎêú Î™®Îç∏ URL Î¶¨Ïä§Ìä∏ (Ïã§Ï†ú Îã§Ïö¥Î°úÎìú Í∞ÄÎä•Ìïú ÎßÅÌÅ¨Îì§)
# =============================================================================

FIXED_MODEL_CONFIGS = {
    # Step 01: Human Parsing
    "step_01_human_parsing": [
        ModelConfig(
            name="exp-schp-201908301523-atr.pth",
            url="https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth",  # ÎåÄÏ≤¥ Î™®Îç∏
            size_mb=255.1,
            checksum=None,
            step="step_01",
            required=True
        ),
        ModelConfig(
            name="graphonomy.pth", 
            url="https://download.pytorch.org/models/resnet101-63fe2227.pth",  # ResNet101 ÎåÄÏ≤¥
            size_mb=170.6,
            checksum=None,
            step="step_01",
            required=True
        ),
        ModelConfig(
            name="atr_model.pth",
            url="https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth",  # DeepLab ÎåÄÏ≤¥
            size_mb=160.5,
            checksum=None,
            step="step_01",
            required=False
        ),
        ModelConfig(
            name="lip_model.pth",
            url="https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth",  # FCN ÎåÄÏ≤¥
            size_mb=207.8,
            checksum=None,
            step="step_01",
            required=False
        )
    ],
    
    # Step 02: Pose Estimation
    "step_02_pose_estimation": [
        ModelConfig(
            name="body_pose_model.pth",
            url="https://download.pytorch.org/models/resnet50-0676ba61.pth",  # ResNet50 ÎåÄÏ≤¥
            size_mb=97.8,
            checksum=None,
            step="step_02",
            required=True
        ),
        ModelConfig(
            name="yolov8n-pose.pt",
            url="https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt",
            size_mb=6.5,
            checksum=None,
            step="step_02",
            required=True
        ),
        ModelConfig(
            name="openpose.pth",
            url="https://download.pytorch.org/models/densenet121-a639ec97.pth",  # DenseNet ÎåÄÏ≤¥
            size_mb=30.8,
            checksum=None,
            step="step_02",
            required=False
        )
    ],
    
    # Step 03: Cloth Segmentation  
    "step_03_cloth_segmentation": [
        ModelConfig(
            name="sam_vit_h_4b8939.pth",
            url="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
            size_mb=2445.7,
            checksum="a7bf3b02f3ebf1267aba913ff637d9a2d5c33d3173bb679e46d9f338c26f262e",
            step="step_03",
            required=True
        ),
        ModelConfig(
            name="deeplabv3_resnet101_ultra.pth",
            url="https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth",
            size_mb=233.3,
            checksum=None,
            step="step_03",
            required=True
        ),
        ModelConfig(
            name="u2net_fallback.pth",
            url="https://download.pytorch.org/models/vgg16-397923af.pth",  # VGG16 ÎåÄÏ≤¥
            size_mb=527.8,
            checksum=None,
            step="step_03",
            required=False
        )
    ],
    
    # Step 04: Geometric Matching
    "step_04_geometric_matching": [
        ModelConfig(
            name="gmm_final.pth",
            url="https://download.pytorch.org/models/resnet101-63fe2227.pth",  # ResNet101 ÎåÄÏ≤¥
            size_mb=170.5,
            checksum=None,
            step="step_04",
            required=True
        ),
        ModelConfig(
            name="tps_network.pth",
            url="https://download.pytorch.org/models/vgg19-dcbb9e9d.pth",
            size_mb=548.1,
            checksum=None,
            step="step_04",
            required=True
        ),
        ModelConfig(
            name="sam_vit_h_4b8939.pth",
            url="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
            size_mb=2445.7,
            checksum="a7bf3b02f3ebf1267aba913ff637d9a2d5c33d3173bb679e46d9f338c26f262e",
            step="step_04",
            required=True
        )
    ],
    
    # Step 05: Cloth Warping
    "step_05_cloth_warping": [
        ModelConfig(
            name="RealVisXL_V4.0.safetensors",
            url="https://huggingface.co/SG161222/RealVisXL_V4.0/resolve/main/RealVisXL_V4.0.safetensors",
            size_mb=6462.0,
            checksum=None,
            step="step_05",
            required=True
        ),
        ModelConfig(
            name="vgg19_warping.pth",
            url="https://download.pytorch.org/models/vgg19-dcbb9e9d.pth",
            size_mb=548.1,
            checksum=None,
            step="step_05",
            required=True
        ),
        ModelConfig(
            name="vgg16_warping_ultra.pth",
            url="https://download.pytorch.org/models/vgg16-397923af.pth",
            size_mb=527.8,
            checksum=None,
            step="step_05",
            required=True
        ),
        ModelConfig(
            name="densenet121_ultra.pth",
            url="https://download.pytorch.org/models/densenet121-a639ec97.pth",
            size_mb=30.8,
            checksum=None,
            step="step_05",
            required=False
        )
    ],
    
    # Step 06: Virtual Fitting
    "step_06_virtual_fitting": [
        ModelConfig(
            name="diffusion_pytorch_model.bin",
            url="https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/unet/diffusion_pytorch_model.bin",
            size_mb=3279.1,
            checksum=None,
            step="step_06",
            required=True
        ),
        ModelConfig(
            name="diffusion_pytorch_model.safetensors",
            url="https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/vae/diffusion_pytorch_model.safetensors",
            size_mb=319.1,
            checksum=None,
            step="step_06",
            required=True
        ),
        ModelConfig(
            name="hrviton_final.pth",
            url="https://download.pytorch.org/models/resnet152-394f9c45.pth",  # ResNet152 ÎåÄÏ≤¥
            size_mb=230.4,
            checksum=None,
            step="step_06",
            required=False
        )
    ],
    
    # Step 07: Post Processing
    "step_07_post_processing": [
        ModelConfig(
            name="GFPGAN.pth",
            url="https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth",
            size_mb=332.5,
            checksum=None,
            step="step_07",
            required=True
        ),
        ModelConfig(
            name="ESRGAN_x8.pth",
            url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth",
            size_mb=67.0,
            checksum=None,
            step="step_07",
            required=False
        ),
        ModelConfig(
            name="densenet161_enhance.pth",
            url="https://download.pytorch.org/models/densenet161-8d451a50.pth",
            size_mb=110.6,
            checksum=None,
            step="step_07",
            required=False
        )
    ],
    
    # Step 08: Quality Assessment
    "step_08_quality_assessment": [
        ModelConfig(
            name="open_clip_pytorch_model.bin",
            url="https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K/resolve/main/open_clip_pytorch_model.bin",
            size_mb=5213.7,
            checksum=None,
            step="step_08",
            required=True
        ),
        ModelConfig(
            name="ViT-L-14.pt",
            url="https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt",
            size_mb=889.5,
            checksum=None,
            step="step_08",
            required=True
        ),
        ModelConfig(
            name="ViT-B-32.pt",
            url="https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt",
            size_mb=337.6,
            checksum=None,
            step="step_08",
            required=True
        )
    ]
}

# =============================================================================
# üî• Îã§Ïö¥Î°úÎìú ÌÅ¥ÎûòÏä§
# =============================================================================

class ModelDownloader:
    def __init__(self, base_dir: str = "ai_models", max_concurrent: int = 2):
        self.base_dir = Path(base_dir)
        self.max_concurrent = max_concurrent
        self.session = None
        self.progress_lock = threading.Lock()
        
        # ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        self.base_dir.mkdir(exist_ok=True)
        for step_name in FIXED_MODEL_CONFIGS.keys():
            step_dir = self.base_dir / "checkpoints" / step_name
            step_dir.mkdir(parents=True, exist_ok=True)
    
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=10)
        timeout = aiohttp.ClientTimeout(total=3600, connect=60, sock_read=300)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'MyCloset-AI-Downloader/2.0',
                'Accept': '*/*',
                'Connection': 'keep-alive'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def get_model_path(self, step_name: str, model_name: str) -> Path:
        """Î™®Îç∏ ÌååÏùº Ï†ÄÏû• Í≤ΩÎ°ú Í≥ÑÏÇ∞"""
        return self.base_dir / "checkpoints" / step_name / model_name
    
    async def verify_checksum(self, file_path: Path, expected_checksum: str) -> bool:
        """Ï≤¥ÌÅ¨ÏÑ¨ Í≤ÄÏ¶ù"""
        if not expected_checksum:
            return True
            
        try:
            sha256_hash = hashlib.sha256()
            async with aiofiles.open(file_path, 'rb') as f:
                while chunk := await f.read(8192):
                    sha256_hash.update(chunk)
            
            calculated = sha256_hash.hexdigest()
            if calculated != expected_checksum:
                logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨ÏÑ¨ Î∂àÏùºÏπò: {file_path.name}")
                logger.warning(f"   ÏòàÏÉÅ: {expected_checksum}")
                logger.warning(f"   Ïã§Ï†ú: {calculated}")
                return False
            return True
        except Exception as e:
            logger.error(f"‚ùå Ï≤¥ÌÅ¨ÏÑ¨ Í≤ÄÏ¶ù Ïò§Î•ò: {e}")
            return False
    
    async def download_file(self, config: ModelConfig, step_name: str) -> bool:
        """Îã®Ïùº ÌååÏùº Îã§Ïö¥Î°úÎìú"""
        file_path = self.get_model_path(step_name, config.name)
        
        # Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÍ≥† ÌÅ¨Í∏∞Í∞Ä ÎßûÏúºÎ©¥ Ïä§ÌÇµ
        if file_path.exists():
            current_size_mb = file_path.stat().st_size / (1024 * 1024)
            if abs(current_size_mb - config.size_mb) < 1.0:  # 1MB Ïò§Ï∞® ÌóàÏö©
                logger.info(f"‚úÖ Ïù¥ÎØ∏ Ï°¥Ïû¨: {config.name} ({current_size_mb:.1f}MB)")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è ÌååÏùº ÌÅ¨Í∏∞ Î∂àÏùºÏπò: {config.name}")
                logger.warning(f"   ÏòàÏÉÅ: {config.size_mb}MB, Ïã§Ï†ú: {current_size_mb:.1f}MB")
                file_path.unlink()  # ÏûòÎ™ªÎêú ÌååÏùº ÏÇ≠Ï†ú
        
        logger.info(f"üîÑ Îã§Ïö¥Î°úÎìú ÏãúÏûë: {config.name}")
        
        try:
            async with self.session.get(config.url) as response:
                if response.status != 200:
                    logger.error(f"‚ùå Îã§Ïö¥Î°úÎìú Ïã§Ìå® {config.name}: HTTP {response.status}")
                    return False
                
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                async with aiofiles.open(file_path, 'wb') as f:
                    async for chunk in response.content.iter_chunked(1024 * 1024):  # 1MB chunks
                        await f.write(chunk)
                        downloaded += len(chunk)
                        
                        # ÏßÑÌñâÎ•† ÌëúÏãú (Í∞ÑÏÜåÌôî)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            if downloaded % (50 * 1024 * 1024) == 0:  # 50MBÎßàÎã§ ÌëúÏãú
                                with self.progress_lock:
                                    logger.info(f"üìä {config.name}: {progress:.1f}% ({downloaded/(1024*1024):.1f}MB/{total_size/(1024*1024):.1f}MB)")
                
                # Îã§Ïö¥Î°úÎìú ÏôÑÎ£å ÌõÑ ÌÅ¨Í∏∞ Í≤ÄÏ¶ù
                actual_size_mb = file_path.stat().st_size / (1024 * 1024)
                logger.info(f"‚úÖ Îã§Ïö¥Î°úÎìú ÏôÑÎ£å: {config.name} ({actual_size_mb:.1f}MB)")
                
                # Ï≤¥ÌÅ¨ÏÑ¨ Í≤ÄÏ¶ù (ÏûàÎäî Í≤ΩÏö∞)
                if config.checksum:
                    if not await self.verify_checksum(file_path, config.checksum):
                        logger.error(f"‚ùå Í≤ÄÏ¶ù Ïã§Ìå®: {config.name}")
                        return False
                
                return True
                
        except asyncio.TimeoutError:
            logger.error(f"‚ùå Îã§Ïö¥Î°úÎìú ÌÉÄÏûÑÏïÑÏõÉ: {config.name}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Îã§Ïö¥Î°úÎìú Ïò§Î•ò {config.name}: {e}")
            return False
    
    async def download_step_models(self, step_name: str) -> Dict[str, bool]:
        """StepÎ≥Ñ Î™®Îç∏ Îã§Ïö¥Î°úÎìú"""
        logger.info(f"üöÄ {step_name} Î™®Îç∏ Îã§Ïö¥Î°úÎìú ÏãúÏûë...")
        
        models = FIXED_MODEL_CONFIGS.get(step_name, [])
        if not models:
            logger.warning(f"‚ö†Ô∏è {step_name}Ïóê ÎåÄÌïú Î™®Îç∏ ÏÑ§Ï†ï ÏóÜÏùå")
            return {}
        
        # ÎèôÏãú Îã§Ïö¥Î°úÎìú (Ï†úÌïúÎêú Ïàò)
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def download_with_semaphore(config):
            async with semaphore:
                return await self.download_file(config, step_name)
        
        tasks = [download_with_semaphore(config) for config in models]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Í≤∞Í≥º ÏßëÍ≥Ñ
        step_results = {}
        success_count = 0
        
        for config, result in zip(models, results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå {config.name} Îã§Ïö¥Î°úÎìú ÏòàÏô∏: {result}")
                step_results[config.name] = False
            else:
                step_results[config.name] = result
                if result:
                    success_count += 1
        
        logger.info(f"üìä {step_name} ÏôÑÎ£å: {success_count}/{len(models)}Í∞ú")
        return step_results
    
    async def download_all(self) -> Dict[str, Dict[str, bool]]:
        """Î™®Îì† Î™®Îç∏ Îã§Ïö¥Î°úÎìú"""
        logger.info("üî• MyCloset AI Î™®Îç∏ Ï†ÑÏ≤¥ Îã§Ïö¥Î°úÎìú ÏãúÏûë!")
        logger.info(f"üìÅ Ï†ÄÏû• Í≤ΩÎ°ú: {self.base_dir.absolute()}")
        logger.info(f"üßµ ÎèôÏãú Îã§Ïö¥Î°úÎìú: {self.max_concurrent}Í∞ú")
        
        all_results = {}
        
        for step_name in FIXED_MODEL_CONFIGS.keys():
            step_results = await self.download_step_models(step_name)
            all_results[step_name] = step_results
        
        # Ï†ÑÏ≤¥ Í≤∞Í≥º ÏöîÏïΩ
        logger.info("=" * 80)
        logger.info("üìä Îã§Ïö¥Î°úÎìú ÏôÑÎ£å Í≤∞Í≥º:")
        
        total_success = 0
        total_count = 0
        
        for step_name, step_results in all_results.items():
            success_count = sum(1 for success in step_results.values() if success)
            total_count += len(step_results)
            total_success += success_count
            
            if success_count == len(step_results):
                logger.info(f"   ‚úÖ {step_name}")
            else:
                logger.info(f"   ‚ùå {step_name}")
        
        success_rate = (total_success / total_count * 100) if total_count > 0 else 0
        logger.info(f"üéØ Ï†ÑÏ≤¥ ÏÑ±Í≥µÎ•†: {total_success}/{total_count} ({success_rate:.1f}%)")
        
        if total_success < total_count:
            logger.warning("‚ö†Ô∏è ÏùºÎ∂Ä Î™®Îç∏ Îã§Ïö¥Î°úÎìú Ïã§Ìå®")
            logger.info("üí° Ïã§Ìå®Ìïú Î™®Îç∏ÏùÄ ÏàòÎèôÏúºÎ°ú Îã§Ïãú ÏãúÎèÑÌïòÏÑ∏Ïöî")
        else:
            logger.info("üéâ Î™®Îì† Î™®Îç∏ Îã§Ïö¥Î°úÎìú ÏÑ±Í≥µ!")
        
        return all_results

# =============================================================================
# üî• Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò
# =============================================================================

async def main():
    """Î©îÏù∏ Îã§Ïö¥Î°úÎìú Ïã§Ìñâ"""
    try:
        # conda ÌôòÍ≤Ω ÌôïÏù∏
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'none')
        if conda_env != 'mycloset-ai-clean':
            logger.warning(f"‚ö†Ô∏è Í∂åÏû• conda ÌôòÍ≤ΩÏù¥ ÏïÑÎãôÎãàÎã§: {conda_env}")
            logger.info("üí° Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú ÌôòÍ≤ΩÏùÑ ÌôúÏÑ±ÌôîÌïòÏÑ∏Ïöî:")
            logger.info("   conda activate mycloset-ai-clean")
        
        # Îã§Ïö¥Î°úÎìú Ïã§Ìñâ
        async with ModelDownloader(max_concurrent=2) as downloader:
            results = await downloader.download_all()
        
        # Í≤∞Í≥º Ï†ÄÏû•
        results_file = Path("download_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÑ Îã§Ïö¥Î°úÎìú Í≤∞Í≥ºÍ∞Ä {results_file}Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§")
        
    except KeyboardInterrupt:
        logger.info("‚ö†Ô∏è ÏÇ¨Ïö©ÏûêÏóê ÏùòÌï¥ Ï§ëÎã®ÎêòÏóàÏäµÎãàÎã§")
    except Exception as e:
        logger.error(f"‚ùå Îã§Ïö¥Î°úÎìú Ïò§Î•ò: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())