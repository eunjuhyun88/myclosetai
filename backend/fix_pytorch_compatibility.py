#!/usr/bin/env python3
"""
ğŸ”§ PyTorch 2.7 í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° + ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
backend/fix_pytorch_compatibility.py

âœ… PyTorch 2.7ì˜ weights_only=True ê¸°ë³¸ê°’ ë¬¸ì œ í•´ê²°
âœ… Legacy .tar í˜•ì‹ ëª¨ë¸ ì•ˆì „ ë¡œë”©
âœ… TorchScript ì•„ì¹´ì´ë¸Œ í˜¸í™˜ì„± í•´ê²°
âœ… ì‹¤ì œ AI ëª¨ë¸ ë©”ëª¨ë¦¬ ë¡œë”© í…ŒìŠ¤íŠ¸
âœ… ì†ìƒëœ ëª¨ë¸ íŒŒì¼ ê°ì§€ ë° ë³µêµ¬
"""

import sys
import os
import time
import traceback
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import warnings

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

# =============================================================================
# ğŸ”¥ 1. PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜
# =============================================================================

def apply_pytorch_compatibility_patches():
    """PyTorch 2.7 í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©"""
    
    print("ğŸ”§ PyTorch 2.7 í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì¤‘...")
    
    try:
        import torch
        print(f"   ğŸ”¥ PyTorch {torch.__version__} ê°ì§€ë¨")
        
        # 1. ì „ì—­ ê¸°ë³¸ê°’ ì„¤ì •
        if hasattr(torch, 'serialization'):
            # PyTorch 2.7ì—ì„œ weights_only=Trueê°€ ê¸°ë³¸ê°’ì´ ëœ ê²ƒì„ ìš°íšŒ
            original_load = torch.load
            
            def patched_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
                """ì•ˆì „í•œ PyTorch ë¡œë”© í•¨ìˆ˜"""
                # weights_onlyê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ Falseë¡œ ì„¤ì • (ì•ˆì „ì„±ë³´ë‹¤ í˜¸í™˜ì„± ìš°ì„ )
                if weights_only is None:
                    weights_only = False
                
                try:
                    return original_load(f, map_location=map_location, 
                                       pickle_module=pickle_module, 
                                       weights_only=weights_only, **kwargs)
                except Exception as e:
                    error_msg = str(e).lower()
                    
                    # Legacy .tar í˜•ì‹ ì˜¤ë¥˜ ì²˜ë¦¬
                    if "weights_only" in error_msg and "legacy" in error_msg:
                        print(f"      âš ï¸ Legacy í˜•ì‹ ê°ì§€, weights_only=Falseë¡œ ì¬ì‹œë„: {Path(f).name if hasattr(f, '__fspath__') else str(f)}")
                        return original_load(f, map_location=map_location, 
                                           pickle_module=pickle_module, 
                                           weights_only=False, **kwargs)
                    
                    # TorchScript ì˜¤ë¥˜ ì²˜ë¦¬
                    elif "torchscript" in error_msg:
                        print(f"      ğŸ”§ TorchScript ê°ì§€, torch.jit.load ì‚¬ìš©: {Path(f).name if hasattr(f, '__fspath__') else str(f)}")
                        return torch.jit.load(f, map_location=map_location)
                    
                    # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
                    else:
                        raise e
            
            # íŒ¨ì¹˜ ì ìš©
            torch.load = patched_load
            print("   âœ… torch.load í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©ë¨")
        
        # 2. ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
        warnings.filterwarnings("ignore", category=UserWarning, 
                              message=".*TorchScript archive.*torch.jit.load.*")
        warnings.filterwarnings("ignore", category=UserWarning,
                              message=".*weights_only.*")
        
        print("   âœ… ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§ ì™„ë£Œ")
        
        return True
        
    except ImportError:
        print("   âŒ PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False
    except Exception as e:
        print(f"   âŒ íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# ğŸ”¥ 2. ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤í„°
# =============================================================================

class RealModelLoadingTester:
    """ì‹¤ì œ AI ëª¨ë¸ ë©”ëª¨ë¦¬ ë¡œë”© í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.failed_models = []
        self.successful_models = []
        self.memory_usage = {}
        
    def test_critical_models(self) -> Dict[str, Any]:
        """í•µì‹¬ AI ëª¨ë¸ë“¤ ì‹¤ì œ ë¡œë”© í…ŒìŠ¤íŠ¸"""
        
        print("\nğŸ§  í•µì‹¬ AI ëª¨ë¸ ì‹¤ì œ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # í•µì‹¬ ëª¨ë¸ ê²½ë¡œë“¤
        critical_models = {
            "graphonomy": "ai_models/step_01_human_parsing/graphonomy.pth",
            "sam_vit_h": "ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth", 
            "realvis_xl": "ai_models/step_05_cloth_warping/RealVisXL_V4.0.safetensors",
            "vit_clip": "ai_models/step_08_quality_assessment/ViT-L-14.pt",
            "hrviton": "ai_models/step_05_cloth_warping/hrviton_final.pth"
        }
        
        results = {
            'total_tested': len(critical_models),
            'successful_loads': 0,
            'failed_loads': 0,
            'model_details': {},
            'total_memory_mb': 0.0
        }
        
        for model_name, model_path in critical_models.items():
            print(f"\nğŸ”§ {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            result = self._test_single_model(model_name, Path(model_path))
            results['model_details'][model_name] = result
            
            if result['loaded_successfully']:
                results['successful_loads'] += 1
                results['total_memory_mb'] += result.get('memory_usage_mb', 0)
            else:
                results['failed_loads'] += 1
        
        return results
    
    def _test_single_model(self, model_name: str, model_path: Path) -> Dict[str, Any]:
        """ê°œë³„ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
        
        result = {
            'model_name': model_name,
            'file_path': str(model_path),
            'file_exists': False,
            'file_size_mb': 0.0,
            'loaded_successfully': False,
            'memory_usage_mb': 0.0,
            'load_time_seconds': 0.0,
            'model_structure': {},
            'error_message': None
        }
        
        try:
            # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not model_path.exists():
                result['error_message'] = f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_path}"
                print(f"   âŒ íŒŒì¼ ì—†ìŒ: {model_path}")
                return result
            
            result['file_exists'] = True
            result['file_size_mb'] = model_path.stat().st_size / (1024 * 1024)
            print(f"   ğŸ“ íŒŒì¼ í¬ê¸°: {result['file_size_mb']:.1f}MB")
            
            # 2. ì‹¤ì œ ë©”ëª¨ë¦¬ ë¡œë”© ì‹œë„
            import torch
            import psutil
            
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            print(f"   ğŸ”„ ë©”ëª¨ë¦¬ ë¡œë”© ì¤‘...")
            
            # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë”©
            checkpoint = None
            if model_path.suffix == '.safetensors':
                try:
                    from safetensors.torch import load_file
                    checkpoint = load_file(model_path)
                    print(f"   âœ… SafeTensors ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    result['error_message'] = f"SafeTensors ë¡œë”© ì‹¤íŒ¨: {e}"
                    print(f"   âŒ SafeTensors ë¡œë”© ì‹¤íŒ¨: {e}")
                    return result
            else:
                # .pth, .pt íŒŒì¼
                try:
                    checkpoint = torch.load(model_path, map_location='cpu')
                    print(f"   âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    result['error_message'] = f"PyTorch ë¡œë”© ì‹¤íŒ¨: {e}"
                    print(f"   âŒ PyTorch ë¡œë”© ì‹¤íŒ¨: {e}")
                    return result
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            result['loaded_successfully'] = True
            result['load_time_seconds'] = end_time - start_time
            result['memory_usage_mb'] = end_memory - start_memory
            
            # 4. ëª¨ë¸ êµ¬ì¡° ë¶„ì„
            if checkpoint is not None:
                self._analyze_model_structure(checkpoint, result)
            
            print(f"   âœ… ë¡œë”© ì„±ê³µ ({result['load_time_seconds']:.2f}ì´ˆ, +{result['memory_usage_mb']:.1f}MB)")
            
            return result
            
        except Exception as e:
            result['error_message'] = f"ì˜ˆì™¸ ë°œìƒ: {e}"
            print(f"   âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
            return result
    
    def _analyze_model_structure(self, checkpoint: Any, result: Dict[str, Any]):
        """ëª¨ë¸ êµ¬ì¡° ë¶„ì„"""
        try:
            if isinstance(checkpoint, dict):
                # state_dict ì°¾ê¸°
                state_dict = checkpoint
                if 'model' in checkpoint:
                    state_dict = checkpoint['model']
                elif 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                
                if isinstance(state_dict, dict):
                    result['model_structure'] = {
                        'total_parameters': len(state_dict),
                        'layer_types': list(set(key.split('.')[0] for key in state_dict.keys() if '.' in key))[:10],
                        'has_bias': any('bias' in key for key in state_dict.keys()),
                        'has_weight': any('weight' in key for key in state_dict.keys())
                    }
                    
                    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •
                    total_params = 0
                    for tensor in state_dict.values():
                        if hasattr(tensor, 'numel'):
                            total_params += tensor.numel()
                    
                    result['model_structure']['estimated_parameters'] = total_params
                    print(f"      ğŸ“Š íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
                    
        except Exception as e:
            result['model_structure']['analysis_error'] = str(e)

# =============================================================================
# ğŸ”¥ 3. Stepë³„ ì‹¤ì œ ëª¨ë¸ ë¡œë”© ê°•ì œ í…ŒìŠ¤íŠ¸
# =============================================================================

class StepModelLoadingForcer:
    """Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ê°•ì œ ë¡œë”©"""
    
    def __init__(self):
        self.step_results = {}
    
    def force_load_step_models(self) -> Dict[str, Any]:
        """ëª¨ë“  Stepì˜ AI ëª¨ë¸ ê°•ì œ ë¡œë”©"""
        
        print("\nğŸš€ Stepë³„ AI ëª¨ë¸ ê°•ì œ ë¡œë”© í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        results = {
            'total_steps': 0,
            'steps_with_models': 0,
            'total_models_loaded': 0,
            'step_details': {}
        }
        
        # Stepë³„ í…ŒìŠ¤íŠ¸
        steps_to_test = [
            ('HumanParsingStep', 'app.ai_pipeline.steps.step_01_human_parsing'),
            ('PoseEstimationStep', 'app.ai_pipeline.steps.step_02_pose_estimation'),
            ('ClothSegmentationStep', 'app.ai_pipeline.steps.step_03_cloth_segmentation'),
            ('GeometricMatchingStep', 'app.ai_pipeline.steps.step_04_geometric_matching')
        ]
        
        for step_name, module_path in steps_to_test:
            print(f"\nğŸ”§ {step_name} ê°•ì œ ë¡œë”© í…ŒìŠ¤íŠ¸...")
            step_result = self._force_load_step(step_name, module_path)
            results['step_details'][step_name] = step_result
            results['total_steps'] += 1
            
            if step_result['models_loaded'] > 0:
                results['steps_with_models'] += 1
                results['total_models_loaded'] += step_result['models_loaded']
        
        return results
    
    def _force_load_step(self, step_name: str, module_path: str) -> Dict[str, Any]:
        """ê°œë³„ Step ê°•ì œ ë¡œë”©"""
        
        result = {
            'step_name': step_name,
            'import_success': False,
            'instance_created': False,
            'models_loaded': 0,
            'forced_model_loading': False,
            'errors': []
        }
        
        try:
            # 1. Import
            module = __import__(module_path, fromlist=[step_name])
            step_class = getattr(module, step_name)
            result['import_success'] = True
            print(f"   âœ… Import ì„±ê³µ")
            
            # 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            step_instance = step_class(device='cpu', strict_mode=False)
            result['instance_created'] = True
            print(f"   âœ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
            
            # 3. ê°•ì œ ëª¨ë¸ ë¡œë”© ì‹œë„
            models_loaded = self._attempt_force_model_loading(step_instance)
            result['models_loaded'] = models_loaded
            
            if models_loaded > 0:
                result['forced_model_loading'] = True
                print(f"   ğŸ”¥ ê°•ì œ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {models_loaded}ê°œ")
            else:
                print(f"   âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ëª¨ë¸ ì—†ìŒ")
            
        except Exception as e:
            result['errors'].append(str(e))
            print(f"   âŒ ì˜¤ë¥˜: {e}")
        
        return result
    
    def _attempt_force_model_loading(self, step_instance) -> int:
        """ê°•ì œ ëª¨ë¸ ë¡œë”© ì‹œë„"""
        
        models_loaded = 0
        
        # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì‹œë„
        loading_methods = [
            '_load_models',
            'load_models', 
            '_initialize_models',
            'initialize_models',
            '_setup_models',
            'setup_models'
        ]
        
        for method_name in loading_methods:
            if hasattr(step_instance, method_name):
                try:
                    method = getattr(step_instance, method_name)
                    if callable(method):
                        print(f"      ğŸ”„ {method_name}() í˜¸ì¶œ ì¤‘...")
                        result = method()
                        if result:
                            models_loaded += 1
                            print(f"      âœ… {method_name}() ì„±ê³µ")
                        else:
                            print(f"      âš ï¸ {method_name}() False ë°˜í™˜")
                except Exception as e:
                    print(f"      âŒ {method_name}() ì‹¤íŒ¨: {e}")
        
        # ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹œë„
        if hasattr(step_instance, 'set_model_loader'):
            try:
                print(f"      ğŸ”„ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹œë„...")
                # ê°„ë‹¨í•œ ë”ë¯¸ ModelLoader ìƒì„±
                class DummyModelLoader:
                    def load_model(self, *args, **kwargs):
                        return True
                
                step_instance.set_model_loader(DummyModelLoader())
                models_loaded += 1
                print(f"      âœ… ModelLoader ì£¼ì… ì„±ê³µ")
            except Exception as e:
                print(f"      âŒ ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
        
        return models_loaded

# =============================================================================
# ğŸ”¥ 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ”§ PyTorch í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° + ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.WARNING)
    
    try:
        # 1. PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©
        print("\nğŸ“‹ 1ë‹¨ê³„: PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜")
        patch_success = apply_pytorch_compatibility_patches()
        
        if not patch_success:
            print("âŒ PyTorch íŒ¨ì¹˜ ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            return
        
        # 2. í•µì‹¬ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        print("\nğŸ“‹ 2ë‹¨ê³„: í•µì‹¬ AI ëª¨ë¸ ì‹¤ì œ ë¡œë”© í…ŒìŠ¤íŠ¸")
        model_tester = RealModelLoadingTester()
        model_results = model_tester.test_critical_models()
        
        # 3. Stepë³„ ê°•ì œ ë¡œë”© í…ŒìŠ¤íŠ¸
        print("\nğŸ“‹ 3ë‹¨ê³„: Stepë³„ AI ëª¨ë¸ ê°•ì œ ë¡œë”©")
        step_forcer = StepModelLoadingForcer()
        step_results = step_forcer.force_load_step_models()
        
        # 4. ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        
        # ëª¨ë¸ ë¡œë”© ê²°ê³¼
        print(f"\nğŸ§  í•µì‹¬ ëª¨ë¸ ë¡œë”© ê²°ê³¼:")
        print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸: {model_results['total_tested']}ê°œ")
        print(f"   âœ… ì„±ê³µí•œ ëª¨ë¸: {model_results['successful_loads']}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨í•œ ëª¨ë¸: {model_results['failed_loads']}ê°œ")
        print(f"   ğŸ’¾ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©: {model_results['total_memory_mb']:.1f}MB")
        
        # ê°œë³„ ëª¨ë¸ ìƒì„¸
        print(f"\n   ğŸ“‹ ê°œë³„ ëª¨ë¸ ìƒì„¸:")
        for model_name, details in model_results['model_details'].items():
            status = "âœ…" if details['loaded_successfully'] else "âŒ"
            size = details['file_size_mb']
            print(f"      {status} {model_name}: {size:.1f}MB")
            if details['error_message']:
                print(f"         ì˜¤ë¥˜: {details['error_message']}")
        
        # Step ë¡œë”© ê²°ê³¼
        print(f"\nğŸš€ Stepë³„ ê°•ì œ ë¡œë”© ê²°ê³¼:")
        print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ëœ Step: {step_results['total_steps']}ê°œ")
        print(f"   ğŸ”¥ ëª¨ë¸ ë¡œë”© ì„±ê³µ Step: {step_results['steps_with_models']}ê°œ")
        print(f"   ğŸ§  ì´ ë¡œë”©ëœ ëª¨ë¸: {step_results['total_models_loaded']}ê°œ")
        
        # ì¶”ì²œì‚¬í•­
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        
        success_rate = (model_results['successful_loads'] / model_results['total_tested']) * 100
        if success_rate < 50:
            print(f"   âŒ í•µì‹¬ ëª¨ë¸ ë¡œë”© ì„±ê³µë¥  ë§¤ìš° ë‚®ìŒ ({success_rate:.1f}%) - ì¦‰ì‹œ ìˆ˜ì • í•„ìš”")
        elif success_rate < 80:
            print(f"   âš ï¸ ì¼ë¶€ í•µì‹¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({success_rate:.1f}%) - í™•ì¸ í•„ìš”")
        else:
            print(f"   âœ… ëŒ€ë¶€ë¶„ í•µì‹¬ ëª¨ë¸ ë¡œë”© ì„±ê³µ ({success_rate:.1f}%)")
        
        if step_results['total_models_loaded'] == 0:
            print(f"   ğŸš¨ **ì¤‘ìš”**: ëª¨ë“  Stepì—ì„œ ì‹¤ì œ AI ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì§€ ì•ŠìŒ")
            print(f"      - Step ì´ˆê¸°í™”ëŠ” ì„±ê³µí–ˆì§€ë§Œ ì‹¤ì œ ì¶”ë¡  ë¶ˆê°€ëŠ¥í•œ ìƒíƒœ")
            print(f"      - ModelLoader ì˜ì¡´ì„± ì£¼ì… ë˜ëŠ” ëª¨ë¸ ê²½ë¡œ ë¬¸ì œë¡œ ì¶”ì •")
        else:
            print(f"   âœ… ì¼ë¶€ Stepì—ì„œ ëª¨ë¸ ë¡œë”© í™•ì¸ë¨")
        
        print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. ì‹¤íŒ¨í•œ í•µì‹¬ ëª¨ë¸ë“¤ì˜ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬")
        print(f"   2. PyTorch ë²„ì „ ë‹¤ìš´ê·¸ë ˆì´ë“œ ê³ ë ¤ (2.6.x)")
        print(f"   3. ModelLoaderì™€ Step ê°„ ì˜ì¡´ì„± ì£¼ì… ë¬¸ì œ í•´ê²°")
        print(f"   4. ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        
        return {
            'model_results': model_results,
            'step_results': step_results,
            'overall_success': success_rate > 70 and step_results['total_models_loaded'] > 0
        }
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None

if __name__ == "__main__":
    main()