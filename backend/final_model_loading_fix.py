#!/usr/bin/env python3
"""
ğŸ¯ ìµœì¢… ëª¨ë¸ ë¡œë”© ë¬¸ì œ í•´ê²° ìŠ¤í¬ë¦½íŠ¸
ì™„ì „í•œ ê°€ì¤‘ì¹˜ ë¡œë”©ê³¼ ëª¨ë¸ íŒŒì¼ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def fix_human_parsing_model():
    """Human Parsing ëª¨ë¸ ë¬¸ì œ í•´ê²°"""
    print("ğŸ”§ Human Parsing ëª¨ë¸ ë¬¸ì œ í•´ê²°...")
    
    try:
        import torch
        
        # PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜
        original_load = torch.load
        
        def safe_load_with_fallback(*args, **kwargs):
            """ì•ˆì „í•œ ë¡œë”© with í´ë°±"""
            try:
                # weights_only=Trueë¡œ ì‹œë„
                kwargs['weights_only'] = True
                return original_load(*args, **kwargs)
            except Exception as e1:
                try:
                    # weights_only ì œê±°
                    kwargs.pop('weights_only', None)
                    return original_load(*args, **kwargs)
                except Exception as e2:
                    try:
                        # pickle_loadë¡œ ì‹œë„
                        return original_load(*args, **kwargs, pickle_module=torch._utils._rebuild_tensor_v2)
                    except Exception as e3:
                        try:
                            # ì™„ì „í•œ í´ë°±
                            return original_load(*args, **kwargs, map_location='cpu')
                        except Exception as e4:
                            print(f"   âŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {e4}")
                            return None
        
        torch.load = safe_load_with_fallback
        
        # Human Parsing ëª¨ë¸ í…ŒìŠ¤íŠ¸
        checkpoint_path = 'ai_models/step_01_human_parsing/graphonomy.pth'
        if os.path.exists(checkpoint_path):
            print(f"   ğŸ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸: {checkpoint_path}")
            
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if checkpoint:
                    print(f"   âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    
                    # State dict ì¶”ì¶œ
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    else:
                        state_dict = checkpoint
                    
                    print(f"   ğŸ“Š State dict í‚¤ ìˆ˜: {len(state_dict)}")
                    
                    # í‚¤ íŒ¨í„´ ë¶„ì„
                    key_patterns = {}
                    for key in state_dict.keys():
                        parts = key.split('.')
                        if len(parts) > 0:
                            prefix = parts[0]
                            key_patterns[prefix] = key_patterns.get(prefix, 0) + 1
                    
                    print(f"   ğŸ·ï¸ ì£¼ìš” í‚¤ íŒ¨í„´:")
                    for pattern, count in sorted(key_patterns.items(), key=lambda x: x[1], reverse=True)[:5]:
                        print(f"      - {pattern}: {count}ê°œ")
                    
                    return True
                else:
                    print(f"   âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨")
                    return False
                    
            except Exception as e:
                print(f"   âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì˜¤ë¥˜: {e}")
                return False
        else:
            print(f"   âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
            return False
            
    except Exception as e:
        print(f"   âŒ Human Parsing ëª¨ë¸ ìˆ˜ì • ì‹¤íŒ¨: {e}")
        return False

def improve_key_mapping_algorithm():
    """í‚¤ ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜ ëŒ€í­ ê°œì„ """
    print("\nğŸ”‘ í‚¤ ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜ ëŒ€í­ ê°œì„ ...")
    
    try:
        from app.ai_pipeline.utils.model_loader import KeyMapper
        
        def enhanced_map_keys_v2(self, checkpoint, target_architecture, model_state_dict):
            """ëŒ€í­ ê°œì„ ëœ í‚¤ ë§¤í•‘ v2"""
            try:
                # State dict ì¶”ì¶œ
                if 'state_dict' in checkpoint:
                    source_state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    source_state_dict = checkpoint['model']
                else:
                    source_state_dict = checkpoint
                
                # 1ë‹¨ê³„: ì •í™• ë§¤ì¹­
                exact_matches = {}
                for source_key, source_value in source_state_dict.items():
                    if source_key in model_state_dict:
                        if hasattr(source_value, 'shape') and hasattr(model_state_dict[source_key], 'shape'):
                            if source_value.shape == model_state_dict[source_key].shape:
                                exact_matches[source_key] = source_value
                
                # 2ë‹¨ê³„: í‚¤ ì •ê·œí™” ë§¤ì¹­
                def normalize_key(key):
                    # ë” ì •êµí•œ ì •ê·œí™”
                    normalized = key.lower()
                    normalized = normalized.replace('module.', '').replace('model.', '').replace('net.', '')
                    normalized = normalized.replace('_', '').replace('.', '').replace('-', '')
                    normalized = normalized.replace('backbone', '').replace('features', '')
                    return normalized
                
                normalized_source = {}
                for key, value in source_state_dict.items():
                    norm_key = normalize_key(key)
                    normalized_source[norm_key] = (key, value)
                
                normalized_target = {}
                for key in model_state_dict.keys():
                    norm_key = normalize_key(key)
                    normalized_target[norm_key] = key
                
                # ì •ê·œí™”ëœ ë§¤ì¹­
                normalized_matches = {}
                for norm_source_key, (orig_source_key, source_value) in normalized_source.items():
                    if norm_source_key in normalized_target:
                        target_key = normalized_target[norm_source_key]
                        if hasattr(source_value, 'shape') and hasattr(model_state_dict[target_key], 'shape'):
                            if source_value.shape == model_state_dict[target_key].shape:
                                normalized_matches[target_key] = source_value
                
                # 3ë‹¨ê³„: ë¶€ë¶„ ë§¤ì¹­
                partial_matches = {}
                for norm_source_key, (orig_source_key, source_value) in normalized_source.items():
                    for norm_target_key, target_key in normalized_target.items():
                        if (norm_source_key in norm_target_key or norm_target_key in norm_source_key):
                            if hasattr(source_value, 'shape') and hasattr(model_state_dict[target_key], 'shape'):
                                if source_value.shape == model_state_dict[target_key].shape:
                                    partial_matches[target_key] = source_value
                                    break
                
                # 4ë‹¨ê³„: í…ì„œ í¬ê¸° ê¸°ë°˜ ë§¤ì¹­
                size_matches = {}
                source_sizes = {}
                for key, value in source_state_dict.items():
                    if hasattr(value, 'shape'):
                        size_str = 'x'.join(map(str, value.shape))
                        if size_str not in source_sizes:
                            source_sizes[size_str] = []
                        source_sizes[size_str].append((key, value))
                
                target_sizes = {}
                for key, value in model_state_dict.items():
                    if hasattr(value, 'shape'):
                        size_str = 'x'.join(map(str, value.shape))
                        if size_str not in target_sizes:
                            target_sizes[size_str] = []
                        target_sizes[size_str].append(key)
                
                for size_str, source_items in source_sizes.items():
                    if size_str in target_sizes and len(source_items) == len(target_sizes[size_str]):
                        for i, (source_key, source_value) in enumerate(source_items):
                            target_key = target_sizes[size_str][i]
                            size_matches[target_key] = source_value
                
                # ëª¨ë“  ë§¤ì¹­ ê²°ê³¼ í•©ì¹˜ê¸°
                final_matches = {}
                final_matches.update(exact_matches)
                final_matches.update(normalized_matches)
                final_matches.update(partial_matches)
                final_matches.update(size_matches)
                
                success_count = len(final_matches)
                total_count = len(model_state_dict)
                
                print(f"   ğŸ“Š ê°œì„ ëœ ë§¤í•‘ ê²°ê³¼:")
                print(f"      - ì •í™• ë§¤ì¹­: {len(exact_matches)}ê°œ")
                print(f"      - ì •ê·œí™” ë§¤ì¹­: {len(normalized_matches)}ê°œ")
                print(f"      - ë¶€ë¶„ ë§¤ì¹­: {len(partial_matches)}ê°œ")
                print(f"      - í¬ê¸° ë§¤ì¹­: {len(size_matches)}ê°œ")
                print(f"      - ì´ ì„±ê³µ: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
                
                return final_matches
                
            except Exception as e:
                print(f"   âŒ ê°œì„ ëœ ë§¤í•‘ ì‹¤íŒ¨: {e}")
                return {}
        
        # KeyMapperì— ê°œì„ ëœ ë©”ì„œë“œ ì¶”ê°€
        KeyMapper.enhanced_map_keys_v2 = enhanced_map_keys_v2
        print("   âœ… í‚¤ ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜ ëŒ€í­ ê°œì„  ì™„ë£Œ")
        
    except Exception as e:
        print(f"   âŒ í‚¤ ë§¤í•‘ ê°œì„  ì‹¤íŒ¨: {e}")

def test_improved_loading():
    """ê°œì„ ëœ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ê°œì„ ëœ ë¡œë”© í…ŒìŠ¤íŠ¸...")
    
    try:
        from app.ai_pipeline.utils.model_loader import ModelLoader
        
        loader = ModelLoader()
        
        test_cases = [
            ('human_parsing', 'ai_models/step_01_human_parsing/graphonomy.pth'),
            ('pose_estimation', 'ai_models/step_02_pose_estimation/hrnet_w48_coco_384x288.pth'),
            ('cloth_segmentation', 'ai_models/step_03/sam.pth')
        ]
        
        results = {}
        
        for step, path in test_cases:
            if os.path.exists(path):
                print(f"\n   ğŸ”§ {step} ê°œì„ ëœ ë¡œë”© í…ŒìŠ¤íŠ¸...")
                try:
                    model = loader.load_model_for_step(step, checkpoint_path=path)
                    if model:
                        param_count = sum(p.numel() for p in model.parameters())
                        print(f"      âœ… ë¡œë”© ì„±ê³µ: {param_count:,} íŒŒë¼ë¯¸í„°")
                        results[step] = True
                    else:
                        print(f"      âŒ ë¡œë”© ì‹¤íŒ¨")
                        results[step] = False
                except Exception as e:
                    print(f"      âŒ ë¡œë”© ì˜¤ë¥˜: {e}")
                    results[step] = False
            else:
                print(f"   âš ï¸ {step}: íŒŒì¼ ì—†ìŒ ({path})")
                results[step] = False
        
        # ê²°ê³¼ ìš”ì•½
        success_count = sum(1 for success in results.values() if success)
        total_count = len(results)
        
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"   - ì„±ê³µ: {success_count}/{total_count}")
        print(f"   - ì„±ê³µë¥ : {success_count/total_count*100:.1f}%")
        
        return results
        
    except Exception as e:
        print(f"   âŒ ê°œì„ ëœ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {}

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ ìµœì¢… ëª¨ë¸ ë¡œë”© ë¬¸ì œ í•´ê²° ì‹œì‘...")
    print("=" * 60)
    
    # 1. Human Parsing ëª¨ë¸ ë¬¸ì œ í•´ê²°
    human_parsing_fixed = fix_human_parsing_model()
    
    # 2. í‚¤ ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜ ëŒ€í­ ê°œì„ 
    improve_key_mapping_algorithm()
    
    # 3. ê°œì„ ëœ ë¡œë”© í…ŒìŠ¤íŠ¸
    results = test_improved_loading()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ìµœì¢… ëª¨ë¸ ë¡œë”© ë¬¸ì œ í•´ê²° ì™„ë£Œ!")
    
    if human_parsing_fixed:
        print("âœ… Human Parsing ëª¨ë¸ ë¬¸ì œ í•´ê²°ë¨")
    else:
        print("âŒ Human Parsing ëª¨ë¸ ë¬¸ì œ í•´ê²° ì‹¤íŒ¨")
    
    success_count = sum(1 for success in results.values() if success)
    total_count = len(results)
    print(f"ğŸ“Š ì „ì²´ ì„±ê³µë¥ : {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
    
    print("=" * 60)

if __name__ == "__main__":
    main()
