#!/usr/bin/env python3
"""
ğŸ”¥ ê³ ê¸‰ AI ëª¨ë¸ ìˆ˜ì • ë„êµ¬
========================

ë¬¸ì œê°€ ìˆëŠ” AI ëª¨ë¸ë“¤ì„ ê³ ê¸‰ ê¸°ìˆ ë¡œ ìˆ˜ì •í•˜ëŠ” ë„êµ¬

Author: MyCloset AI Team
Date: 2025-08-08
Version: 2.0
"""

import os
import sys
import json
import shutil
import logging
import zipfile
import pickle
import struct
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import subprocess

# PyTorch ê´€ë ¨
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# SafeTensors ê´€ë ¨
try:
    import safetensors
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

logger = logging.getLogger(__name__)

class AdvancedModelFixer:
    """ê³ ê¸‰ AI ëª¨ë¸ ìˆ˜ì • ë„êµ¬"""
    
    def __init__(self):
        self.fixed_models = []
        self.failed_models = []
        
    def fix_all_problematic_models(self):
        """ëª¨ë“  ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ìˆ˜ì •"""
        print("ğŸ”¥ ê³ ê¸‰ AI ëª¨ë¸ ìˆ˜ì • ë„êµ¬")
        print("=" * 80)
        
        # ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ëª©ë¡
        problematic_models = [
            "backend/ai_models/Graphonomy/training_args.bin",
            "backend/ai_models/step_01_human_parsing/graphonomy.pth",
            "backend/ai_models/step_01_human_parsing/graphonomy_root.pth",
            "backend/ai_models/step_03_cloth_segmentation/u2net_official.pth",
            "backend/ai_models/step_06_virtual_fitting/ootdiffusion/text_encoder/ootdiffusion/text_encoder/text_encoder_pytorch_model.bin",
            "backend/ai_models/step_06_virtual_fitting/ootdiffusion/vae/ootdiffusion/vae/vae_diffusion_pytorch_model.bin"
        ]
        
        for model_path in problematic_models:
            if Path(model_path).exists():
                print(f"\nğŸ”§ ìˆ˜ì • ì‹œë„: {Path(model_path).name}")
                if self.fix_model(model_path):
                    self.fixed_models.append(model_path)
                    print(f"âœ… ìˆ˜ì • ì™„ë£Œ: {Path(model_path).name}")
                else:
                    self.failed_models.append(model_path)
                    print(f"âŒ ìˆ˜ì • ì‹¤íŒ¨: {Path(model_path).name}")
            else:
                print(f"\nâš ï¸ íŒŒì¼ ì—†ìŒ: {model_path}")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ìˆ˜ì • ê²°ê³¼:")
        print(f"   âœ… ì„±ê³µ: {len(self.fixed_models)}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {len(self.failed_models)}ê°œ")
    
    def fix_model(self, model_path: str) -> bool:
        """ê°œë³„ ëª¨ë¸ ìˆ˜ì •"""
        try:
            # ë°±ì—… ìƒì„±
            backup_path = f"{model_path}.backup"
            if not Path(backup_path).exists():
                shutil.copy2(model_path, backup_path)
                print(f"   ğŸ“¦ ë°±ì—… ìƒì„±: {backup_path}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = Path(model_path).stat().st_size
            if file_size == 0:
                print(f"   âš ï¸ íŒŒì¼ í¬ê¸°ê°€ 0ì…ë‹ˆë‹¤")
                return False
            
            # íŒŒì¼ íƒ€ì… ê°ì§€
            file_type = self._detect_file_type(model_path)
            print(f"   ğŸ” íŒŒì¼ íƒ€ì…: {file_type}")
            
            # íŒŒì¼ íƒ€ì…ë³„ ìˆ˜ì •
            if file_type == "zip":
                return self._fix_zip_model(model_path)
            elif file_type == "pytorch":
                return self._fix_pytorch_model(model_path)
            elif file_type == "corrupted":
                return self._fix_corrupted_model(model_path)
            else:
                return self._fix_unknown_model(model_path)
                
        except Exception as e:
            print(f"   âŒ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _detect_file_type(self, model_path: str) -> str:
        """íŒŒì¼ íƒ€ì… ê°ì§€"""
        try:
            with open(model_path, 'rb') as f:
                header = f.read(1024)
            
            # ZIP íŒŒì¼ ì‹œê·¸ë‹ˆì²˜
            if header.startswith(b'PK\x03\x04'):
                return "zip"
            
            # PyTorch ì‹œê·¸ë‹ˆì²˜
            if b'pytorch' in header.lower():
                return "pytorch"
            
            # ì†ìƒëœ íŒŒì¼ ê°ì§€
            if len(header) < 100 or all(b == 0 for b in header[:100]):
                return "corrupted"
            
            return "unknown"
            
        except Exception:
            return "unknown"
    
    def _fix_zip_model(self, model_path: str) -> bool:
        """ZIP í˜•íƒœì˜ ëª¨ë¸ ìˆ˜ì •"""
        try:
            print(f"   ğŸ”„ ZIP íŒŒì¼ ìˆ˜ì • ì‹œë„")
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = f"{model_path}_temp"
            if Path(temp_dir).exists():
                shutil.rmtree(temp_dir)
            Path(temp_dir).mkdir(parents=True, exist_ok=True)
            
            # ZIP íŒŒì¼ ì••ì¶• í•´ì œ
            with zipfile.ZipFile(model_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # data.pkl íŒŒì¼ ì°¾ê¸°
            data_pkl_path = Path(temp_dir) / "data.pkl"
            if data_pkl_path.exists():
                print(f"   ğŸ” data.pkl ë°œê²¬")
                
                # data.pkl ë¡œë”© ì‹œë„
                try:
                    model_data = torch.load(str(data_pkl_path), map_location='cpu')
                    torch.save(model_data, model_path)
                    print(f"   âœ… data.pklì—ì„œ ëª¨ë¸ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ")
                    
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    shutil.rmtree(temp_dir)
                    return True
                    
                except Exception as e:
                    print(f"   âŒ data.pkl ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ë‹¤ë¥¸ íŒŒì¼ë“¤ í™•ì¸
            for file_path in Path(temp_dir).rglob("*"):
                if file_path.is_file() and file_path.suffix in ['.pkl', '.pt', '.pth']:
                    try:
                        model_data = torch.load(str(file_path), map_location='cpu')
                        torch.save(model_data, model_path)
                        print(f"   âœ… {file_path.name}ì—ì„œ ëª¨ë¸ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ")
                        
                        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                        shutil.rmtree(temp_dir)
                        return True
                        
                    except Exception as e:
                        print(f"   âŒ {file_path.name} ë¡œë”© ì‹¤íŒ¨: {e}")
                        continue
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            shutil.rmtree(temp_dir)
            return False
            
        except Exception as e:
            print(f"   âŒ ZIP íŒŒì¼ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _fix_pytorch_model(self, model_path: str) -> bool:
        """PyTorch ëª¨ë¸ ìˆ˜ì •"""
        try:
            print(f"   ğŸ”„ PyTorch ëª¨ë¸ ìˆ˜ì • ì‹œë„")
            
            # ë°©ë²• 1: weights_only=False (ë³´ì•ˆ ì£¼ì˜)
            try:
                print(f"   ğŸ”„ ë°©ë²• 1: weights_only=False ì‹œë„")
                model_data = torch.load(model_path, map_location='cpu', weights_only=False)
                torch.save(model_data, model_path)
                print(f"   âœ… ë°©ë²• 1 ì„±ê³µ")
                return True
            except Exception as e1:
                print(f"   âŒ ë°©ë²• 1 ì‹¤íŒ¨: {e1}")
            
            # ë°©ë²• 2: SafeTensors ë³€í™˜
            if SAFETENSORS_AVAILABLE:
                try:
                    print(f"   ğŸ”„ ë°©ë²• 2: SafeTensors ë³€í™˜ ì‹œë„")
                    if self._convert_to_safetensors(model_path):
                        print(f"   âœ… ë°©ë²• 2 ì„±ê³µ")
                        return True
                except Exception as e2:
                    print(f"   âŒ ë°©ë²• 2 ì‹¤íŒ¨: {e2}")
            
            # ë°©ë²• 3: íŒŒì¼ ì¬êµ¬ì„±
            try:
                print(f"   ğŸ”„ ë°©ë²• 3: íŒŒì¼ ì¬êµ¬ì„± ì‹œë„")
                if self._reconstruct_pytorch_file(model_path):
                    print(f"   âœ… ë°©ë²• 3 ì„±ê³µ")
                    return True
            except Exception as e3:
                print(f"   âŒ ë°©ë²• 3 ì‹¤íŒ¨: {e3}")
            
            return False
            
        except Exception as e:
            print(f"   âŒ PyTorch ëª¨ë¸ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _fix_corrupted_model(self, model_path: str) -> bool:
        """ì†ìƒëœ ëª¨ë¸ ìˆ˜ì •"""
        try:
            print(f"   ğŸ”„ ì†ìƒëœ ëª¨ë¸ ìˆ˜ì • ì‹œë„")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = Path(model_path).stat().st_size
            
            if file_size == 0:
                print(f"   âš ï¸ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return False
            
            # íŒŒì¼ì˜ ì²˜ìŒ ë¶€ë¶„ ì½ê¸°
            with open(model_path, 'rb') as f:
                data = f.read()
            
            # PyTorch ì‹œê·¸ë‹ˆì²˜ ì°¾ê¸°
            pytorch_signatures = [b'PK\x03\x04', b'pytorch', b'pickle']
            
            for signature in pytorch_signatures:
                pos = data.find(signature)
                if pos != -1:
                    print(f"   ğŸ” PyTorch ì‹œê·¸ë‹ˆì²˜ ë°œê²¬: {signature}")
                    
                    # ì‹œê·¸ë‹ˆì²˜ë¶€í„° ëê¹Œì§€ ì¶”ì¶œ
                    valid_data = data[pos:]
                    
                    # ì„ì‹œ íŒŒì¼ì— ì €ì¥
                    temp_path = f"{model_path}_temp"
                    with open(temp_path, 'wb') as f:
                        f.write(valid_data)
                    
                    # ì„ì‹œ íŒŒì¼ í…ŒìŠ¤íŠ¸
                    try:
                        model_data = torch.load(temp_path, map_location='cpu', weights_only=True)
                        torch.save(model_data, model_path)
                        os.remove(temp_path)
                        print(f"   âœ… ì†ìƒëœ íŒŒì¼ì—ì„œ ìœ íš¨í•œ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ")
                        return True
                    except Exception as e:
                        print(f"   âŒ ì„ì‹œ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
                        os.remove(temp_path)
            
            return False
            
        except Exception as e:
            print(f"   âŒ ì†ìƒëœ ëª¨ë¸ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _fix_unknown_model(self, model_path: str) -> bool:
        """ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ìˆ˜ì •"""
        try:
            print(f"   ğŸ”„ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ìˆ˜ì • ì‹œë„")
            
            # ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„
            methods = [
                self._try_weights_only_false,
                self._try_safetensors_conversion,
                self._try_file_reconstruction,
                self._try_binary_repair
            ]
            
            for i, method in enumerate(methods, 1):
                try:
                    print(f"   ğŸ”„ ë°©ë²• {i} ì‹œë„")
                    if method(model_path):
                        print(f"   âœ… ë°©ë²• {i} ì„±ê³µ")
                        return True
                except Exception as e:
                    print(f"   âŒ ë°©ë²• {i} ì‹¤íŒ¨: {e}")
                    continue
            
            return False
            
        except Exception as e:
            print(f"   âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _try_weights_only_false(self, model_path: str) -> bool:
        """weights_only=Falseë¡œ ì‹œë„"""
        model_data = torch.load(model_path, map_location='cpu', weights_only=False)
        torch.save(model_data, model_path)
        return True
    
    def _try_safetensors_conversion(self, model_path: str) -> bool:
        """SafeTensors ë³€í™˜ ì‹œë„"""
        if not SAFETENSORS_AVAILABLE:
            return False
        
        # ëª¨ë¸ ë¡œë”©
        model_data = None
        for method in ['weights_only_true', 'weights_only_false']:
            try:
                if method == 'weights_only_true':
                    model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                elif method == 'weights_only_false':
                    model_data = torch.load(model_path, map_location='cpu', weights_only=False)
                
                if model_data is not None:
                    break
            except:
                continue
        
        if model_data is None:
            return False
        
        # SafeTensorsë¡œ ì €ì¥
        safetensors_path = model_path.replace('.pth', '.safetensors').replace('.pt', '.safetensors')
        
        if isinstance(model_data, dict):
            if 'state_dict' in model_data:
                safetensors.save_file(model_data['state_dict'], safetensors_path)
            else:
                safetensors.save_file(model_data, safetensors_path)
        else:
            safetensors.save_file({'model': model_data}, safetensors_path)
        
        # ì›ë³¸ íŒŒì¼ êµì²´
        shutil.move(model_path, f"{model_path}.old")
        shutil.move(safetensors_path, model_path)
        
        return True
    
    def _try_file_reconstruction(self, model_path: str) -> bool:
        """íŒŒì¼ ì¬êµ¬ì„± ì‹œë„"""
        # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
        with open(model_path, 'rb') as f:
            data = f.read()
        
        # PyTorch ê´€ë ¨ ì‹œê·¸ë‹ˆì²˜ ì°¾ê¸°
        signatures = [
            (b'PK\x03\x04', 'zip'),
            (b'pytorch', 'pytorch'),
            (b'pickle', 'pickle'),
            (b'\x80\x02', 'pickle'),  # pickle í”„ë¡œí† ì½œ 2
            (b'\x80\x03', 'pickle'),  # pickle í”„ë¡œí† ì½œ 3
            (b'\x80\x04', 'pickle'),  # pickle í”„ë¡œí† ì½œ 4
        ]
        
        for signature, file_type in signatures:
            pos = data.find(signature)
            if pos != -1:
                print(f"   ğŸ” {file_type} ì‹œê·¸ë‹ˆì²˜ ë°œê²¬")
                
                # ì‹œê·¸ë‹ˆì²˜ë¶€í„° ëê¹Œì§€ ì¶”ì¶œ
                valid_data = data[pos:]
                
                # ì„ì‹œ íŒŒì¼ì— ì €ì¥
                temp_path = f"{model_path}_temp"
                with open(temp_path, 'wb') as f:
                    f.write(valid_data)
                
                # ì„ì‹œ íŒŒì¼ í…ŒìŠ¤íŠ¸
                try:
                    if file_type == 'zip':
                        # ZIP íŒŒì¼ ì²˜ë¦¬
                        with zipfile.ZipFile(temp_path, 'r') as zip_ref:
                            temp_dir = f"{temp_path}_extract"
                            zip_ref.extractall(temp_dir)
                            
                            # data.pkl ì°¾ê¸°
                            data_pkl_path = Path(temp_dir) / "data.pkl"
                            if data_pkl_path.exists():
                                model_data = torch.load(str(data_pkl_path), map_location='cpu')
                                torch.save(model_data, model_path)
                                shutil.rmtree(temp_dir)
                                os.remove(temp_path)
                                return True
                            
                            shutil.rmtree(temp_dir)
                    else:
                        # PyTorch/Pickle íŒŒì¼ ì²˜ë¦¬
                        model_data = torch.load(temp_path, map_location='cpu', weights_only=True)
                        torch.save(model_data, model_path)
                        os.remove(temp_path)
                        return True
                        
                except Exception as e:
                    print(f"   âŒ {file_type} íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    if Path(temp_path).exists():
                        os.remove(temp_path)
                    continue
        
        return False
    
    def _try_binary_repair(self, model_path: str) -> bool:
        """ë°”ì´ë„ˆë¦¬ ìˆ˜ë¦¬ ì‹œë„"""
        try:
            # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
            with open(model_path, 'rb') as f:
                data = f.read()
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            if len(data) < 100:
                return False
            
            # í—¤ë” ë¶€ë¶„ ì œê±° ì‹œë„
            for i in range(0, min(1000, len(data)), 100):
                try:
                    temp_path = f"{model_path}_temp"
                    with open(temp_path, 'wb') as f:
                        f.write(data[i:])
                    
                    # ì„ì‹œ íŒŒì¼ í…ŒìŠ¤íŠ¸
                    model_data = torch.load(temp_path, map_location='cpu', weights_only=True)
                    torch.save(model_data, model_path)
                    os.remove(temp_path)
                    print(f"   âœ… ë°”ì´ë„ˆë¦¬ ìˆ˜ë¦¬ ì„±ê³µ (ì˜¤í”„ì…‹: {i})")
                    return True
                    
                except Exception as e:
                    if Path(temp_path).exists():
                        os.remove(temp_path)
                    continue
            
            return False
            
        except Exception as e:
            print(f"   âŒ ë°”ì´ë„ˆë¦¬ ìˆ˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _convert_to_safetensors(self, model_path: str) -> bool:
        """PyTorch ëª¨ë¸ì„ SafeTensorsë¡œ ë³€í™˜"""
        try:
            # ëª¨ë¸ ë¡œë”©
            model_data = None
            for method in ['weights_only_true', 'weights_only_false']:
                try:
                    if method == 'weights_only_true':
                        model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    elif method == 'weights_only_false':
                        model_data = torch.load(model_path, map_location='cpu', weights_only=False)
                    
                    if model_data is not None:
                        break
                except:
                    continue
            
            if model_data is None:
                return False
            
            # SafeTensorsë¡œ ì €ì¥
            safetensors_path = model_path.replace('.pth', '.safetensors').replace('.pt', '.safetensors')
            
            if isinstance(model_data, dict):
                if 'state_dict' in model_data:
                    safetensors.save_file(model_data['state_dict'], safetensors_path)
                else:
                    safetensors.save_file(model_data, safetensors_path)
            else:
                safetensors.save_file({'model': model_data}, safetensors_path)
            
            # ì›ë³¸ íŒŒì¼ êµì²´
            shutil.move(model_path, f"{model_path}.old")
            shutil.move(safetensors_path, model_path)
            
            return True
            
        except Exception as e:
            print(f"   âŒ SafeTensors ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
    
    def _reconstruct_pytorch_file(self, model_path: str) -> bool:
        """PyTorch íŒŒì¼ ì¬êµ¬ì„±"""
        try:
            # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
            with open(model_path, 'rb') as f:
                data = f.read()
            
            # PyTorch ì‹œê·¸ë‹ˆì²˜ ì°¾ê¸°
            pytorch_signatures = [b'PK\x03\x04', b'pytorch', b'pickle']
            
            for signature in pytorch_signatures:
                pos = data.find(signature)
                if pos != -1:
                    # ì‹œê·¸ë‹ˆì²˜ë¶€í„° ëê¹Œì§€ ì¶”ì¶œ
                    valid_data = data[pos:]
                    
                    # ì„ì‹œ íŒŒì¼ì— ì €ì¥
                    temp_path = f"{model_path}_temp"
                    with open(temp_path, 'wb') as f:
                        f.write(valid_data)
                    
                    # ì„ì‹œ íŒŒì¼ í…ŒìŠ¤íŠ¸
                    try:
                        model_data = torch.load(temp_path, map_location='cpu', weights_only=True)
                        torch.save(model_data, model_path)
                        os.remove(temp_path)
                        return True
                    except Exception as e:
                        if Path(temp_path).exists():
                            os.remove(temp_path)
                        continue
            
            return False
            
        except Exception as e:
            print(f"   âŒ PyTorch íŒŒì¼ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
            return False
    
    def verify_fixed_models(self):
        """ìˆ˜ì •ëœ ëª¨ë¸ë“¤ ê²€ì¦"""
        print(f"\nğŸ” ìˆ˜ì •ëœ ëª¨ë¸ë“¤ ê²€ì¦:")
        
        verified_count = 0
        for model_path in self.fixed_models:
            print(f"\nğŸ” ê²€ì¦ ì¤‘: {Path(model_path).name}")
            
            try:
                # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                if model_path.endswith('.safetensors') and SAFETENSORS_AVAILABLE:
                    with safetensors.safe_open(model_path, framework="pt", device="cpu") as f:
                        keys = list(f.keys())
                    print(f"   âœ… SafeTensors ë¡œë”© ì„±ê³µ (í‚¤ ìˆ˜: {len(keys)})")
                    verified_count += 1
                else:
                    model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    print(f"   âœ… PyTorch ë¡œë”© ì„±ê³µ")
                    verified_count += 1
                    
            except Exception as e:
                print(f"   âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼: {verified_count}/{len(self.fixed_models)}ê°œ ì„±ê³µ")
    
    def generate_report(self):
        """ìˆ˜ì • ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("ğŸ”¥ ê³ ê¸‰ AI ëª¨ë¸ ìˆ˜ì • ë¦¬í¬íŠ¸")
        report.append("=" * 80)
        report.append(f"ğŸ“… ìˆ˜ì • ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        report.append(f"ğŸ“Š ìˆ˜ì • ê²°ê³¼:")
        report.append(f"   âœ… ì„±ê³µ: {len(self.fixed_models)}ê°œ")
        report.append(f"   âŒ ì‹¤íŒ¨: {len(self.failed_models)}ê°œ")
        report.append("")
        
        if self.fixed_models:
            report.append("âœ… ìˆ˜ì • ì™„ë£Œëœ ëª¨ë¸ë“¤:")
            for model_path in self.fixed_models:
                report.append(f"   - {Path(model_path).name}")
            report.append("")
        
        if self.failed_models:
            report.append("âŒ ìˆ˜ì • ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
            for model_path in self.failed_models:
                report.append(f"   - {Path(model_path).name}")
            report.append("")
        
        return "\n".join(report)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ìˆ˜ì •ê¸° ì´ˆê¸°í™”
    fixer = AdvancedModelFixer()
    
    # ëª¨ë“  ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ìˆ˜ì •
    fixer.fix_all_problematic_models()
    
    # ìˆ˜ì •ëœ ëª¨ë¸ë“¤ ê²€ì¦
    fixer.verify_fixed_models()
    
    # ìˆ˜ì • ë¦¬í¬íŠ¸ ìƒì„±
    report = fixer.generate_report()
    print(report)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    with open("advanced_ai_model_fix_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\nğŸ’¾ ìˆ˜ì • ë¦¬í¬íŠ¸ ì €ì¥: advanced_ai_model_fix_report.txt")
    print("\nğŸ‰ ê³ ê¸‰ AI ëª¨ë¸ ìˆ˜ì • ì™„ë£Œ!")

if __name__ == "__main__":
    from datetime import datetime
    main()
