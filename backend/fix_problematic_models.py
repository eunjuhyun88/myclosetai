#!/usr/bin/env python3
"""
ğŸ”¥ ë¬¸ì œê°€ ìˆëŠ” AI ëª¨ë¸ ìˆ˜ì • ë„êµ¬
================================

ë¶„ì„ì—ì„œ ë°œê²¬ëœ ë¬¸ì œê°€ ìˆëŠ” AI ëª¨ë¸ë“¤ì„ ìˆ˜ì •í•˜ëŠ” ë„êµ¬

Author: MyCloset AI Team
Date: 2025-08-08
Version: 1.0
"""

import os
import sys
import json
import shutil
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import subprocess

# PyTorch ê´€ë ¨
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# SafeTensors ê´€ë ¨
try:
    import safetensors
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

logger = logging.getLogger(__name__)

class ProblematicModelFixer:
    """ë¬¸ì œê°€ ìˆëŠ” AI ëª¨ë¸ ìˆ˜ì • ë„êµ¬"""
    
    def __init__(self, analysis_file: str = "comprehensive_ai_model_analysis.json"):
        self.analysis_file = analysis_file
        self.analysis_data = None
        self.problematic_models = []
        self.fixed_models = []
        
    def load_analysis_data(self) -> bool:
        """ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                self.analysis_data = json.load(f)
            
            # ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ì°¾ê¸°
            for model_path, model_info in self.analysis_data['models'].items():
                if not model_info['valid']:
                    self.problematic_models.append({
                        'path': model_path,
                        'info': model_info
                    })
            
            print(f"ğŸ” ë°œê²¬ëœ ë¬¸ì œ ëª¨ë¸: {len(self.problematic_models)}ê°œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_problematic_models(self):
        """ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ë¶„ì„"""
        print("\nğŸ“‹ ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ë¶„ì„:")
        
        for i, model in enumerate(self.problematic_models, 1):
            path = model['path']
            info = model['info']
            
            print(f"\n{i}. {Path(path).name}")
            print(f"   ğŸ“ ê²½ë¡œ: {path}")
            print(f"   ğŸ“Š í¬ê¸°: {info['size_mb']:.1f}MB")
            print(f"   ğŸ¯ Step: {info['step_category']}")
            print(f"   ğŸ—ï¸ êµ¬ì¡°: {info['structure_type']}")
            
            if info['issues']:
                print(f"   âš ï¸ ë¬¸ì œì :")
                for issue in info['issues']:
                    print(f"      - {issue}")
            
            if info['architecture_hints']:
                print(f"   ğŸ›ï¸ ì•„í‚¤í…ì²˜: {', '.join(info['architecture_hints'])}")
    
    def fix_header_too_large_models(self):
        """í—¤ë”ê°€ ë„ˆë¬´ í° ëª¨ë¸ë“¤ ìˆ˜ì •"""
        print("\nğŸ”§ í—¤ë”ê°€ ë„ˆë¬´ í° ëª¨ë¸ë“¤ ìˆ˜ì • ì‹œë„:")
        
        for model in self.problematic_models:
            path = model['path']
            info = model['info']
            
            # í—¤ë”ê°€ ë„ˆë¬´ í° ë¬¸ì œ í™•ì¸
            if any("header too large" in issue for issue in info['issues']):
                print(f"\nğŸ”§ ìˆ˜ì • ì‹œë„: {Path(path).name}")
                
                if self._fix_header_too_large_model(path):
                    self.fixed_models.append(path)
                    print(f"âœ… ìˆ˜ì • ì™„ë£Œ: {Path(path).name}")
                else:
                    print(f"âŒ ìˆ˜ì • ì‹¤íŒ¨: {Path(path).name}")
    
    def _fix_header_too_large_model(self, model_path: str) -> bool:
        """í—¤ë”ê°€ ë„ˆë¬´ í° ëª¨ë¸ ìˆ˜ì •"""
        try:
            # ë°±ì—… ìƒì„±
            backup_path = f"{model_path}.backup"
            if not Path(backup_path).exists():
                shutil.copy2(model_path, backup_path)
                print(f"   ğŸ“¦ ë°±ì—… ìƒì„±: {backup_path}")
            
            # ë°©ë²• 1: torch.load with map_location='cpu'
            try:
                print(f"   ğŸ”„ ë°©ë²• 1: torch.load with map_location='cpu' ì‹œë„")
                model_data = torch.load(model_path, map_location='cpu')
                
                # ìˆ˜ì •ëœ ëª¨ë¸ ì €ì¥
                torch.save(model_data, model_path)
                print(f"   âœ… ë°©ë²• 1 ì„±ê³µ")
                return True
                
            except Exception as e1:
                print(f"   âŒ ë°©ë²• 1 ì‹¤íŒ¨: {e1}")
                
                # ë°©ë²• 2: weights_only=True
                try:
                    print(f"   ğŸ”„ ë°©ë²• 2: weights_only=True ì‹œë„")
                    model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    
                    # ìˆ˜ì •ëœ ëª¨ë¸ ì €ì¥
                    torch.save(model_data, model_path)
                    print(f"   âœ… ë°©ë²• 2 ì„±ê³µ")
                    return True
                    
                except Exception as e2:
                    print(f"   âŒ ë°©ë²• 2 ì‹¤íŒ¨: {e2}")
                    
                    # ë°©ë²• 3: SafeTensorsë¡œ ë³€í™˜
                    if SAFETENSORS_AVAILABLE:
                        try:
                            print(f"   ğŸ”„ ë°©ë²• 3: SafeTensors ë³€í™˜ ì‹œë„")
                            if self._convert_to_safetensors(model_path):
                                print(f"   âœ… ë°©ë²• 3 ì„±ê³µ")
                                return True
                        except Exception as e3:
                            print(f"   âŒ ë°©ë²• 3 ì‹¤íŒ¨: {e3}")
                    
                    # ë°©ë²• 4: íŒŒì¼ ì¬êµ¬ì„±
                    try:
                        print(f"   ğŸ”„ ë°©ë²• 4: íŒŒì¼ ì¬êµ¬ì„± ì‹œë„")
                        if self._reconstruct_model_file(model_path):
                            print(f"   âœ… ë°©ë²• 4 ì„±ê³µ")
                            return True
                    except Exception as e4:
                        print(f"   âŒ ë°©ë²• 4 ì‹¤íŒ¨: {e4}")
            
            return False
            
        except Exception as e:
            print(f"   âŒ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _convert_to_safetensors(self, model_path: str) -> bool:
        """PyTorch ëª¨ë¸ì„ SafeTensorsë¡œ ë³€í™˜"""
        try:
            # ì„ì‹œë¡œ ëª¨ë¸ ë¡œë”© ì‹œë„
            model_data = None
            
            # ë‹¤ì–‘í•œ ë¡œë”© ë°©ë²• ì‹œë„
            for method in ['weights_only_true', 'weights_only_false']:
                try:
                    if method == 'weights_only_true':
                        model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    elif method == 'weights_only_false':
                        model_data = torch.load(model_path, map_location='cpu', weights_only=False)
                    
                    if model_data is not None:
                        break
                except:
                    continue
            
            if model_data is None:
                return False
            
            # SafeTensorsë¡œ ì €ì¥
            safetensors_path = model_path.replace('.pth', '.safetensors').replace('.pt', '.safetensors')
            
            if isinstance(model_data, dict):
                # state_dict í˜•íƒœì¸ ê²½ìš°
                if 'state_dict' in model_data:
                    safetensors.save_file(model_data['state_dict'], safetensors_path)
                else:
                    safetensors.save_file(model_data, safetensors_path)
            else:
                # ì§ì ‘ í…ì„œì¸ ê²½ìš°
                safetensors.save_file({'model': model_data}, safetensors_path)
            
            # ì›ë³¸ íŒŒì¼ ë°±ì—…í•˜ê³  SafeTensors íŒŒì¼ë¡œ êµì²´
            shutil.move(model_path, f"{model_path}.old")
            shutil.move(safetensors_path, model_path)
            
            return True
            
        except Exception as e:
            print(f"   âŒ SafeTensors ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
    
    def _reconstruct_model_file(self, model_path: str) -> bool:
        """ëª¨ë¸ íŒŒì¼ ì¬êµ¬ì„±"""
        try:
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = Path(model_path).stat().st_size
            
            if file_size == 0:
                print(f"   âš ï¸ íŒŒì¼ í¬ê¸°ê°€ 0ì…ë‹ˆë‹¤")
                return False
            
            # íŒŒì¼ì˜ ì²˜ìŒ ë¶€ë¶„ ì½ê¸°
            with open(model_path, 'rb') as f:
                header = f.read(1024)  # ì²˜ìŒ 1KB ì½ê¸°
            
            # PyTorch ì‹œê·¸ë‹ˆì²˜ í™•ì¸
            if b'PK\x03\x04' in header:  # ZIP íŒŒì¼ ì‹œê·¸ë‹ˆì²˜
                print(f"   ğŸ” ZIP íŒŒì¼ë¡œ ê°ì§€ë¨")
                return self._fix_zip_model_file(model_path)
            elif b'pytorch' in header.lower():
                print(f"   ğŸ” PyTorch íŒŒì¼ë¡œ ê°ì§€ë¨")
                return self._fix_pytorch_model_file(model_path)
            else:
                print(f"   ğŸ” ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹")
                return False
                
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
            return False
    
    def _fix_zip_model_file(self, model_path: str) -> bool:
        """ZIP í˜•íƒœì˜ ëª¨ë¸ íŒŒì¼ ìˆ˜ì •"""
        try:
            import zipfile
            
            # ZIP íŒŒì¼ë¡œ ì—´ê¸°
            with zipfile.ZipFile(model_path, 'r') as zip_ref:
                # ì••ì¶• í•´ì œ
                temp_dir = f"{model_path}_temp"
                zip_ref.extractall(temp_dir)
                
                # data.pkl íŒŒì¼ ì°¾ê¸°
                data_pkl_path = Path(temp_dir) / "data.pkl"
                if data_pkl_path.exists():
                    # data.pklì„ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ì €ì¥
                    model_data = torch.load(str(data_pkl_path), map_location='cpu')
                    torch.save(model_data, model_path)
                    
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    shutil.rmtree(temp_dir)
                    return True
            
            return False
            
        except Exception as e:
            print(f"   âŒ ZIP íŒŒì¼ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _fix_pytorch_model_file(self, model_path: str) -> bool:
        """PyTorch ëª¨ë¸ íŒŒì¼ ìˆ˜ì •"""
        try:
            # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
            with open(model_path, 'rb') as f:
                data = f.read()
            
            # PyTorch í—¤ë” ì°¾ê¸°
            pytorch_header = b'pytorch'
            header_pos = data.find(pytorch_header)
            
            if header_pos != -1:
                # í—¤ë” ë¶€ë¶„ì„ ê±´ë„ˆë›°ê³  ë°ì´í„° ë¶€ë¶„ë§Œ ì¶”ì¶œ
                model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                torch.save(model_data, model_path)
                return True
            
            return False
            
        except Exception as e:
            print(f"   âŒ PyTorch íŒŒì¼ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def fix_invalid_json_models(self):
        """ì˜ëª»ëœ JSON í—¤ë” ëª¨ë¸ë“¤ ìˆ˜ì •"""
        print("\nğŸ”§ ì˜ëª»ëœ JSON í—¤ë” ëª¨ë¸ë“¤ ìˆ˜ì • ì‹œë„:")
        
        for model in self.problematic_models:
            path = model['path']
            info = model['info']
            
            # ì˜ëª»ëœ JSON ë¬¸ì œ í™•ì¸
            if any("invalid JSON" in issue for issue in info['issues']):
                print(f"\nğŸ”§ ìˆ˜ì • ì‹œë„: {Path(path).name}")
                
                if self._fix_invalid_json_model(path):
                    self.fixed_models.append(path)
                    print(f"âœ… ìˆ˜ì • ì™„ë£Œ: {Path(path).name}")
                else:
                    print(f"âŒ ìˆ˜ì • ì‹¤íŒ¨: {Path(path).name}")
    
    def _fix_invalid_json_model(self, model_path: str) -> bool:
        """ì˜ëª»ëœ JSON í—¤ë” ëª¨ë¸ ìˆ˜ì •"""
        try:
            # ë°±ì—… ìƒì„±
            backup_path = f"{model_path}.backup"
            if not Path(backup_path).exists():
                shutil.copy2(model_path, backup_path)
                print(f"   ğŸ“¦ ë°±ì—… ìƒì„±: {backup_path}")
            
            # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
            with open(model_path, 'rb') as f:
                data = f.read()
            
            # PyTorch ì‹œê·¸ë‹ˆì²˜ ì°¾ê¸°
            pytorch_signature = b'PK\x03\x04'  # ZIP íŒŒì¼ ì‹œê·¸ë‹ˆì²˜
            
            if pytorch_signature in data:
                # ZIP íŒŒì¼ë¡œ ì²˜ë¦¬
                return self._fix_zip_model_file(model_path)
            else:
                # ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                try:
                    model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    torch.save(model_data, model_path)
                    return True
                except:
                    return False
            
        except Exception as e:
            print(f"   âŒ JSON í—¤ë” ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def fix_missing_file_models(self):
        """íŒŒì¼ì´ ì—†ëŠ” ëª¨ë¸ë“¤ ì²˜ë¦¬"""
        print("\nğŸ”§ íŒŒì¼ì´ ì—†ëŠ” ëª¨ë¸ë“¤ ì²˜ë¦¬:")
        
        for model in self.problematic_models:
            path = model['path']
            info = model['info']
            
            # íŒŒì¼ì´ ì—†ëŠ” ë¬¸ì œ í™•ì¸
            if any("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ" in issue for issue in info['issues']):
                print(f"\nâš ï¸ íŒŒì¼ ì—†ìŒ: {Path(path).name}")
                print(f"   ğŸ“ ê²½ë¡œ: {path}")
                
                # ëŒ€ì²´ íŒŒì¼ ì°¾ê¸°
                alternative_file = self._find_alternative_file(path)
                if alternative_file:
                    print(f"   ğŸ” ëŒ€ì²´ íŒŒì¼ ë°œê²¬: {Path(alternative_file).name}")
                    
                    # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
                    try:
                        if Path(path).parent.exists():
                            os.symlink(alternative_file, path)
                            print(f"   âœ… ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì™„ë£Œ")
                            self.fixed_models.append(path)
                        else:
                            print(f"   âŒ ëŒ€ìƒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                    except Exception as e:
                        print(f"   âŒ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                else:
                    print(f"   âŒ ëŒ€ì²´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def _find_alternative_file(self, original_path: str) -> Optional[str]:
        """ëŒ€ì²´ íŒŒì¼ ì°¾ê¸°"""
        original_name = Path(original_path).name
        original_dir = Path(original_path).parent
        
        # ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ìŠ·í•œ ì´ë¦„ì˜ íŒŒì¼ ì°¾ê¸°
        if original_dir.exists():
            for file_path in original_dir.glob("*"):
                if file_path.is_file() and file_path.name != original_name:
                    # íŒŒì¼ ì´ë¦„ì´ ë¹„ìŠ·í•œì§€ í™•ì¸
                    if any(keyword in file_path.name.lower() for keyword in 
                           ['model', 'pytorch', 'checkpoint', 'weights']):
                        return str(file_path)
        
        # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        parent_dir = original_dir.parent
        if parent_dir.exists():
            for file_path in parent_dir.rglob("*"):
                if file_path.is_file() and file_path.name == original_name:
                    return str(file_path)
        
        return None
    
    def verify_fixed_models(self):
        """ìˆ˜ì •ëœ ëª¨ë¸ë“¤ ê²€ì¦"""
        print(f"\nğŸ” ìˆ˜ì •ëœ ëª¨ë¸ë“¤ ê²€ì¦:")
        
        verified_count = 0
        for model_path in self.fixed_models:
            print(f"\nğŸ” ê²€ì¦ ì¤‘: {Path(model_path).name}")
            
            try:
                # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                if model_path.endswith('.safetensors') and SAFETENSORS_AVAILABLE:
                    with safetensors.safe_open(model_path, framework="pt", device="cpu") as f:
                        keys = list(f.keys())
                    print(f"   âœ… SafeTensors ë¡œë”© ì„±ê³µ (í‚¤ ìˆ˜: {len(keys)})")
                    verified_count += 1
                else:
                    model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    print(f"   âœ… PyTorch ë¡œë”© ì„±ê³µ")
                    verified_count += 1
                    
            except Exception as e:
                print(f"   âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼: {verified_count}/{len(self.fixed_models)}ê°œ ì„±ê³µ")
    
    def generate_fix_report(self):
        """ìˆ˜ì • ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("ğŸ”¥ AI ëª¨ë¸ ìˆ˜ì • ë¦¬í¬íŠ¸")
        report.append("=" * 80)
        report.append(f"ğŸ“… ìˆ˜ì • ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        report.append(f"ğŸ“Š ì „ì²´ ë¬¸ì œ ëª¨ë¸: {len(self.problematic_models)}ê°œ")
        report.append(f"âœ… ìˆ˜ì • ì™„ë£Œ: {len(self.fixed_models)}ê°œ")
        report.append(f"âŒ ìˆ˜ì • ì‹¤íŒ¨: {len(self.problematic_models) - len(self.fixed_models)}ê°œ")
        report.append("")
        
        if self.fixed_models:
            report.append("âœ… ìˆ˜ì • ì™„ë£Œëœ ëª¨ë¸ë“¤:")
            for model_path in self.fixed_models:
                report.append(f"   - {Path(model_path).name}")
            report.append("")
        
        remaining_problems = [m for m in self.problematic_models if m['path'] not in self.fixed_models]
        if remaining_problems:
            report.append("âŒ ìˆ˜ì • ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
            for model in remaining_problems:
                report.append(f"   - {Path(model['path']).name}")
                for issue in model['info']['issues']:
                    report.append(f"     ë¬¸ì œ: {issue}")
            report.append("")
        
        return "\n".join(report)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”¥ ë¬¸ì œê°€ ìˆëŠ” AI ëª¨ë¸ ìˆ˜ì • ë„êµ¬")
    print("=" * 80)
    
    # ìˆ˜ì •ê¸° ì´ˆê¸°í™”
    fixer = ProblematicModelFixer()
    
    # 1. ë¶„ì„ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‹ 1ë‹¨ê³„: ë¶„ì„ ë°ì´í„° ë¡œë“œ")
    if not fixer.load_analysis_data():
        print("âŒ ë¶„ì„ ë°ì´í„° ë¡œë“œë¥¼ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 2. ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ë¶„ì„
    print("\nğŸ“‹ 2ë‹¨ê³„: ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ë“¤ ë¶„ì„")
    fixer.analyze_problematic_models()
    
    # 3. í—¤ë”ê°€ ë„ˆë¬´ í° ëª¨ë¸ë“¤ ìˆ˜ì •
    print("\nğŸ”§ 3ë‹¨ê³„: í—¤ë”ê°€ ë„ˆë¬´ í° ëª¨ë¸ë“¤ ìˆ˜ì •")
    fixer.fix_header_too_large_models()
    
    # 4. ì˜ëª»ëœ JSON í—¤ë” ëª¨ë¸ë“¤ ìˆ˜ì •
    print("\nğŸ”§ 4ë‹¨ê³„: ì˜ëª»ëœ JSON í—¤ë” ëª¨ë¸ë“¤ ìˆ˜ì •")
    fixer.fix_invalid_json_models()
    
    # 5. íŒŒì¼ì´ ì—†ëŠ” ëª¨ë¸ë“¤ ì²˜ë¦¬
    print("\nğŸ”§ 5ë‹¨ê³„: íŒŒì¼ì´ ì—†ëŠ” ëª¨ë¸ë“¤ ì²˜ë¦¬")
    fixer.fix_missing_file_models()
    
    # 6. ìˆ˜ì •ëœ ëª¨ë¸ë“¤ ê²€ì¦
    print("\nğŸ” 6ë‹¨ê³„: ìˆ˜ì •ëœ ëª¨ë¸ë“¤ ê²€ì¦")
    fixer.verify_fixed_models()
    
    # 7. ìˆ˜ì • ë¦¬í¬íŠ¸ ìƒì„±
    print("\nğŸ“‹ 7ë‹¨ê³„: ìˆ˜ì • ë¦¬í¬íŠ¸ ìƒì„±")
    report = fixer.generate_fix_report()
    print(report)
    
    # 8. ë¦¬í¬íŠ¸ ì €ì¥
    with open("ai_model_fix_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\nğŸ’¾ ìˆ˜ì • ë¦¬í¬íŠ¸ ì €ì¥: ai_model_fix_report.txt")
    print("\nğŸ‰ AI ëª¨ë¸ ìˆ˜ì • ì™„ë£Œ!")

if __name__ == "__main__":
    from datetime import datetime
    main()
