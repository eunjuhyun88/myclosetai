#!/usr/bin/env python3
"""
ğŸ”§ Graphonomy ëª¨ë¸ íŒŒì¼ ë³µêµ¬ ë° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
backend/fix_graphonomy_model.py

âœ… ì†ìƒëœ graphonomy.pth íŒŒì¼ ë¶„ì„
âœ… ëŒ€ì²´ ëª¨ë¸ íŒŒì¼ íƒì§€ ë° í™œìš©
âœ… ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦
âœ… Human Parsing Step ì‹¤ì œ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import shutil
import time
from pathlib import Path
import torch
import hashlib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

def analyze_damaged_graphonomy():
    """ì†ìƒëœ graphonomy.pth ë¶„ì„"""
    
    print("ğŸ” ì†ìƒëœ Graphonomy ëª¨ë¸ ë¶„ì„ ì¤‘...")
    
    graphonomy_path = Path("ai_models/step_01_human_parsing/graphonomy.pth")
    
    if not graphonomy_path.exists():
        print("âŒ graphonomy.pth íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        return False
    
    file_size = graphonomy_path.stat().st_size / (1024 * 1024)
    print(f"   ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
    
    # íŒŒì¼ ì†ìƒ ì›ì¸ ë¶„ì„
    try:
        with open(graphonomy_path, 'rb') as f:
            header = f.read(1024)
            
        # PyTorch íŒŒì¼ í—¤ë” í™•ì¸
        if b'PK' in header[:10]:
            print("   ğŸ” ZIP ì•„ì¹´ì´ë¸Œ í˜•ì‹ ê°ì§€ë¨")
        elif b'PYTORCH' in header:
            print("   ğŸ” PyTorch ë„¤ì´í‹°ë¸Œ í˜•ì‹ ê°ì§€ë¨")
        else:
            print("   âŒ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹")
            
        # ì²´í¬ì„¬ ê³„ì‚°
        with open(graphonomy_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        print(f"   ğŸ” MD5 ì²´í¬ì„¬: {file_hash}")
        
    except Exception as e:
        print(f"   âŒ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False
    
    return True

def find_alternative_models():
    """ëŒ€ì²´ Human Parsing ëª¨ë¸ ì°¾ê¸°"""
    
    print("\nğŸ” ëŒ€ì²´ Human Parsing ëª¨ë¸ íƒìƒ‰ ì¤‘...")
    
    parsing_dir = Path("ai_models/step_01_human_parsing")
    alternatives = []
    
    if parsing_dir.exists():
        for model_file in parsing_dir.glob("*.pth"):
            if model_file.name != "graphonomy.pth":
                try:
                    # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                    checkpoint = torch.load(model_file, map_location='cpu', weights_only=False)
                    size_mb = model_file.stat().st_size / (1024 * 1024)
                    
                    alternatives.append({
                        'name': model_file.name,
                        'path': model_file,
                        'size_mb': size_mb,
                        'loadable': True,
                        'checkpoint': checkpoint
                    })
                    
                    print(f"   âœ… {model_file.name}: {size_mb:.1f}MB (ë¡œë”© ê°€ëŠ¥)")
                    
                except Exception as e:
                    print(f"   âŒ {model_file.name}: ë¡œë”© ì‹¤íŒ¨ - {e}")
    
    return alternatives

def create_graphonomy_fallback(alternatives):
    """Graphonomy í´ë°± ëª¨ë¸ ìƒì„±"""
    
    print("\nğŸ”§ Graphonomy í´ë°± ëª¨ë¸ ìƒì„± ì¤‘...")
    
    if not alternatives:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ì²´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    # ê°€ì¥ í° ëª¨ë¸ì„ ìš°ì„  ì„ íƒ
    best_alternative = max(alternatives, key=lambda x: x['size_mb'])
    
    print(f"   ğŸ¯ ìµœì  ëŒ€ì²´ ëª¨ë¸: {best_alternative['name']}")
    
    # í´ë°± ëª¨ë¸ ìƒì„±
    fallback_path = Path("ai_models/step_01_human_parsing/graphonomy_fixed.pth")
    
    try:
        # ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = best_alternative['checkpoint']
        
        # í•„ìš”í•œ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
        if isinstance(checkpoint, dict):
            # Graphonomy í˜•ì‹ìœ¼ë¡œ ì¬êµ¬ì„±
            fixed_checkpoint = {
                'model_state_dict': checkpoint.get('state_dict', checkpoint),
                'model_name': 'graphonomy_fixed',
                'num_classes': 20,  # LIP ë°ì´í„°ì…‹
                'input_size': [512, 512],
                'architecture': 'DeepLabV3+',
                'source_file': best_alternative['name']
            }
            
            # ì €ì¥
            torch.save(fixed_checkpoint, fallback_path, pickle_protocol=2)
            
            print(f"   âœ… í´ë°± ëª¨ë¸ ìƒì„± ì™„ë£Œ: {fallback_path}")
            print(f"   ğŸ“Š í¬ê¸°: {fallback_path.stat().st_size / (1024*1024):.1f}MB")
            
            return True
            
    except Exception as e:
        print(f"   âŒ í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def test_human_parsing_with_fixed_model():
    """ìˆ˜ì •ëœ ëª¨ë¸ë¡œ Human Parsing í…ŒìŠ¤íŠ¸"""
    
    print("\nğŸ§  Human Parsing Step ì‹¤ì œ í…ŒìŠ¤íŠ¸...")
    
    try:
        # Human Parsing Step import
        from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìˆ˜ì •ëœ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©)
        step = HumanParsingStep(device='cpu', strict_mode=False)
        
        print("   âœ… HumanParsingStep ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
        
        # ëª¨ë¸ ê²½ë¡œ ìˆ˜ì •
        fixed_model_path = Path("ai_models/step_01_human_parsing/graphonomy_fixed.pth")
        if fixed_model_path.exists():
            # ì›ë³¸ì„ ë°±ì—…í•˜ê³  ìˆ˜ì •ëœ ëª¨ë¸ì„ ì‚¬ìš©
            original_path = Path("ai_models/step_01_human_parsing/graphonomy.pth")
            backup_path = Path("ai_models/step_01_human_parsing/graphonomy_damaged.pth")
            
            if original_path.exists():
                shutil.move(str(original_path), str(backup_path))
                print("   ğŸ“¦ ì†ìƒëœ ëª¨ë¸ì„ graphonomy_damaged.pthë¡œ ë°±ì—…")
            
            shutil.copy2(str(fixed_model_path), str(original_path))
            print("   ğŸ”„ ìˆ˜ì •ëœ ëª¨ë¸ì„ graphonomy.pthë¡œ ë³µì‚¬")
        
        # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        try:
            if hasattr(step, 'initialize'):
                result = step.initialize()
                if result:
                    print("   ğŸ‰ Human Parsing Step ì´ˆê¸°í™” ì„±ê³µ!")
                    return True
                else:
                    print("   âš ï¸ ì´ˆê¸°í™”ê°€ Falseë¥¼ ë°˜í™˜")
            else:
                print("   âš ï¸ initialize ë©”ì„œë“œ ì—†ìŒ")
        except Exception as init_error:
            print(f"   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {init_error}")
        
        return False
        
    except Exception as e:
        print(f"   âŒ Human Parsing Step í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def download_fresh_graphonomy():
    """ìƒˆë¡œìš´ Graphonomy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´"""
    
    print("\nğŸ“¥ ìƒˆë¡œìš´ Graphonomy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´")
    print("=" * 60)
    
    print("ğŸ”— ê³µì‹ ì†ŒìŠ¤:")
    print("   1. GitHub: https://github.com/Gaoyiminggithub/Graphonomy")
    print("   2. Google Drive: (ë…¼ë¬¸ ì €ì ì œê³µ)")
    print("   3. Papers With Code: https://paperswithcode.com/paper/graphonomy-universal-human-parsing-via-graph")
    
    print("\nğŸ“‹ í•„ìš”í•œ íŒŒì¼:")
    print("   - graphonomy_universal_learned.pth (ì•½ 1.2GB)")
    print("   - ë˜ëŠ” inference.pth")
    
    print("\nğŸ¯ ì„¤ì¹˜ ìœ„ì¹˜:")
    print("   ai_models/step_01_human_parsing/graphonomy.pth")
    
    print("\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
    print("   - ëª¨ë¸ì´ LIP ë°ì´í„°ì…‹(20 í´ë˜ìŠ¤) í˜•ì‹ì¸ì§€ í™•ì¸")
    print("   - PyTorch 2.7 í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ì•¼ í•¨")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ”§ Graphonomy ëª¨ë¸ ë³µêµ¬ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # 1. ì†ìƒëœ ëª¨ë¸ ë¶„ì„
    print("\nğŸ“‹ 1ë‹¨ê³„: ì†ìƒëœ ëª¨ë¸ ë¶„ì„")
    damaged_analysis = analyze_damaged_graphonomy()
    
    # 2. ëŒ€ì²´ ëª¨ë¸ íƒìƒ‰
    print("\nğŸ“‹ 2ë‹¨ê³„: ëŒ€ì²´ ëª¨ë¸ íƒìƒ‰")
    alternatives = find_alternative_models()
    
    # 3. í´ë°± ëª¨ë¸ ìƒì„±
    if alternatives:
        print("\nğŸ“‹ 3ë‹¨ê³„: í´ë°± ëª¨ë¸ ìƒì„±")
        fallback_created = create_graphonomy_fallback(alternatives)
        
        if fallback_created:
            # 4. ì‹¤ì œ í…ŒìŠ¤íŠ¸
            print("\nğŸ“‹ 4ë‹¨ê³„: Human Parsing ì‹¤ì œ í…ŒìŠ¤íŠ¸")
            test_success = test_human_parsing_with_fixed_model()
            
            if test_success:
                print("\nğŸ‰ Graphonomy ëª¨ë¸ ë³µêµ¬ ì™„ë£Œ!")
                print("âœ… Human Parsing Stepì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤")
                return True
    
    # 5. ìƒˆ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´
    print("\nğŸ“‹ ìµœì¢… ë‹¨ê³„: ìƒˆ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”")
    download_fresh_graphonomy()
    
    print("\nğŸ’¡ ì¶”ì²œ í•´ê²°ì±…:")
    print("   1. ì„ì‹œ í•´ê²°: í´ë°± ëª¨ë¸ ì‚¬ìš© (ê¸°ëŠ¥ ì œí•œì )")
    print("   2. ì˜êµ¬ í•´ê²°: ìƒˆë¡œìš´ Graphonomy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    print("   3. ëŒ€ì•ˆ: ë‹¤ë¥¸ Human Parsing ëª¨ë¸ ì‚¬ìš©")
    
    return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print("\nâœ… ë³µêµ¬ ì™„ë£Œ - ì´ì œ AI ì¶”ë¡ ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ìˆ˜ë™ ê°œì… í•„ìš” - ìœ„ì˜ ì•ˆë‚´ë¥¼ ë”°ë¼ ëª¨ë¸ì„ êµì²´í•˜ì„¸ìš”.")