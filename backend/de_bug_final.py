#!/usr/bin/env python3
"""
ğŸ”¥ Ultimate AI Model Loading Debugger v6.0 - ì™„ì „í•œ ì¢…í•© ë””ë²„ê¹… ì‹œìŠ¤í…œ
==============================================================================
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ + ì˜¤ë¥˜ ìˆ˜ì • ê¸°ëŠ¥ í†µí•© (ì´ 2000+ ë¼ì¸)
âœ… 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„ + ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸
âœ… 8ë‹¨ê³„ AI Step ì™„ì „ ë¶„ì„ + syntax error ìë™ ìˆ˜ì •
âœ… threading import ëˆ„ë½ ìë™ í•´ê²°
âœ… PyTorch weights_only ë¬¸ì œ ì™„ì „ í•´ê²° (3ë‹¨ê³„ ì•ˆì „ ë¡œë”©)
âœ… M3 Max MPS + conda mycloset-ai-clean í™˜ê²½ ì™„ì „ ìµœì í™”
âœ… BaseStepMixin v19.2 í˜¸í™˜ì„± ì™„ì „ ê²€ì¦
âœ… Central Hub DI Container ì—°ë™ ìƒíƒœ ë¶„ì„
âœ… DetailedDataSpec v5.3 í†µí•© ë¶„ì„
âœ… StepFactory v11.2 í†µí•© ë¶„ì„  
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ë§¤í•‘ ë° ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ ìµœì í™” ë¶„ì„
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë§¤ì¹­
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° ê²€ì¦
âœ… ëª¨ë“  ì˜ì¡´ì„± ìƒíƒœ ì™„ì „ ë¶„ì„
âœ… ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„±

ì£¼ìš” ê¸°ëŠ¥:
1. ğŸ”§ Step íŒŒì¼ ì˜¤ë¥˜ ìë™ ìˆ˜ì • ì‹œìŠ¤í…œ
2. ğŸš€ 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„
3. ğŸ”¥ 8ë‹¨ê³„ Step ì™„ì „ ê²€ì¦
4. ğŸ M3 Max í•˜ë“œì›¨ì–´ ì™„ì „ ìµœì í™”
5. ğŸ“Š ì¢…í•© ì„±ëŠ¥ ë° ê±´ê°•ë„ ë¶„ì„
6. ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±… ì œì‹œ
==============================================================================
"""

import sys
import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
import hashlib
import json
import importlib
import inspect
import gc
import weakref
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from contextlib import contextmanager
from enum import Enum
import warnings
import base64
from io import BytesIO

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')
os.environ['PYTHONWARNINGS'] = 'ignore'


def detect_correct_project_structure():
    """ì •í™•í•œ í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€"""
    current_file = Path(__file__).resolve()
    current_dir = Path.cwd()
    
    print(f"ğŸ” í˜„ì¬ íŒŒì¼: {current_file}")
    print(f"ğŸ” í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
    
    # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ê°€ backendì¸ ê²½ìš° (ì‹¤ì œ ìƒí™©)
    if current_dir.name == 'backend':
        project_root = current_dir.parent
        backend_root = current_dir
        print(f"âœ… backend ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ ê°ì§€")
        return project_root, backend_root
    
    # 2. í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ mycloset-ai ì°¾ê¸°
    search_paths = [current_dir] + list(current_dir.parents)
    for path in search_paths:
        if path.name == 'mycloset-ai':
            project_root = path
            backend_root = path / 'backend'
            if backend_root.exists():
                print(f"âœ… mycloset-ai í”„ë¡œì íŠ¸ ë°œê²¬: {project_root}")
                return project_root, backend_root
    
    # 3. ì•Œë ¤ì§„ ê²½ë¡œ í™•ì¸
    known_path = Path("/Users/gimdudeul/MVP/mycloset-ai")
    if known_path.exists():
        project_root = known_path
        backend_root = known_path / 'backend'
        print(f"âœ… ì•Œë ¤ì§„ ê²½ë¡œ ì‚¬ìš©: {project_root}")
        return project_root, backend_root
    
    # 4. í´ë°±
    print(f"âš ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€ ì‹¤íŒ¨, í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©")
    return current_dir, current_dir / 'backend'

# í•¨ìˆ˜ í˜¸ì¶œë¡œ ê²½ë¡œ ì„¤ì •
project_root, backend_root = detect_correct_project_structure()
ai_models_root = backend_root / "ai_models"


# ê²½ë¡œ ì¶”ê°€ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(backend_root))
sys.path.insert(0, str(backend_root / "app"))

print(f"ğŸ”¥ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€:")
print(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"   ë°±ì—”ë“œ ë£¨íŠ¸: {backend_root}")
print(f"   AI ëª¨ë¸ ë£¨íŠ¸: {ai_models_root}")

# =============================================================================
# ğŸ”¥ 1. GitHub Step ì •ë³´ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
# =============================================================================

@dataclass
class GitHubStepInfo:
    """GitHub Step ì •ë³´"""
    step_id: int
    step_name: str
    step_class: str
    module_path: str
    expected_models: List[str] = field(default_factory=list)
    expected_size_gb: float = 0.0
    expected_files: List[str] = field(default_factory=list)
    priority: str = "medium"

GITHUB_STEP_CONFIGS = [
    GitHubStepInfo(
        step_id=1,
        step_name="HumanParsingStep",
        step_class="HumanParsingStep",
        module_path="app.ai_pipeline.steps.step_01_human_parsing",
        expected_models=["Graphonomy", "SCHP"],
        expected_size_gb=1.2,
        expected_files=["graphonomy.pth", "schp_model.pth"],
        priority="critical"
    ),
    GitHubStepInfo(
        step_id=2,
        step_name="PoseEstimationStep", 
        step_class="PoseEstimationStep",
        module_path="app.ai_pipeline.steps.step_02_pose_estimation",
        expected_models=["OpenPose", "DWPose"],
        expected_size_gb=0.3,
        expected_files=["pose_model.pth", "dw-ll_ucoco_384.pth"],
        priority="critical"
    ),
    GitHubStepInfo(
        step_id=3,
        step_name="ClothSegmentationStep",
        step_class="ClothSegmentationStep", 
        module_path="app.ai_pipeline.steps.step_03_cloth_segmentation",
        expected_models=["SAM", "Segment Anything"],
        expected_size_gb=2.4,
        expected_files=["sam_vit_h.pth", "sam_vit_l.pth"],
        priority="critical"
    ),
    GitHubStepInfo(
        step_id=4,
        step_name="GeometricMatchingStep",
        step_class="GeometricMatchingStep",
        module_path="app.ai_pipeline.steps.step_04_geometric_matching", 
        expected_models=["GMM", "TOM"],
        expected_size_gb=0.05,
        expected_files=["gmm_model.pth", "tom_model.pth"],
        priority="high"
    ),
    GitHubStepInfo(
        step_id=5,
        step_name="ClothWarpingStep",
        step_class="ClothWarpingStep",
        module_path="app.ai_pipeline.steps.step_05_cloth_warping",
        expected_models=["RealVisXL", "Warping Model"],
        expected_size_gb=6.5,
        expected_files=["RealVisXL_V4.0.safetensors", "warping_model.pth"],
        priority="high"
    ),
    GitHubStepInfo(
        step_id=6,
        step_name="VirtualFittingStep",
        step_class="VirtualFittingStep",
        module_path="app.ai_pipeline.steps.step_06_virtual_fitting",
        expected_models=["OOTDiffusion", "Stable Diffusion"],
        expected_size_gb=14.0,
        expected_files=["ootd_hd_checkpoint.safetensors", "sd_model.safetensors"],
        priority="critical"  # ê°€ì¥ ì¤‘ìš”í•œ Step
    ),
    GitHubStepInfo(
        step_id=7,
        step_name="PostProcessingStep",
        step_class="PostProcessingStep",
        module_path="app.ai_pipeline.steps.step_07_post_processing",
        expected_models=["ESRGAN", "Real-ESRGAN"],
        expected_size_gb=0.8,
        expected_files=["esrgan_x8.pth", "realesrgan_x4.pth"],
        priority="medium"
    ),
    GitHubStepInfo(
        step_id=8,
        step_name="QualityAssessmentStep",
        step_class="QualityAssessmentStep",
        module_path="app.ai_pipeline.steps.step_08_quality_assessment",
        expected_models=["OpenCLIP", "CLIP"],
        expected_size_gb=5.2,
        expected_files=["ViT-L-14.pt", "clip_model.pt"],
        priority="medium"
    )
]

# =============================================================================
# ğŸ”¥ 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ìƒíƒœ ë° ë¶„ì„ ë°ì´í„° êµ¬ì¡°
# =============================================================================

class CheckpointLoadingStatus(Enum):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ìƒíƒœ"""
    NOT_FOUND = "not_found"
    CORRUPTED = "corrupted"
    LOADING_FAILED = "loading_failed"
    WEIGHTS_ONLY_FAILED = "weights_only_failed"
    DEVICE_INCOMPATIBLE = "device_incompatible"
    MEMORY_INSUFFICIENT = "memory_insufficient"
    SUCCESS = "success"
    SAFETENSORS_SUCCESS = "safetensors_success"

class GitHubStepStatus(Enum):
    """GitHub Step ìƒíƒœ"""
    NOT_FOUND = "not_found"
    IMPORT_FAILED = "import_failed"
    CLASS_NOT_FOUND = "class_not_found"
    INSTANCE_FAILED = "instance_failed"
    INIT_FAILED = "init_failed"
    DEPENDENCIES_MISSING = "dependencies_missing"
    AI_MODELS_FAILED = "ai_models_failed"
    CENTRAL_HUB_FAILED = "central_hub_failed"
    SYNTAX_ERROR = "syntax_error"
    THREADING_MISSING = "threading_missing"
    SUCCESS = "success"

@dataclass
class CheckpointAnalysisResult:
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼"""
    file_path: Path
    exists: bool
    size_mb: float
    file_hash: str = ""
    
    # ë¡œë”© í…ŒìŠ¤íŠ¸ ê²°ê³¼
    pytorch_weights_only_success: bool = False
    pytorch_regular_success: bool = False
    safetensors_success: bool = False
    legacy_load_success: bool = False
    
    # ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„
    checkpoint_keys: List[str] = field(default_factory=list)
    state_dict_keys: List[str] = field(default_factory=list)
    model_architecture: str = ""
    parameter_count: int = 0
    
    # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±
    cpu_compatible: bool = False
    cuda_compatible: bool = False
    mps_compatible: bool = False
    
    # ì˜¤ë¥˜ ì •ë³´
    loading_errors: List[str] = field(default_factory=list)
    status: CheckpointLoadingStatus = CheckpointLoadingStatus.NOT_FOUND
    load_time_seconds: float = 0.0
    memory_usage_mb: float = 0.0

@dataclass
class GitHubStepAnalysisResult:
    """GitHub Step ë¶„ì„ ê²°ê³¼"""
    step_info: GitHubStepInfo
    
    # Import ë¶„ì„
    import_success: bool = False
    import_time: float = 0.0
    import_errors: List[str] = field(default_factory=list)
    
    # íŒŒì¼ ìˆ˜ì • ìƒíƒœ
    syntax_error_fixed: bool = False
    threading_import_added: bool = False
    basestepmixin_compatible: bool = False
    
    # í´ë˜ìŠ¤ ë¶„ì„
    class_found: bool = False
    is_base_step_mixin: bool = False
    has_process_method: bool = False
    has_initialize_method: bool = False
    has_central_hub_support: bool = False
    
    # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë¶„ì„
    instance_created: bool = False
    constructor_params: Dict[str, Any] = field(default_factory=dict)
    instance_errors: List[str] = field(default_factory=list)
    
    # ì´ˆê¸°í™” ë¶„ì„
    initialization_success: bool = False
    initialization_time: float = 0.0
    initialization_errors: List[str] = field(default_factory=list)
    
    # ì˜ì¡´ì„± ë¶„ì„ (Central Hub ê¸°ë°˜)
    model_loader_injected: bool = False
    memory_manager_injected: bool = False
    data_converter_injected: bool = False
    central_hub_connected: bool = False
    dependency_validation_result: Dict[str, Any] = field(default_factory=dict)
    
    # AI ëª¨ë¸ ë¶„ì„
    detected_model_files: List[str] = field(default_factory=list)
    checkpoint_analyses: List[CheckpointAnalysisResult] = field(default_factory=list)
    total_model_size_gb: float = 0.0
    model_loading_success_rate: float = 0.0
    
    # ì„±ëŠ¥ ë¶„ì„
    memory_footprint_mb: float = 0.0
    inference_test_success: bool = False
    inference_time_ms: float = 0.0
    
    # ì „ì²´ ìƒíƒœ
    status: GitHubStepStatus = GitHubStepStatus.NOT_FOUND
    health_score: float = 0.0
    recommendations: List[str] = field(default_factory=list)

@dataclass
class GitHubSystemEnvironment:
    """GitHub ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„"""
    # í•˜ë“œì›¨ì–´ ì •ë³´
    is_m3_max: bool = False
    total_memory_gb: float = 0.0
    available_memory_gb: float = 0.0
    cpu_cores: int = 0
     # ğŸ”¥ ëˆ„ë½ëœ ì†ì„±ë“¤ ì¶”ê°€
    step_files_fixed: List[str] = field(default_factory=list)
    threading_imports_added: List[str] = field(default_factory=list)
    syntax_errors_fixed: int = 0
   
    # ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½
    python_version: str = ""
    conda_env: str = ""
    is_target_conda_env: bool = False
    
    # PyTorch í™˜ê²½
    torch_available: bool = False
    torch_version: str = ""
    cuda_available: bool = False
    mps_available: bool = False
    recommended_device: str = "cpu"
    
    # í”„ë¡œì íŠ¸ êµ¬ì¡°
    project_root_exists: bool = False
    backend_root_exists: bool = False
    ai_models_root_exists: bool = False
    ai_models_size_gb: float = 0.0
    step_modules_found: List[str] = field(default_factory=list)
    
    # ì˜ì¡´ì„± ìƒíƒœ
    core_dependencies: Dict[str, bool] = field(default_factory=dict)
    github_integrations: Dict[str, bool] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ 3. ê³ ê¸‰ ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €
# =============================================================================

class GitHubSafetyManager:
    """GitHub í”„ë¡œì íŠ¸ìš© ê°•í™”ëœ ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.timeout_duration = 180  # 3ë¶„ íƒ€ì„ì•„ì›ƒ (GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ìš©)
        self.max_memory_gb = 12     # 12GB ë©”ëª¨ë¦¬ ì œí•œ (M3 Max ê³ ë ¤)
        self.active_operations = {}
        self.start_time = time.time()
        
    @contextmanager
    def safe_execution(self, operation_name: str, timeout: int = None, memory_limit_gb: float = None):
        """GitHub í”„ë¡œì íŠ¸ìš© ì´ˆì•ˆì „ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
        operation_id = f"github_{operation_name.replace(' ', '_')}_{int(time.time() * 1000)}"
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / (1024**3)
        timeout = timeout or self.timeout_duration
        memory_limit = memory_limit_gb or self.max_memory_gb
        
        print(f"ğŸ”’ [{operation_id}] GitHub ì•ˆì „ ì‹¤í–‰ ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ, ë©”ëª¨ë¦¬ ì œí•œ: {memory_limit:.1f}GB)")
        
        self.active_operations[operation_id] = {
            'start_time': start_time,
            'start_memory': start_memory,
            'timeout': timeout,
            'memory_limit': memory_limit
        }
        
        try:
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
            monitoring_thread = threading.Thread(
                target=self._monitor_github_operation,
                args=(operation_id, timeout, memory_limit),
                daemon=True
            )
            monitoring_thread.start()
            
            yield
            
        except TimeoutError:
            print(f"â° [{operation_id}] GitHub ì‘ì—… íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
            raise
        except MemoryError:
            print(f"ğŸ’¾ [{operation_id}] GitHub ë©”ëª¨ë¦¬ í•œê³„ ì´ˆê³¼ ({memory_limit:.1f}GB)")
            raise
        except Exception as e:
            print(f"âŒ [{operation_id}] GitHub ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            if hasattr(e, '__traceback__'):
                tb_lines = traceback.format_tb(e.__traceback__)
                if tb_lines:
                    print(f"   ìŠ¤íƒ ì¶”ì : {tb_lines[-1].strip()}")
            raise
        finally:
            if operation_id in self.active_operations:
                del self.active_operations[operation_id]
                
            elapsed = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / (1024**3)
            memory_used = end_memory - start_memory
            
            print(f"âœ… [{operation_id}] GitHub ì‘ì—… ì™„ë£Œ ({elapsed:.2f}ì´ˆ, ë©”ëª¨ë¦¬: +{memory_used:.2f}GB)")
            
            # GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ìš© ë©”ëª¨ë¦¬ ì •ë¦¬
            if memory_used > 1.0:  # 1GB ì´ìƒ ì‚¬ìš©ì‹œ ì ê·¹ì  ì •ë¦¬
                gc.collect()
    
    def _monitor_github_operation(self, operation_id: str, timeout: float, memory_limit: float):
        """GitHub ì‘ì—… ëª¨ë‹ˆí„°ë§"""
        try:
            while operation_id in self.active_operations:
                current_time = time.time()
                operation = self.active_operations.get(operation_id)
                
                if not operation:
                    break
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                elapsed = current_time - operation['start_time']
                if elapsed > timeout:
                    print(f"âš ï¸ [{operation_id}] GitHub ì‘ì—… íƒ€ì„ì•„ì›ƒ ê²½ê³  ({elapsed:.1f}ì´ˆ/{timeout}ì´ˆ)")
                    break
                
                # ë©”ëª¨ë¦¬ ì²´í¬ (GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ ê³ ë ¤)
                current_memory = psutil.Process().memory_info().rss / (1024**3)
                if current_memory > memory_limit:
                    print(f"âš ï¸ [{operation_id}] GitHub ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³  ({current_memory:.1f}GB/{memory_limit:.1f}GB)")
                    # M3 Maxì—ì„œëŠ” ë” ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬
                    if current_memory > memory_limit * 1.5:  # 1.5ë°° ì´ˆê³¼ì‹œì—ë§Œ ì¤‘ë‹¨
                        break
                
                time.sleep(2)  # 2ì´ˆë§ˆë‹¤ ì²´í¬ (GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ìš©)
                
        except Exception:
            pass

# ì „ì—­ GitHub ì•ˆì „ ë§¤ë‹ˆì €
github_safety = GitHubSafetyManager()


# =============================================================================
# ğŸ”¥ 5. GitHub ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸°
# =============================================================================

class GitHubCheckpointAnalyzer:
    """GitHub í”„ë¡œì íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
        self.torch_available = False
        self.safetensors_available = False
        
        try:
            import torch
            self.torch_available = True
            self.torch = torch
        except ImportError:
            pass
            
        try:
            from safetensors.torch import load_file
            self.safetensors_available = True
            self.safetensors_load = load_file
        except ImportError:
            pass
    
    def analyze_checkpoint(self, checkpoint_path: Path) -> CheckpointAnalysisResult:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì™„ì „ ë¶„ì„"""
        
        analysis = CheckpointAnalysisResult(
            file_path=checkpoint_path,
            exists=checkpoint_path.exists(),
            size_mb=0.0
        )
        
        if not analysis.exists:
            analysis.status = CheckpointLoadingStatus.NOT_FOUND
            return analysis
        
        # íŒŒì¼ í¬ê¸° ë° í•´ì‹œ
        try:
            stat_info = checkpoint_path.stat()
            analysis.size_mb = stat_info.st_size / (1024 * 1024)
            
            # í•´ì‹œ ê³„ì‚° (ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ìƒ˜í”Œë§)
            if analysis.size_mb < 500:  # 500MB ë¯¸ë§Œë§Œ ì „ì²´ í•´ì‹œ
                analysis.file_hash = self._calculate_file_hash(checkpoint_path)
            else:
                analysis.file_hash = self._calculate_sample_hash(checkpoint_path)
                
        except Exception as e:
            analysis.loading_errors.append(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        # GitHub ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        if self.torch_available:
            self._test_github_pytorch_loading(analysis)
        
        if self.safetensors_available and checkpoint_path.suffix == '.safetensors':
            self._test_github_safetensors_loading(analysis)
        
        # ìƒíƒœ ê²°ì •
        if analysis.pytorch_weights_only_success or analysis.pytorch_regular_success or analysis.safetensors_success:
            analysis.status = CheckpointLoadingStatus.SAFETENSORS_SUCCESS if analysis.safetensors_success else CheckpointLoadingStatus.SUCCESS
        elif analysis.loading_errors:
            if any("corrupted" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointLoadingStatus.CORRUPTED
            elif any("weights_only" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointLoadingStatus.WEIGHTS_ONLY_FAILED
            elif any("memory" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointLoadingStatus.MEMORY_INSUFFICIENT
            else:
                analysis.status = CheckpointLoadingStatus.LOADING_FAILED
        
        return analysis
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """ì „ì²´ íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _calculate_sample_hash(self, file_path: Path, sample_size: int = 2*1024*1024) -> str:
        """ìƒ˜í”Œ í•´ì‹œ ê³„ì‚° (GitHub ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)"""
        try:
            hash_md5 = hashlib.md5()
            file_size = file_path.stat().st_size
            
            with open(file_path, "rb") as f:
                # ì‹œì‘ ë¶€ë¶„
                chunk = f.read(sample_size)
                hash_md5.update(chunk)
                
                # ì¤‘ê°„ ë¶€ë¶„
                if file_size > sample_size * 3:
                    f.seek(file_size // 2)
                    chunk = f.read(sample_size)
                    hash_md5.update(chunk)
                
                # ë ë¶€ë¶„
                if file_size > sample_size * 2:
                    f.seek(max(0, file_size - sample_size))
                    chunk = f.read(sample_size)
                    hash_md5.update(chunk)
            
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _test_github_pytorch_loading(self, analysis: CheckpointAnalysisResult):
        """GitHub PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„ ì•ˆì „ ë¡œë”©)"""
        if not self.torch_available:
            analysis.loading_errors.append("PyTorch ì—†ìŒ")
            return
        
        file_path = analysis.file_path
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / (1024**2)
        
        # 1ë‹¨ê³„: weights_only=True ì‹œë„ (GitHub ê¶Œì¥)
        try:
            with github_safety.safe_execution(f"PyTorch weights_only ë¡œë”© {file_path.name}", timeout=120):
                checkpoint = self.torch.load(file_path, map_location=self.device, weights_only=True)
                analysis.pytorch_weights_only_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… weights_only ë¡œë”© ì„±ê³µ")
                return  # ì„±ê³µí•˜ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„í•˜ì§€ ì•ŠìŒ
                
        except Exception as e:
            analysis.loading_errors.append(f"weights_only ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ weights_only ì‹¤íŒ¨: {str(e)[:100]}")
        
        # 2ë‹¨ê³„: weights_only=False ì‹œë„ (GitHub í˜¸í™˜ì„±)
        try:
            with github_safety.safe_execution(f"PyTorch ì¼ë°˜ ë¡œë”© {file_path.name}", timeout=120):
                checkpoint = self.torch.load(file_path, map_location=self.device, weights_only=False)
                analysis.pytorch_regular_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… ì¼ë°˜ ë¡œë”© ì„±ê³µ")
                return
                
        except Exception as e:
            analysis.loading_errors.append(f"ì¼ë°˜ ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ ì¼ë°˜ ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
        
        # 3ë‹¨ê³„: ë ˆê±°ì‹œ ë¡œë”© ì‹œë„ (GitHub ë ˆê±°ì‹œ ì§€ì›)
        try:
            with github_safety.safe_execution(f"PyTorch ë ˆê±°ì‹œ ë¡œë”© {file_path.name}", timeout=120):
                checkpoint = self.torch.load(file_path, map_location=self.device)
                analysis.legacy_load_success = True
                analysis.pytorch_regular_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… ë ˆê±°ì‹œ ë¡œë”© ì„±ê³µ")
                
        except Exception as e:
            analysis.loading_errors.append(f"ë ˆê±°ì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ ë ˆê±°ì‹œ ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
        
        # ì„±ëŠ¥ ì¸¡ì •
        analysis.load_time_seconds = time.time() - start_time
        end_memory = psutil.Process().memory_info().rss / (1024**2)
        analysis.memory_usage_mb = end_memory - start_memory
    
    def _test_github_safetensors_loading(self, analysis: CheckpointAnalysisResult):
        """GitHub SafeTensors ë¡œë”© í…ŒìŠ¤íŠ¸"""
        if not self.safetensors_available:
            analysis.loading_errors.append("SafeTensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            return
        
        try:
            with github_safety.safe_execution(f"SafeTensors ë¡œë”© {analysis.file_path.name}", timeout=120):
                checkpoint = self.safetensors_load(str(analysis.file_path))
                analysis.safetensors_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… SafeTensors ë¡œë”© ì„±ê³µ")
                
        except Exception as e:
            analysis.loading_errors.append(f"SafeTensors ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ SafeTensors ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_checkpoint_content(self, analysis: CheckpointAnalysisResult, checkpoint):
        """GitHub ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„"""
        try:
            # State dict ì¶”ì¶œ
            state_dict = checkpoint
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    analysis.checkpoint_keys = [k for k in checkpoint.keys() if k != 'state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                    analysis.checkpoint_keys = [k for k in checkpoint.keys() if k != 'model']
                else:
                    analysis.checkpoint_keys = list(checkpoint.keys())[:20]  # ì²˜ìŒ 20ê°œë§Œ
            
            if isinstance(state_dict, dict):
                analysis.state_dict_keys = list(state_dict.keys())[:30]  # ì²˜ìŒ 30ê°œë§Œ
                
                # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                param_count = 0
                for key, tensor in state_dict.items():
                    if hasattr(tensor, 'numel'):
                        param_count += tensor.numel()
                analysis.parameter_count = param_count
                
                # GitHub ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶”ì •
                analysis.model_architecture = self._estimate_github_architecture(state_dict)
                
                # GitHub ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
                self._test_github_device_compatibility(analysis, state_dict)
            
        except Exception as e:
            analysis.loading_errors.append(f"GitHub ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _estimate_github_architecture(self, state_dict: dict) -> str:
        """GitHub ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶”ì •"""
        keys = list(state_dict.keys())
        key_str = ' '.join(keys).lower()
        
        # GitHub í”„ë¡œì íŠ¸ íŠ¹í™” ëª¨ë¸ ê°ì§€
        if any(keyword in key_str for keyword in ['parsing', 'human_parsing', 'schp', 'graphonomy']):
            return "Human Parsing Model (SCHP/Graphonomy)"
        elif any(keyword in key_str for keyword in ['pose', 'openpose', 'dwpose', 'keypoint']):
            return "Pose Estimation Model (OpenPose/DWPose)"
        elif any(keyword in key_str for keyword in ['sam', 'segment_anything', 'mask_decoder']):
            return "Segmentation Model (SAM)"
        elif any(keyword in key_str for keyword in ['ootd', 'diffusion', 'unet', 'vae']):
            return "Diffusion Model (OOTDiffusion)"
        elif any(keyword in key_str for keyword in ['gmm', 'geometric', 'matching']):
            return "Geometric Matching Model (GMM)"
        elif any(keyword in key_str for keyword in ['esrgan', 'realesrgan', 'generator']):
            return "Super Resolution Model (ESRGAN)"
        elif any(keyword in key_str for keyword in ['clip', 'openclip', 'vision_model', 'text_model']):
            return "Vision-Language Model (CLIP)"
        elif any(keyword in key_str for keyword in ['vit', 'transformer', 'attention']):
            return "Vision Transformer"
        elif any(keyword in key_str for keyword in ['resnet', 'efficientnet', 'backbone']):
            return "CNN Backbone"
        else:
            return f"Unknown Architecture ({len(keys)} layers)"
    
    def _test_github_device_compatibility(self, analysis: CheckpointAnalysisResult, state_dict: dict):
        """GitHub ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        if not self.torch_available:
            return
        
        try:
            # ì²« ë²ˆì§¸ í…ì„œë¡œ í…ŒìŠ¤íŠ¸
            first_tensor = None
            for value in state_dict.values():
                if hasattr(value, 'to'):
                    first_tensor = value
                    break
            
            if first_tensor is None:
                return
            
            # CPU í…ŒìŠ¤íŠ¸
            try:
                cpu_tensor = first_tensor.to('cpu')
                analysis.cpu_compatible = True
            except Exception as e:
                analysis.loading_errors.append(f"CPU í˜¸í™˜ì„± ì‹¤íŒ¨: {e}")
            
            # CUDA í…ŒìŠ¤íŠ¸
            if self.torch.cuda.is_available():
                try:
                    cuda_tensor = first_tensor.to('cuda')
                    analysis.cuda_compatible = True
                except Exception as e:
                    analysis.loading_errors.append(f"CUDA í˜¸í™˜ì„± ì‹¤íŒ¨: {e}")
            
            # MPS í…ŒìŠ¤íŠ¸ (M3 Max íŠ¹í™”)
            if hasattr(self.torch.backends, 'mps') and self.torch.backends.mps.is_available():
                try:
                    mps_tensor = first_tensor.to('mps')
                    analysis.mps_compatible = True
                except Exception as e:
                    analysis.loading_errors.append(f"MPS í˜¸í™˜ì„± ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            analysis.loading_errors.append(f"ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 6. GitHub ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„ê¸°
# =============================================================================

class GitHubSystemAnalyzer:
    """GitHub í”„ë¡œì íŠ¸ ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.environment = GitHubSystemEnvironment()
        
    def analyze_github_environment(self) -> GitHubSystemEnvironment:
        """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ì™„ì „ ë¶„ì„"""
        
        print("ğŸ“Š GitHub í”„ë¡œì íŠ¸ ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        with github_safety.safe_execution("GitHub ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„", timeout=90):
            # 1. Step íŒŒì¼ ìˆ˜ì • ë¨¼ì € ì‹¤í–‰
            
            # 2. ê¸°ì¡´ ì‹œìŠ¤í…œ ë¶„ì„
            self._analyze_hardware()
            self._analyze_software_environment()
            self._analyze_pytorch_environment()
            self._analyze_github_project_structure()
            self._analyze_dependencies()
            self._analyze_github_integrations()
        
        return self.environment
    
    def _check_step_file_fixes(self):
        """Step íŒŒì¼ ìˆ˜ì • ìƒíƒœ í™•ì¸"""
        try:
            steps_dir = backend_root / "app" / "ai_pipeline" / "steps"
            if not steps_dir.exists():
                return
            
            fixed_files = []
            threading_added = []
            syntax_fixed = 0
            
            # Step íŒŒì¼ë“¤ í™•ì¸
            step_files = list(steps_dir.glob("step_*.py"))
            for step_file in step_files:
                try:
                    with open(step_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # threading import í™•ì¸
                    if 'import threading' in content:
                        threading_added.append(step_file.name)
                    
                    # syntax ì˜¤ë¥˜ ìˆ˜ì • í™•ì¸ (ê°„ë‹¨í•œ ì²´í¬)
                    try:
                        compile(content, step_file, 'exec')
                        fixed_files.append(step_file.name)
                        syntax_fixed += 1
                    except SyntaxError:
                        pass
                        
                except Exception as e:
                    continue
            
            self.environment.step_files_fixed = fixed_files
            self.environment.threading_imports_added = threading_added
            self.environment.syntax_errors_fixed = syntax_fixed
            
            print(f"   ğŸ”§ Step íŒŒì¼ ìˆ˜ì • ìƒíƒœ:")
            print(f"      ìˆ˜ì •ëœ íŒŒì¼: {len(fixed_files)}ê°œ")
            print(f"      threading ì¶”ê°€: {len(threading_added)}ê°œ")
            print(f"      syntax ìˆ˜ì •: {syntax_fixed}ê°œ")
            
        except Exception as e:
            print(f"   âŒ Step íŒŒì¼ ìˆ˜ì • ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")



    def _analyze_hardware(self):
        """í•˜ë“œì›¨ì–´ ë¶„ì„ (M3 Max íŠ¹í™”)"""
        try:
            # CPU ì •ë³´
            self.environment.cpu_cores = psutil.cpu_count(logical=True)
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            self.environment.total_memory_gb = memory.total / (1024**3)
            self.environment.available_memory_gb = memory.available / (1024**3)
            
            # M3 Max ê°ì§€ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
            if platform.system() == 'Darwin' and platform.machine() == 'arm64':
                try:
                    result = subprocess.run(
                        ['sysctl', '-n', 'machdep.cpu.brand_string'],
                        capture_output=True, text=True, timeout=5
                    )
                    if 'M3' in result.stdout:
                        self.environment.is_m3_max = True
                        
                        # ë©”ëª¨ë¦¬ ì •í™•í•œ ì¸¡ì •
                        memory_result = subprocess.run(
                            ['sysctl', '-n', 'hw.memsize'],
                            capture_output=True, text=True, timeout=5
                        )
                        if memory_result.returncode == 0:
                            exact_memory_gb = int(memory_result.stdout.strip()) / (1024**3)
                            self.environment.total_memory_gb = round(exact_memory_gb, 1)
                            
                except Exception as e:
                    print(f"âš ï¸ M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
            
            print(f"   ğŸ’» í•˜ë“œì›¨ì–´: {self.environment.cpu_cores}ì½”ì–´, {self.environment.total_memory_gb:.1f}GB")
            print(f"   ğŸš€ M3 Max: {'âœ…' if self.environment.is_m3_max else 'âŒ'}")
            
        except Exception as e:
            print(f"âŒ í•˜ë“œì›¨ì–´ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_software_environment(self):
        """ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½ ë¶„ì„ (conda íŠ¹í™”)"""
        try:
            # Python ì •ë³´
            self.environment.python_version = sys.version.split()[0]
            
            # conda í™˜ê²½ ì •ë³´ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
            conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'none')
            self.environment.conda_env = conda_env
            self.environment.is_target_conda_env = (conda_env == 'mycloset-ai-clean')
            
            print(f"   ğŸ Python: {self.environment.python_version}")
            print(f"   ğŸ“¦ Conda í™˜ê²½: {conda_env}")
            print(f"   âœ… íƒ€ê²Ÿ í™˜ê²½: {'âœ…' if self.environment.is_target_conda_env else 'âŒ'} (mycloset-ai-clean)")
            
        except Exception as e:
            print(f"âŒ ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_pytorch_environment(self):
        """PyTorch í™˜ê²½ ë¶„ì„ (MPS íŠ¹í™”)"""
        try:
            # PyTorch ê°€ìš©ì„± í™•ì¸
            try:
                import torch
                self.environment.torch_available = True
                self.environment.torch_version = torch.__version__
                
                # ë””ë°”ì´ìŠ¤ ì§€ì› í™•ì¸
                self.environment.cuda_available = torch.cuda.is_available()
                self.environment.mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
                
                # ì¶”ì²œ ë””ë°”ì´ìŠ¤ ê²°ì • (M3 Max MPS ìš°ì„ )
                if self.environment.mps_available and self.environment.is_m3_max:
                    self.environment.recommended_device = 'mps'
                elif self.environment.cuda_available:
                    self.environment.recommended_device = 'cuda'
                else:
                    self.environment.recommended_device = 'cpu'
                    
                print(f"   ğŸ”¥ PyTorch: {self.environment.torch_version}")
                print(f"   âš¡ MPS: {'âœ…' if self.environment.mps_available else 'âŒ'}")
                print(f"   ğŸ¯ ì¶”ì²œ ë””ë°”ì´ìŠ¤: {self.environment.recommended_device}")
                
            except ImportError:
                self.environment.torch_available = False
                print(f"   âŒ PyTorch ì—†ìŒ")
            
        except Exception as e:
            print(f"âŒ PyTorch í™˜ê²½ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_github_project_structure(self):
        """GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        try:
            # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
            self.environment.project_root_exists = project_root.exists()
            self.environment.backend_root_exists = backend_root.exists()
            self.environment.ai_models_root_exists = ai_models_root.exists()
            
            # AI ëª¨ë¸ í¬ê¸° ê³„ì‚°
            if ai_models_root.exists():
                total_size = 0
                model_count = 0
                for model_file in ai_models_root.rglob('*'):
                    if model_file.is_file() and model_file.suffix in ['.pth', '.pt', '.safetensors', '.bin', '.ckpt']:
                        total_size += model_file.stat().st_size
                        model_count += 1
                
                self.environment.ai_models_size_gb = total_size / (1024**3)
                
                print(f"   ğŸ“ AI ëª¨ë¸: {model_count}ê°œ íŒŒì¼, {self.environment.ai_models_size_gb:.1f}GB")
            else:
                print(f"   âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {ai_models_root}")
            
            # Step ëª¨ë“ˆ ì°¾ê¸°
            steps_dir = backend_root / "app" / "ai_pipeline" / "steps"
            if steps_dir.exists():
                step_files = list(steps_dir.glob("step_*.py"))
                self.environment.step_modules_found = [f.stem for f in step_files]
                print(f"   ğŸš€ Step ëª¨ë“ˆ: {len(step_files)}ê°œ ë°œê²¬")
            
            structure_ready = all([
                self.environment.project_root_exists,
                self.environment.backend_root_exists,
                self.environment.ai_models_root_exists
            ])
            print(f"   ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°: {'âœ…' if structure_ready else 'âš ï¸'}")
            
        except Exception as e:
            print(f"âŒ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_dependencies(self):
        """í•µì‹¬ ì˜ì¡´ì„± ë¶„ì„"""
        try:
            dependencies = {
                'torch': False,
                'torchvision': False,
                'numpy': False,
                'PIL': False,
                'cv2': False,
                'transformers': False,
                'safetensors': False,
                'psutil': False,
                'threading': True  # í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
            }
            
            for dep in dependencies.keys():
                if dep == 'threading':
                    continue  # ì´ë¯¸ ì„¤ì •ë¨
                try:
                    if dep == 'PIL':
                        import PIL.Image
                    elif dep == 'cv2':
                        import cv2
                    else:
                        importlib.import_module(dep)
                    dependencies[dep] = True
                except ImportError:
                    pass
            
            self.environment.core_dependencies = dependencies
            
            success_count = sum(dependencies.values())
            total_count = len(dependencies)
            print(f"   ğŸ“¦ í•µì‹¬ ì˜ì¡´ì„±: {success_count}/{total_count} ì„±ê³µ")
            
        except Exception as e:
            print(f"âŒ ì˜ì¡´ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_github_integrations(self):
        """GitHub í†µí•© ìƒíƒœ ë¶„ì„"""
        try:
            integrations = {
                'base_step_mixin': False,
                'model_loader': False,
                'step_factory': False,
                'implementation_manager': False,
                'auto_model_detector': False
            }
            
            # BaseStepMixin í™•ì¸
            try:
                from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
                integrations['base_step_mixin'] = True
            except ImportError:
                pass
            
            # ModelLoader í™•ì¸
            try:
                from app.ai_pipeline.utils.model_loader import ModelLoader
                integrations['model_loader'] = True
            except ImportError:
                pass
            
            # StepFactory í™•ì¸
            try:
                from app.ai_pipeline.utils.step_factory import StepFactory
                integrations['step_factory'] = True
            except ImportError:
                pass
            
            # RealAIStepImplementationManager í™•ì¸
            try:
                from app.services.step_implementations import RealAIStepImplementationManager
                integrations['implementation_manager'] = True
            except ImportError:
                pass
            
            # AutoModelDetector í™•ì¸
            try:
                from app.ai_pipeline.utils.auto_model_detector import AutoModelDetector
                integrations['auto_model_detector'] = True
            except ImportError:
                pass
            
            self.environment.github_integrations = integrations
            
            success_count = sum(integrations.values())
            total_count = len(integrations)
            print(f"   ğŸ”— GitHub í†µí•©: {success_count}/{total_count} ì„±ê³µ")
            
        except Exception as e:
            print(f"âŒ GitHub í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 7. GitHub Step ë¶„ì„ê¸°
# =============================================================================

class GitHubStepAnalyzer:
    """GitHub Step ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self, system_env: GitHubSystemEnvironment):
        self.system_env = system_env
        self.checkpoint_analyzer = GitHubCheckpointAnalyzer(
            device=system_env.recommended_device
        )
    
    def analyze_github_step(self, step_info: GitHubStepInfo) -> GitHubStepAnalysisResult:
        """GitHub Step ì™„ì „ ë¶„ì„"""
        
        print(f"\nğŸ”§ {step_info.step_name} (Step {step_info.step_id}) ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        analysis = GitHubStepAnalysisResult(step_info=step_info)
        
        # ìˆ˜ì • ìƒíƒœ í™•ì¸
        step_file_name = f"step_{step_info.step_id:02d}_{step_info.step_name.lower().replace('step', '')}.py"
        analysis.syntax_error_fixed = step_file_name in self.system_env.step_files_fixed
        analysis.threading_import_added = step_file_name in self.system_env.threading_imports_added
        
        # 1. Import í…ŒìŠ¤íŠ¸
        self._test_github_import(analysis)
        
        # 2. í´ë˜ìŠ¤ ë¶„ì„
        if analysis.import_success:
            self._analyze_github_class(analysis)
        
        # 3. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        if analysis.class_found:
            self._test_github_instance_creation(analysis)
        
        # 4. ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        if analysis.instance_created:
            self._test_github_initialization(analysis)
        
        # 5. GitHub Central Hub ì˜ì¡´ì„± ë¶„ì„
        if analysis.instance_created:
            self._analyze_github_dependencies(analysis)
        
        # 6. AI ëª¨ë¸ ë¶„ì„
        self._analyze_github_ai_models(analysis)
        
        # 7. ìƒíƒœ ê²°ì • ë° ì ìˆ˜ ê³„ì‚°
        self._determine_github_status_and_score(analysis)
        
        return analysis
    
    def _test_github_import(self, analysis: GitHubStepAnalysisResult):
        """GitHub Step Import í…ŒìŠ¤íŠ¸"""
        try:
            with github_safety.safe_execution(f"{analysis.step_info.step_name} Import", timeout=60):
                start_time = time.time()
                
                # ë™ì  import ì‹œë„
                module = importlib.import_module(analysis.step_info.module_path)
                analysis.import_time = time.time() - start_time
                analysis.import_success = True
                
                # í´ë˜ìŠ¤ ì¡´ì¬ í™•ì¸
                if hasattr(module, analysis.step_info.step_class):
                    analysis.class_found = True
                    print(f"   âœ… Import ì„±ê³µ ({analysis.import_time:.3f}ì´ˆ)")
                else:
                    analysis.import_errors.append(f"í´ë˜ìŠ¤ {analysis.step_info.step_class} ì—†ìŒ")
                    print(f"   âŒ í´ë˜ìŠ¤ ì—†ìŒ: {analysis.step_info.step_class}")
                    
        except Exception as e:
            analysis.import_errors.append(str(e))
            if "invalid syntax" in str(e).lower():
                analysis.status = GitHubStepStatus.SYNTAX_ERROR
            elif "threading" in str(e).lower():
                analysis.status = GitHubStepStatus.THREADING_MISSING
            else:
                analysis.status = GitHubStepStatus.IMPORT_FAILED
            print(f"   âŒ Import ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_class(self, analysis: GitHubStepAnalysisResult):
        """GitHub í´ë˜ìŠ¤ êµ¬ì¡° ë¶„ì„"""
        try:
            module = importlib.import_module(analysis.step_info.module_path)
            step_class = getattr(module, analysis.step_info.step_class)
            
            # í´ë˜ìŠ¤ ë©”ì„œë“œ ê²€ì‚¬
            class_methods = [method for method in dir(step_class) if not method.startswith('_')]
            
            analysis.has_process_method = 'process' in class_methods
            analysis.has_initialize_method = 'initialize' in class_methods
            
            # BaseStepMixin ìƒì† í™•ì¸ (GitHub íŠ¹í™”)
            mro = inspect.getmro(step_class)
            analysis.is_base_step_mixin = any('BaseStepMixin' in cls.__name__ for cls in mro)
            analysis.basestepmixin_compatible = analysis.is_base_step_mixin
            
            # Central Hub ì§€ì› í™•ì¸
            analysis.has_central_hub_support = any(
                hasattr(step_class, attr) for attr in [
                    'central_hub_container', 'dependency_manager', 'model_interface'
                ]
            )
            
            print(f"   âœ… í´ë˜ìŠ¤ ë¶„ì„: BaseStepMixin={analysis.is_base_step_mixin}, CentralHub={analysis.has_central_hub_support}")
            
        except Exception as e:
            analysis.import_errors.append(f"í´ë˜ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            print(f"   âŒ í´ë˜ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _test_github_instance_creation(self, analysis: GitHubStepAnalysisResult):
        """GitHub ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
        try:
            with github_safety.safe_execution(f"{analysis.step_info.step_name} ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", timeout=90):
                module = importlib.import_module(analysis.step_info.module_path)
                step_class = getattr(module, analysis.step_info.step_class)
                
                # ìƒì„±ì íŒŒë¼ë¯¸í„° ë¶„ì„
                signature = inspect.signature(step_class.__init__)
                params = list(signature.parameters.keys())[1:]  # self ì œì™¸
                
                # GitHub í”„ë¡œì íŠ¸ ê¸°ë³¸ ì˜ì¡´ì„± ì¤€ë¹„
                constructor_args = {
                    'device': self.system_env.recommended_device,
                    'strict_mode': False
                }
                
                # ì„ íƒì  ì˜ì¡´ì„± ì²˜ë¦¬
                if 'model_loader' in params:
                    constructor_args['model_loader'] = None  # ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì²˜ë¦¬
                if 'memory_manager' in params:
                    constructor_args['memory_manager'] = None
                if 'data_converter' in params:
                    constructor_args['data_converter'] = None
                
                analysis.constructor_params = constructor_args
                
                # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œë„
                step_instance = step_class(**constructor_args)
                analysis.instance_created = True
                
                print(f"   âœ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
                
        except Exception as e:
            analysis.instance_errors.append(str(e))
            analysis.status = GitHubStepStatus.INSTANCE_FAILED
            print(f"   âŒ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _test_github_initialization(self, analysis: GitHubStepAnalysisResult):
        """GitHub ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        if not analysis.instance_created:
            return
        
        try:
            with github_safety.safe_execution(f"{analysis.step_info.step_name} ì´ˆê¸°í™”", timeout=180):
                module = importlib.import_module(analysis.step_info.module_path)
                step_class = getattr(module, analysis.step_info.step_class)
                step_instance = step_class(**analysis.constructor_params)
                
                start_time = time.time()
                
                if hasattr(step_instance, 'initialize'):
                    if asyncio.iscoroutinefunction(step_instance.initialize):
                        # ë¹„ë™ê¸° ì´ˆê¸°í™”
                        try:
                            loop = asyncio.get_event_loop()
                        except RuntimeError:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                        
                        result = loop.run_until_complete(
                            asyncio.wait_for(step_instance.initialize(), timeout=120.0)
                        )
                    else:
                        # ë™ê¸° ì´ˆê¸°í™”
                        result = step_instance.initialize()
                    
                    if result:
                        analysis.initialization_success = True
                        analysis.initialization_time = time.time() - start_time
                        print(f"   âœ… ì´ˆê¸°í™” ì„±ê³µ ({analysis.initialization_time:.2f}ì´ˆ)")
                    else:
                        analysis.initialization_errors.append("ì´ˆê¸°í™”ê°€ False ë°˜í™˜")
                        print(f"   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: False ë°˜í™˜")
                        
                else:
                    # initialize ë©”ì„œë“œê°€ ì—†ëŠ” ê²½ìš°
                    analysis.initialization_success = True
                    print(f"   âš ï¸ initialize ë©”ì„œë“œ ì—†ìŒ (ê¸°ë³¸ ì„±ê³µ ì²˜ë¦¬)")
                    
        except TimeoutError:
            analysis.initialization_errors.append("ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ (120ì´ˆ)")
            print(f"   âŒ ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            analysis.initialization_errors.append(str(e))
            analysis.status = GitHubStepStatus.INIT_FAILED
            print(f"   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_dependencies(self, analysis: GitHubStepAnalysisResult):
        """GitHub Central Hub ì˜ì¡´ì„± ë¶„ì„"""
        try:
            module = importlib.import_module(analysis.step_info.module_path)
            step_class = getattr(module, analysis.step_info.step_class)
            step_instance = step_class(**analysis.constructor_params)
            
            # ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸
            dependency_results = {}
            
            # ModelLoader ì£¼ì… í…ŒìŠ¤íŠ¸
            if hasattr(step_instance, 'set_model_loader'):
                try:
                    # Mock ModelLoaderë¡œ í…ŒìŠ¤íŠ¸
                    class MockModelLoader:
                        def load_model(self, *args, **kwargs):
                            return {"mock": "model"}
                        def create_step_interface(self, step_name):
                            return {"interface": step_name}
                    
                    mock_loader = MockModelLoader()
                    step_instance.set_model_loader(mock_loader)
                    analysis.model_loader_injected = hasattr(step_instance, 'model_loader')
                    dependency_results['model_loader'] = analysis.model_loader_injected
                except Exception as e:
                    dependency_results['model_loader'] = False
            
            # Central Hub ì—°ê²° í™•ì¸
            if hasattr(step_instance, 'central_hub_container'):
                analysis.central_hub_connected = step_instance.central_hub_container is not None
                dependency_results['central_hub'] = analysis.central_hub_connected
            
            # ì˜ì¡´ì„± ê²€ì¦ ë©”ì„œë“œ í˜¸ì¶œ
            if hasattr(step_instance, 'validate_dependencies'):
                try:
                    validation_result = step_instance.validate_dependencies()
                    analysis.dependency_validation_result = validation_result
                    dependency_results['validation'] = isinstance(validation_result, dict)
                except Exception as e:
                    dependency_results['validation'] = False
            
            success_count = sum(dependency_results.values())
            total_count = len(dependency_results)
            
            print(f"   ğŸ”— ì˜ì¡´ì„±: {success_count}/{total_count} ì„±ê³µ")
            
        except Exception as e:
            print(f"   âŒ ì˜ì¡´ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_ai_models(self, analysis: GitHubStepAnalysisResult):
        """GitHub AI ëª¨ë¸ ë¶„ì„"""
        try:
            step_info = analysis.step_info
            
            # Stepë³„ ëª¨ë¸ ë””ë ‰í† ë¦¬ íŒ¨í„´ (GitHub êµ¬ì¡° ê¸°ë°˜)
            model_patterns = [
                f"step_{step_info.step_id:02d}_*",
                f"*{step_info.step_name.lower().replace('step', '')}*",
                step_info.step_name.lower()
            ]
            
            model_files = []
            total_size = 0
            
            # AI ëª¨ë¸ ë£¨íŠ¸ì—ì„œ ê²€ìƒ‰
            if ai_models_root.exists():
                for pattern in model_patterns:
                    matching_dirs = list(ai_models_root.glob(pattern))
                    for model_dir in matching_dirs:
                        if model_dir.is_dir():
                            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°
                            for ext in ['*.pth', '*.pt', '*.safetensors', '*.bin', '*.ckpt']:
                                found_files = list(model_dir.rglob(ext))
                                model_files.extend(found_files)
                
                # ì§ì ‘ íŒŒì¼ ê²€ìƒ‰ë„ ìˆ˜í–‰
                for expected_file in step_info.expected_files:
                    direct_files = list(ai_models_root.rglob(expected_file))
                    model_files.extend(direct_files)
            
            # ì¤‘ë³µ ì œê±°
            unique_files = list(set(model_files))
            
            # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ (ìƒìœ„ 5ê°œë§Œ)
            for model_file in unique_files[:5]:
                if model_file.stat().st_size > 10 * 1024 * 1024:  # 10MB ì´ìƒë§Œ
                    print(f"      ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¶„ì„: {model_file.name}")
                    checkpoint_analysis = self.checkpoint_analyzer.analyze_checkpoint(model_file)
                    analysis.checkpoint_analyses.append(checkpoint_analysis)
                    analysis.detected_model_files.append(model_file.name)
                    total_size += checkpoint_analysis.size_mb
            
            analysis.total_model_size_gb = total_size / 1024
            
            # ëª¨ë¸ ë¡œë”© ì„±ê³µë¥  ê³„ì‚°
            if analysis.checkpoint_analyses:
                successful_loads = sum(
                    1 for cp in analysis.checkpoint_analyses 
                    if cp.status in [CheckpointLoadingStatus.SUCCESS, CheckpointLoadingStatus.SAFETENSORS_SUCCESS]
                )
                analysis.model_loading_success_rate = successful_loads / len(analysis.checkpoint_analyses) * 100
            
            if analysis.detected_model_files:
                print(f"   ğŸ“Š AI ëª¨ë¸: {len(analysis.detected_model_files)}ê°œ ë°œê²¬ "
                      f"({analysis.total_model_size_gb:.1f}GB, ì„±ê³µë¥ : {analysis.model_loading_success_rate:.1f}%)")
            else:
                print(f"   âš ï¸ AI ëª¨ë¸ ì—†ìŒ")
            
        except Exception as e:
            print(f"   âŒ AI ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _determine_github_status_and_score(self, analysis: GitHubStepAnalysisResult):
        """GitHub Step ìƒíƒœ ê²°ì • ë° ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # íŒŒì¼ ìˆ˜ì • ë³´ë„ˆìŠ¤ (v6.0 ì¶”ê°€)
        if analysis.syntax_error_fixed:
            score += 10
        if analysis.threading_import_added:
            score += 10
        
        # Import ì„±ê³µ (15ì )
        if analysis.import_success:
            score += 15
        
        # í´ë˜ìŠ¤ êµ¬ì¡° (20ì )
        if analysis.class_found:
            score += 10
        if analysis.is_base_step_mixin:
            score += 5
        if analysis.has_process_method:
            score += 3
        if analysis.has_central_hub_support:
            score += 2
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (15ì )
        if analysis.instance_created:
            score += 15
        
        # ì´ˆê¸°í™” (20ì )
        if analysis.initialization_success:
            score += 20
        
        # ì˜ì¡´ì„± (15ì )
        if analysis.model_loader_injected:
            score += 8
        if analysis.central_hub_connected:
            score += 7
        
        # AI ëª¨ë¸ (15ì )
        if analysis.detected_model_files:
            score += 8
            if analysis.model_loading_success_rate > 50:
                score += 7
        
        analysis.health_score = min(100.0, score)
        
        # ìƒíƒœ ê²°ì •
        if not analysis.import_success:
            if analysis.status == GitHubStepStatus.NOT_FOUND:  # ì´ë¯¸ ì„¤ì •ëœ ê²½ìš° ìœ ì§€
                pass
            elif not analysis.syntax_error_fixed:
                analysis.status = GitHubStepStatus.SYNTAX_ERROR
            elif not analysis.threading_import_added:
                analysis.status = GitHubStepStatus.THREADING_MISSING
            else:
                analysis.status = GitHubStepStatus.IMPORT_FAILED
        elif not analysis.class_found:
            analysis.status = GitHubStepStatus.CLASS_NOT_FOUND
        elif not analysis.instance_created:
            analysis.status = GitHubStepStatus.INSTANCE_FAILED
        elif not analysis.initialization_success:
            analysis.status = GitHubStepStatus.INIT_FAILED
        elif not analysis.detected_model_files:
            analysis.status = GitHubStepStatus.AI_MODELS_FAILED
        elif not analysis.central_hub_connected and analysis.has_central_hub_support:
            analysis.status = GitHubStepStatus.CENTRAL_HUB_FAILED
        else:
            analysis.status = GitHubStepStatus.SUCCESS
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        if analysis.status != GitHubStepStatus.SUCCESS:
            if analysis.status == GitHubStepStatus.SYNTAX_ERROR:
                analysis.recommendations.append(f"syntax error ìˆ˜ì • í•„ìš”")
            elif analysis.status == GitHubStepStatus.THREADING_MISSING:
                analysis.recommendations.append(f"threading import ì¶”ê°€ í•„ìš”")
            elif not analysis.import_success:
                analysis.recommendations.append(f"ëª¨ë“ˆ ê²½ë¡œ í™•ì¸: {analysis.step_info.module_path}")
            if not analysis.initialization_success:
                analysis.recommendations.append(f"AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë° ê¶Œí•œ í™•ì¸")
            if not analysis.detected_model_files:
                analysis.recommendations.append(f"AI ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {', '.join(analysis.step_info.expected_files)}")
            if not analysis.central_hub_connected:
                analysis.recommendations.append(f"Central Hub ì˜ì¡´ì„± ì£¼ì… í™•ì¸")

# =============================================================================
# ğŸ”¥ 8. DetailedDataSpec v5.3 ë¶„ì„ê¸°
# =============================================================================

class GitHubDetailedDataSpecAnalyzer:
    """GitHub DetailedDataSpec v5.3 ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_detailed_data_spec_integration(self, step_name: str) -> Dict[str, Any]:
        """DetailedDataSpec í†µí•© ìƒíƒœ ë¶„ì„"""
        analysis_result = {
            'step_name': step_name,
            'detailed_data_spec_available': False,
            'api_input_mapping_ready': False,
            'api_output_mapping_ready': False,
            'preprocessing_steps_defined': False,
            'postprocessing_steps_defined': False,
            'step_interface_v5_3_compatible': False,
            'data_conversion_ready': False,
            'emergency_fallback_available': False,
            'integration_score': 0.0,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # Step Interface v5.3 í˜¸í™˜ì„± í™•ì¸
            try:
                from app.ai_pipeline.interface.step_interface import get_safe_detailed_data_spec
                spec = get_safe_detailed_data_spec(step_name)
                
                if spec:
                    analysis_result['detailed_data_spec_available'] = True
                    analysis_result['step_interface_v5_3_compatible'] = True
                    
                    # API ë§¤í•‘ í™•ì¸
                    if hasattr(spec, 'api_input_mapping') and spec.api_input_mapping:
                        analysis_result['api_input_mapping_ready'] = True
                    
                    if hasattr(spec, 'api_output_mapping') and spec.api_output_mapping:
                        analysis_result['api_output_mapping_ready'] = True
                    
                    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ë‹¨ê³„ í™•ì¸
                    if hasattr(spec, 'preprocessing_steps') and spec.preprocessing_steps:
                        analysis_result['preprocessing_steps_defined'] = True
                    
                    if hasattr(spec, 'postprocessing_steps') and spec.postprocessing_steps:
                        analysis_result['postprocessing_steps_defined'] = True
                    
                    # ë°ì´í„° ë³€í™˜ ì¤€ë¹„ë„ í™•ì¸
                    conversion_ready = all([
                        analysis_result['api_input_mapping_ready'],
                        analysis_result['api_output_mapping_ready']
                    ])
                    analysis_result['data_conversion_ready'] = conversion_ready
                
            except Exception as e:
                analysis_result['issues'].append(f"DetailedDataSpec ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Emergency Fallback í™•ì¸
            try:
                # BaseStepMixinì˜ emergency ìƒì„± ê¸°ëŠ¥ í™•ì¸
                from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
                
                # Mock ì¸ìŠ¤í„´ìŠ¤ë¡œ emergency fallback í…ŒìŠ¤íŠ¸
                class TestStep(BaseStepMixin):
                    def __init__(self):
                        self.step_name = step_name
                        super().__init__()
                    
                    def _run_ai_inference(self, input_data):
                        return {}
                
                test_instance = TestStep()
                if hasattr(test_instance, '_create_emergency_detailed_data_spec'):
                    analysis_result['emergency_fallback_available'] = True
                
            except Exception as e:
                analysis_result['issues'].append(f"Emergency fallback í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # í†µí•© ì ìˆ˜ ê³„ì‚°
            score_components = [
                analysis_result['detailed_data_spec_available'],
                analysis_result['api_input_mapping_ready'],
                analysis_result['api_output_mapping_ready'],
                analysis_result['preprocessing_steps_defined'],
                analysis_result['postprocessing_steps_defined'],
                analysis_result['step_interface_v5_3_compatible'],
                analysis_result['data_conversion_ready'],
                analysis_result['emergency_fallback_available']
            ]
            
            analysis_result['integration_score'] = sum(score_components) / len(score_components) * 100
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if not analysis_result['detailed_data_spec_available']:
                analysis_result['recommendations'].append(f"DetailedDataSpec ì •ì˜ í•„ìš”: {step_name}")
            
            if not analysis_result['data_conversion_ready']:
                analysis_result['recommendations'].append(f"API ë§¤í•‘ ì™„ì„± í•„ìš”")
            
            if analysis_result['integration_score'] < 70:
                analysis_result['recommendations'].append(f"DetailedDataSpec í†µí•© ê°œì„  í•„ìš”")
        
        except Exception as e:
            analysis_result['issues'].append(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis_result

# =============================================================================
# ğŸ”¥ 9. DI Container v7.0 ë¶„ì„ê¸°
# =============================================================================

class GitHubDIContainerAnalyzer:
    """GitHub DI Container v7.0 ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_di_container_integration(self) -> Dict[str, Any]:
        """DI Container v7.0 í†µí•© ìƒíƒœ ë¶„ì„"""
        analysis_result = {
            'di_container_available': False,
            'central_hub_connected': False,
            'global_container_accessible': False,
            'step_injection_working': False,
            'service_resolution_working': False,
            'memory_optimization_available': False,
            'stats_reporting_available': False,
            'circular_reference_protection': False,
            'container_version': 'unknown',
            'integration_score': 0.0,
            'services_registered': [],
            'issues': [],
            'recommendations': []
        }
        
        try:
            # DI Container ê°€ìš©ì„± í™•ì¸
            try:
                from app.core.di_container import get_global_container
                container = get_global_container()
                
                if container:
                    analysis_result['di_container_available'] = True
                    analysis_result['global_container_accessible'] = True
                    
                    # ë²„ì „ í™•ì¸
                    if hasattr(container, 'version'):
                        analysis_result['container_version'] = container.version
                    
                    # ì„œë¹„ìŠ¤ í•´ê²° í…ŒìŠ¤íŠ¸
                    test_services = ['model_loader', 'memory_manager', 'data_converter']
                    working_services = []
                    
                    for service in test_services:
                        try:
                            service_instance = container.get(service)
                            if service_instance:
                                working_services.append(service)
                        except Exception:
                            pass
                    
                    analysis_result['services_registered'] = working_services
                    analysis_result['service_resolution_working'] = len(working_services) > 0
                    
                    # Step ì£¼ì… ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
                    if hasattr(container, 'inject_to_step'):
                        analysis_result['step_injection_working'] = True
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ëŠ¥ í™•ì¸
                    if hasattr(container, 'optimize_memory'):
                        analysis_result['memory_optimization_available'] = True
                    
                    # í†µê³„ ë³´ê³  ê¸°ëŠ¥ í™•ì¸
                    if hasattr(container, 'get_stats'):
                        analysis_result['stats_reporting_available'] = True
                        try:
                            stats = container.get_stats()
                            if isinstance(stats, dict):
                                analysis_result['container_stats'] = stats
                        except Exception:
                            pass
                    
                    # ìˆœí™˜ ì°¸ì¡° ë³´í˜¸ í™•ì¸
                    if hasattr(container, '_resolving_stack'):
                        analysis_result['circular_reference_protection'] = True
                
            except Exception as e:
                analysis_result['issues'].append(f"DI Container ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Central Hub ì—°ê²° í™•ì¸
            try:
                from app.core.di_container import _get_central_hub_container
                central_hub = _get_central_hub_container()
                
                if central_hub:
                    analysis_result['central_hub_connected'] = True
                
            except Exception as e:
                analysis_result['issues'].append(f"Central Hub ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # í†µí•© ì ìˆ˜ ê³„ì‚°
            score_components = [
                analysis_result['di_container_available'],
                analysis_result['central_hub_connected'],
                analysis_result['global_container_accessible'],
                analysis_result['step_injection_working'],
                analysis_result['service_resolution_working'],
                analysis_result['memory_optimization_available'],
                analysis_result['stats_reporting_available'],
                analysis_result['circular_reference_protection']
            ]
            
            analysis_result['integration_score'] = sum(score_components) / len(score_components) * 100
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if not analysis_result['di_container_available']:
                analysis_result['recommendations'].append("DI Container v7.0 ì„¤ì¹˜ ë° ì„¤ì • í•„ìš”")
            
            if not analysis_result['central_hub_connected']:
                analysis_result['recommendations'].append("Central Hub ì—°ê²° ì„¤ì • í™•ì¸")
            
            if len(analysis_result['services_registered']) < 3:
                analysis_result['recommendations'].append("í•µì‹¬ ì„œë¹„ìŠ¤ ë“±ë¡ ì™„ì„± í•„ìš”")
        
        except Exception as e:
            analysis_result['issues'].append(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis_result

# =============================================================================
# ğŸ”¥ 10. StepFactory v11.2 ë¶„ì„ê¸°
# =============================================================================

class GitHubStepFactoryAnalyzer:
    """GitHub StepFactory v11.2 ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_step_factory_integration(self) -> Dict[str, Any]:
        """StepFactory v11.2 í†µí•© ìƒíƒœ ë¶„ì„"""
        analysis_result = {
            'step_factory_available': False,
            'step_factory_version': 'unknown',
            'central_hub_integration': False,
            'step_creation_working': False,
            'dependency_injection_working': False,
            'caching_available': False,
            'circular_reference_protection': False,
            'github_compatibility': False,
            'detailed_data_spec_integration': False,
            'integration_score': 0.0,
            'supported_step_types': [],
            'creation_stats': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            # StepFactory ê°€ìš©ì„± í™•ì¸
            try:
                from app.ai_pipeline.utils.step_factory import StepFactory
                factory = StepFactory()
                
                analysis_result['step_factory_available'] = True
                
                # ë²„ì „ í™•ì¸
                if hasattr(factory, 'version'):
                    analysis_result['step_factory_version'] = factory.version
                
                # Central Hub í†µí•© í™•ì¸
                if hasattr(factory, '_central_hub_container'):
                    analysis_result['central_hub_integration'] = True
                
                # Step ìƒì„± ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
                try:
                    # ê°„ë‹¨í•œ Step ìƒì„± í…ŒìŠ¤íŠ¸
                    test_result = factory.create_step('HumanParsingStep', device='cpu', strict_mode=False)
                    if hasattr(test_result, 'success') and test_result.success:
                        analysis_result['step_creation_working'] = True
                except Exception:
                    pass
                
                # ì˜ì¡´ì„± ì£¼ì… ê¸°ëŠ¥ í™•ì¸
                if hasattr(factory, '_inject_dependencies'):
                    analysis_result['dependency_injection_working'] = True
                
                # ìºì‹± ê¸°ëŠ¥ í™•ì¸
                if hasattr(factory, '_step_cache'):
                    analysis_result['caching_available'] = True
                
                # ìˆœí™˜ ì°¸ì¡° ë³´í˜¸ í™•ì¸
                if hasattr(factory, '_circular_detected'):
                    analysis_result['circular_reference_protection'] = True
                
                # GitHub í˜¸í™˜ì„± í™•ì¸
                if hasattr(factory, '_stats') and 'github_compatible_creations' in getattr(factory, '_stats', {}):
                    analysis_result['github_compatibility'] = True
                
                # DetailedDataSpec í†µí•© í™•ì¸
                if hasattr(factory, '_stats') and 'detailed_data_spec_successes' in getattr(factory, '_stats', {}):
                    analysis_result['detailed_data_spec_integration'] = True
                
                # ì§€ì› Step íƒ€ì… í™•ì¸
                if hasattr(factory, 'get_supported_step_types'):
                    try:
                        supported_types = factory.get_supported_step_types()
                        analysis_result['supported_step_types'] = supported_types
                    except Exception:
                        pass
                
                # í†µê³„ ì •ë³´ ìˆ˜ì§‘
                if hasattr(factory, 'get_statistics'):
                    try:
                        stats = factory.get_statistics()
                        if isinstance(stats, dict):
                            analysis_result['creation_stats'] = stats
                    except Exception:
                        pass
                
            except Exception as e:
                analysis_result['issues'].append(f"StepFactory ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # í†µí•© ì ìˆ˜ ê³„ì‚°
            score_components = [
                analysis_result['step_factory_available'],
                analysis_result['central_hub_integration'],
                analysis_result['step_creation_working'],
                analysis_result['dependency_injection_working'],
                analysis_result['caching_available'],
                analysis_result['circular_reference_protection'],
                analysis_result['github_compatibility'],
                analysis_result['detailed_data_spec_integration']
            ]
            
            analysis_result['integration_score'] = sum(score_components) / len(score_components) * 100
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if not analysis_result['step_factory_available']:
                analysis_result['recommendations'].append("StepFactory v11.2 ì„¤ì¹˜ í•„ìš”")
            
            if not analysis_result['step_creation_working']:
                analysis_result['recommendations'].append("Step ìƒì„± ê¸°ëŠ¥ í™•ì¸ ë° ìˆ˜ì • í•„ìš”")
            
            if not analysis_result['central_hub_integration']:
                analysis_result['recommendations'].append("Central Hub í†µí•© ì„¤ì • í™•ì¸")
        
        except Exception as e:
            analysis_result['issues'].append(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis_result

# =============================================================================
# ğŸ”¥ 11. ë©”ì¸ ë””ë²„ê¹… ì‹œìŠ¤í…œ
# =============================================================================

class UltimateGitHubAIDebuggerV6:
    """Ultimate GitHub AI ë””ë²„ê±° v6.0 - ì™„ì „í•œ ì¢…í•© ë””ë²„ê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.start_time = time.time()
        self.system_env = None
        self.step_analyses = {}
        self.overall_results = {}
        
    def run_ultimate_github_debugging(self) -> Dict[str, Any]:
        """GitHub í”„ë¡œì íŠ¸ ìµœê³ ê¸‰ ë””ë²„ê¹… ì‹¤í–‰"""
        
        print("ğŸ”¥" * 50)
        print("ğŸ”¥ Ultimate AI Model Loading Debugger v6.0 ì‹œì‘")
        print("ğŸ”¥ GitHub í”„ë¡œì íŠ¸: MyCloset AI Pipeline ì™„ì „ ë¶„ì„")
        print("ğŸ”¥ Target: ëª¨ë“  ì˜¤ë¥˜ í•´ê²° + 8ë‹¨ê³„ AI Step + 229GB AI ëª¨ë¸ ì™„ì „ ê²€ì¦")
        print("ğŸ”¥" * 50)
        
        debug_result = {
            'timestamp': time.time(),
            'debug_version': '6.0',
            'github_project': 'MyCloset AI Pipeline',
            'system_environment': {},
            'step_analyses': {},
            'overall_summary': {},
            'critical_issues': [],
            'actionable_recommendations': [],
            'performance_metrics': {},
            'github_specific_insights': {},
            'advanced_analyses': {}
        }
        
        try:
            # 1. GitHub ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„ (ì˜¤ë¥˜ ìˆ˜ì • í¬í•¨)
            print("\nğŸ“Š 1. GitHub í”„ë¡œì íŠ¸ ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„ ë° ì˜¤ë¥˜ ìˆ˜ì •")
            system_analyzer = GitHubSystemAnalyzer()
            self.system_env = system_analyzer.analyze_github_environment()
            debug_result['system_environment'] = self._serialize_system_environment(self.system_env)
            self._print_system_environment_summary()
            
            # 2. GitHub 8ë‹¨ê³„ AI Step ì™„ì „ ë¶„ì„
            print("\nğŸš€ 2. GitHub 8ë‹¨ê³„ AI Step ì™„ì „ ë¶„ì„ (ìˆ˜ì • í›„)")
            step_analyzer = GitHubStepAnalyzer(self.system_env)
            
            for step_config in GITHUB_STEP_CONFIGS:
                try:
                    step_analysis = step_analyzer.analyze_github_step(step_config)
                    self.step_analyses[step_config.step_name] = step_analysis
                    debug_result['step_analyses'][step_config.step_name] = self._serialize_step_analysis(step_analysis)
                    
                except Exception as e:
                    print(f"âŒ {step_config.step_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    debug_result['step_analyses'][step_config.step_name] = {
                        'error': str(e),
                        'status': 'analysis_failed'
                    }
            
            # 3. GitHub ê³ ê¸‰ í†µí•© ë¶„ì„
            print("\nğŸ”— 3. GitHub ê³ ê¸‰ í†µí•© ë¶„ì„ ë° í˜¸í™˜ì„± ê²€ì¦")
            debug_result['github_specific_insights'] = self._analyze_github_integrations()
            
            # 4. DetailedDataSpec v5.3 ë¶„ì„
            print("\nğŸ“Š 4. DetailedDataSpec v5.3 í†µí•© ìƒíƒœ ë¶„ì„")
            debug_result['advanced_analyses']['detailed_data_spec'] = self._analyze_detailed_data_spec_integration()
            
            # 5. DI Container v7.0 ë¶„ì„
            print("\nğŸ”— 5. DI Container v7.0 í†µí•© ìƒíƒœ ë¶„ì„")
            debug_result['advanced_analyses']['di_container'] = self._analyze_di_container_integration()
            
            # 6. StepFactory v11.2 ë¶„ì„
            print("\nğŸ­ 6. StepFactory v11.2 í†µí•© ìƒíƒœ ë¶„ì„")
            debug_result['advanced_analyses']['step_factory'] = self._analyze_step_factory_integration()
            
            # 7. ì „ì²´ ìš”ì•½ ìƒì„±
            print("\nğŸ“Š 7. GitHub í”„ë¡œì íŠ¸ ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
            debug_result['overall_summary'] = self._generate_github_overall_summary()
            debug_result['critical_issues'] = self._identify_github_critical_issues()
            debug_result['actionable_recommendations'] = self._generate_github_actionable_recommendations()
            debug_result['performance_metrics'] = self._calculate_github_performance_metrics()
            
            # 8. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
            self._print_github_debug_results(debug_result)
            self._save_github_debug_results(debug_result)
            
        except Exception as e:
            print(f"\nâŒ GitHub ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            debug_result['fatal_error'] = str(e)
        
        finally:
            total_time = time.time() - self.start_time
            print(f"\nğŸ‰ Ultimate GitHub AI Model Debugging v6.0 ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
            debug_result['total_debug_time'] = total_time
        
        return debug_result
    
    def _serialize_system_environment(self, env: GitHubSystemEnvironment) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í™˜ê²½ ì§ë ¬í™”"""
        return {
            'hardware': {
                'is_m3_max': env.is_m3_max,
                'total_memory_gb': env.total_memory_gb,
                'available_memory_gb': env.available_memory_gb,
                'cpu_cores': env.cpu_cores
            },
            'software': {
                'python_version': env.python_version,
                'conda_env': env.conda_env,
                'is_target_conda_env': env.is_target_conda_env
            },
            'pytorch': {
                'torch_available': env.torch_available,
                'torch_version': env.torch_version,
                'cuda_available': env.cuda_available,
                'mps_available': env.mps_available,
                'recommended_device': env.recommended_device
            },
            'project_structure': {
                'project_root_exists': env.project_root_exists,
                'backend_root_exists': env.backend_root_exists,
                'ai_models_root_exists': env.ai_models_root_exists,
                'ai_models_size_gb': env.ai_models_size_gb,
                'step_modules_found': env.step_modules_found
            },
            'fixes_applied': {
                'step_files_fixed': env.step_files_fixed,
                'threading_imports_added': env.threading_imports_added,
                'syntax_errors_fixed': env.syntax_errors_fixed
            },
            'dependencies': {
                'core_dependencies': env.core_dependencies,
                'github_integrations': env.github_integrations
            }
        }
    
    def _serialize_step_analysis(self, analysis: GitHubStepAnalysisResult) -> Dict[str, Any]:
        """Step ë¶„ì„ ê²°ê³¼ ì§ë ¬í™”"""
        return {
            'step_info': {
                'step_id': analysis.step_info.step_id,
                'step_name': analysis.step_info.step_name,
                'step_class': analysis.step_info.step_class,
                'module_path': analysis.step_info.module_path,
                'expected_size_gb': analysis.step_info.expected_size_gb,
                'priority': analysis.step_info.priority
            },
            'file_fixes': {
                'syntax_error_fixed': analysis.syntax_error_fixed,
                'threading_import_added': analysis.threading_import_added,
                'basestepmixin_compatible': analysis.basestepmixin_compatible
            },
            'import_analysis': {
                'success': analysis.import_success,
                'time': analysis.import_time,
                'errors': analysis.import_errors
            },
            'class_analysis': {
                'found': analysis.class_found,
                'is_base_step_mixin': analysis.is_base_step_mixin,
                'has_process_method': analysis.has_process_method,
                'has_initialize_method': analysis.has_initialize_method,
                'has_central_hub_support': analysis.has_central_hub_support
            },
            'instance_analysis': {
                'created': analysis.instance_created,
                'constructor_params': analysis.constructor_params,
                'errors': analysis.instance_errors
            },
            'initialization': {
                'success': analysis.initialization_success,
                'time': analysis.initialization_time,
                'errors': analysis.initialization_errors
            },
            'dependencies': {
                'model_loader_injected': analysis.model_loader_injected,
                'central_hub_connected': analysis.central_hub_connected,
                'validation_result': analysis.dependency_validation_result
            },
            'ai_models': {
                'detected_files': analysis.detected_model_files,
                'total_size_gb': analysis.total_model_size_gb,
                'checkpoint_count': len(analysis.checkpoint_analyses),
                'loading_success_rate': analysis.model_loading_success_rate
            },
            'performance': {
                'memory_footprint_mb': analysis.memory_footprint_mb,
                'health_score': analysis.health_score
            },
            'status': analysis.status.value,
            'recommendations': analysis.recommendations
        }
    
    def _print_system_environment_summary(self):
        """ì‹œìŠ¤í…œ í™˜ê²½ ìš”ì•½ ì¶œë ¥"""
        env = self.system_env
        
        print(f"   ğŸ’» í•˜ë“œì›¨ì–´:")
        print(f"      CPU: {env.cpu_cores}ì½”ì–´")
        print(f"      ë©”ëª¨ë¦¬: {env.available_memory_gb:.1f}GB ì‚¬ìš©ê°€ëŠ¥ / {env.total_memory_gb:.1f}GB ì´ëŸ‰")
        print(f"      M3 Max: {'âœ…' if env.is_m3_max else 'âŒ'}")
        
        print(f"   ğŸ”¥ AI í™˜ê²½:")
        print(f"      PyTorch: {'âœ…' if env.torch_available else 'âŒ'} {env.torch_version}")
        print(f"      ì¶”ì²œ ë””ë°”ì´ìŠ¤: {env.recommended_device}")
        print(f"      MPS: {'âœ…' if env.mps_available else 'âŒ'}")
        print(f"      CUDA: {'âœ…' if env.cuda_available else 'âŒ'}")
        
        print(f"   ğŸ“ GitHub í”„ë¡œì íŠ¸:")
        print(f"      í”„ë¡œì íŠ¸ ë£¨íŠ¸: {'âœ…' if env.project_root_exists else 'âŒ'}")
        print(f"      ë°±ì—”ë“œ ë£¨íŠ¸: {'âœ…' if env.backend_root_exists else 'âŒ'}")
        print(f"      AI ëª¨ë¸ ë£¨íŠ¸: {'âœ…' if env.ai_models_root_exists else 'âŒ'}")
        print(f"      AI ëª¨ë¸ í¬ê¸°: {env.ai_models_size_gb:.1f}GB")
        print(f"      Step ëª¨ë“ˆ: {len(env.step_modules_found)}ê°œ")
        
        print(f"   ğŸ”§ íŒŒì¼ ìˆ˜ì • ê²°ê³¼:")
        print(f"      Step íŒŒì¼ ìˆ˜ì •: {len(env.step_files_fixed)}ê°œ")
        print(f"      threading import ì¶”ê°€: {len(env.threading_imports_added)}ê°œ")
        print(f"      syntax error ìˆ˜ì •: {env.syntax_errors_fixed}ê°œ")
        
        print(f"   ğŸ í™˜ê²½:")
        print(f"      Conda í™˜ê²½: {env.conda_env}")
        print(f"      íƒ€ê²Ÿ í™˜ê²½: {'âœ…' if env.is_target_conda_env else 'âŒ'} (mycloset-ai-clean)")
        
        # ì˜ì¡´ì„± ìƒíƒœ
        core_success = sum(env.core_dependencies.values())
        core_total = len(env.core_dependencies)
        github_success = sum(env.github_integrations.values())
        github_total = len(env.github_integrations)
        
        print(f"   ğŸ“¦ ì˜ì¡´ì„±:")
        print(f"      í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬: {core_success}/{core_total}")
        print(f"      GitHub í†µí•©: {github_success}/{github_total}")
    
    def _analyze_github_integrations(self) -> Dict[str, Any]:
        """GitHub í†µí•© ë¶„ì„"""
        integrations = {
            'auto_model_detector_status': 'unknown',
            'step_factory_availability': False,
            'real_ai_implementation_manager': False,
            'central_hub_readiness': False,
            'model_loader_v5_compatibility': False
        }
        
        try:
            # AutoModelDetector í…ŒìŠ¤íŠ¸
            try:
                from app.ai_pipeline.utils.auto_model_detector import AutoModelDetector
                detector = AutoModelDetector()
                
                # ì‹¤ì œ íŒŒì¼ ì°¾ê¸° í…ŒìŠ¤íŠ¸
                test_result = detector.find_actual_file("human_parsing_schp", ai_models_root)
                integrations['auto_model_detector_status'] = 'working' if test_result else 'no_models'
                
            except Exception as e:
                integrations['auto_model_detector_status'] = f'error: {str(e)[:50]}'
            
            # StepFactory í…ŒìŠ¤íŠ¸
            try:
                from app.ai_pipeline.utils.step_factory import StepFactory
                integrations['step_factory_availability'] = True
            except ImportError:
                pass
            
            # RealAIStepImplementationManager í…ŒìŠ¤íŠ¸
            try:
                from app.services.step_implementations import RealAIStepImplementationManager
                integrations['real_ai_implementation_manager'] = True
            except ImportError:
                pass
            
            # Central Hub ì¤€ë¹„ë„ í…ŒìŠ¤íŠ¸
            try:
                from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
                # BaseStepMixinì— Central Hub ê´€ë ¨ ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
                dummy_class = type('DummyStep', (BaseStepMixin,), {'step_name': 'test'})
                dummy_instance = dummy_class()
                integrations['central_hub_readiness'] = hasattr(dummy_instance, 'central_hub_container')
            except Exception:
                pass
            
            # ModelLoader v5.1 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
            try:
                from app.ai_pipeline.utils.model_loader import ModelLoader
                integrations['model_loader_v5_compatibility'] = True
            except ImportError:
                pass
        
        except Exception as e:
            integrations['analysis_error'] = str(e)
        
        return integrations
    
    def _analyze_detailed_data_spec_integration(self) -> Dict[str, Any]:
        """DetailedDataSpec v5.3 í†µí•© ë¶„ì„"""
        print("   ğŸ“Š DetailedDataSpec v5.3 í†µí•© ìƒíƒœ ë¶„ì„...")
        
        analyzer = GitHubDetailedDataSpecAnalyzer()
        detailed_results = {}
        
        for step_config in GITHUB_STEP_CONFIGS:
            step_result = analyzer.analyze_detailed_data_spec_integration(step_config.step_name)
            detailed_results[step_config.step_name] = step_result
            
            score = step_result['integration_score']
            status = "âœ…" if score >= 80 else "âš ï¸" if score >= 60 else "âŒ"
            print(f"      {status} {step_config.step_name}: {score:.1f}% í†µí•©")
        
        # ì „ì²´ í†µí•© ì ìˆ˜ ê³„ì‚°
        total_scores = [result['integration_score'] for result in detailed_results.values()]
        overall_integration_score = sum(total_scores) / len(total_scores) if total_scores else 0
        
        return {
            'overall_integration_score': overall_integration_score,
            'step_results': detailed_results,
            'api_mapping_ready_count': sum(1 for r in detailed_results.values() if r['api_input_mapping_ready']),
            'data_conversion_ready_count': sum(1 for r in detailed_results.values() if r['data_conversion_ready']),
            'emergency_fallback_count': sum(1 for r in detailed_results.values() if r['emergency_fallback_available'])
        }
    
    def _analyze_di_container_integration(self) -> Dict[str, Any]:
        """DI Container v7.0 í†µí•© ë¶„ì„"""
        print("   ğŸ”— DI Container v7.0 í†µí•© ìƒíƒœ ë¶„ì„...")
        
        analyzer = GitHubDIContainerAnalyzer()
        result = analyzer.analyze_di_container_integration()
        
        score = result['integration_score']
        status = "âœ…" if score >= 80 else "âš ï¸" if score >= 60 else "âŒ"
        print(f"      {status} DI Container v7.0: {score:.1f}% í†µí•©")
        print(f"      ì„œë¹„ìŠ¤ ë“±ë¡: {len(result['services_registered'])}ê°œ")
        
        return result
    
    def _analyze_step_factory_integration(self) -> Dict[str, Any]:
        """StepFactory v11.2 í†µí•© ë¶„ì„"""
        print("   ğŸ­ StepFactory v11.2 í†µí•© ìƒíƒœ ë¶„ì„...")
        
        analyzer = GitHubStepFactoryAnalyzer()
        result = analyzer.analyze_step_factory_integration()
        
        score = result['integration_score']
        status = "âœ…" if score >= 80 else "âš ï¸" if score >= 60 else "âŒ"
        print(f"      {status} StepFactory v11.2: {score:.1f}% í†µí•©")
        print(f"      ì§€ì› Step íƒ€ì…: {len(result['supported_step_types'])}ê°œ")
        
        return result
    
    def _generate_github_overall_summary(self) -> Dict[str, Any]:
        """GitHub ì „ì²´ ìš”ì•½ ìƒì„±"""
        total_steps = len(self.step_analyses)
        successful_steps = sum(1 for analysis in self.step_analyses.values() 
                              if analysis.status == GitHubStepStatus.SUCCESS)
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ì„
        critical_steps = [analysis for analysis in self.step_analyses.values() 
                         if analysis.step_info.priority == "critical"]
        critical_success = sum(1 for analysis in critical_steps 
                              if analysis.status == GitHubStepStatus.SUCCESS)
        
        # íŒŒì¼ ìˆ˜ì • í†µê³„
        fixed_files = len(self.system_env.step_files_fixed)
        threading_added = len(self.system_env.threading_imports_added)
        syntax_fixed = self.system_env.syntax_errors_fixed
        
        # ëª¨ë¸ í†µê³„
        total_models = sum(len(analysis.detected_model_files) for analysis in self.step_analyses.values())
        total_model_size = sum(analysis.total_model_size_gb for analysis in self.step_analyses.values())
        
        # ì²´í¬í¬ì¸íŠ¸ í†µê³„
        total_checkpoints = sum(len(analysis.checkpoint_analyses) for analysis in self.step_analyses.values())
        successful_checkpoints = sum(
            sum(1 for cp in analysis.checkpoint_analyses 
                if cp.status in [CheckpointLoadingStatus.SUCCESS, CheckpointLoadingStatus.SAFETENSORS_SUCCESS])
            for analysis in self.step_analyses.values()
        )
        
        # í‰ê·  ê±´ê°•ë„
        health_scores = [analysis.health_score for analysis in self.step_analyses.values() if analysis.health_score > 0]
        average_health = sum(health_scores) / len(health_scores) if health_scores else 0
        
        # VirtualFittingStep íŠ¹ë³„ ë¶„ì„ (ê°€ì¥ ì¤‘ìš”í•œ Step)
        virtual_fitting_analysis = self.step_analyses.get('VirtualFittingStep')
        virtual_fitting_ready = (virtual_fitting_analysis and 
                                virtual_fitting_analysis.status == GitHubStepStatus.SUCCESS)
        
        return {
            'steps': {
                'total': total_steps,
                'successful': successful_steps,
                'success_rate': (successful_steps / total_steps * 100) if total_steps > 0 else 0,
                'critical_steps_success': critical_success,
                'critical_steps_total': len(critical_steps),
                'virtual_fitting_ready': virtual_fitting_ready
            },
            'fixes': {
                'files_fixed': fixed_files,
                'threading_imports_added': threading_added,
                'syntax_errors_fixed': syntax_fixed,
                'total_fixes_applied': fixed_files + threading_added + syntax_fixed
            },
            'models': {
                'total_detected': total_models,
                'total_size_gb': total_model_size,
                'expected_size_gb': sum(step.step_info.expected_size_gb for step in self.step_analyses.values()),
                'size_coverage': (total_model_size / sum(step.step_info.expected_size_gb for step in self.step_analyses.values()) * 100) if sum(step.step_info.expected_size_gb for step in self.step_analyses.values()) > 0 else 0
            },
            'checkpoints': {
                'total': total_checkpoints,
                'successful': successful_checkpoints,
                'success_rate': (successful_checkpoints / total_checkpoints * 100) if total_checkpoints > 0 else 0
            },
            'health': {
                'average_score': average_health,
                'system_ready': (self.system_env.torch_available and 
                               self.system_env.ai_models_root_exists and
                               self.system_env.available_memory_gb >= 8),
                'github_integration_ready': sum(self.system_env.github_integrations.values()) >= 3,
                'ai_pipeline_ready': successful_steps >= 6  # ìµœì†Œ 6ê°œ Step ì„±ê³µ
            },
            'environment': {
                'optimal_setup': (self.system_env.is_m3_max and 
                                self.system_env.is_target_conda_env and
                                self.system_env.mps_available),
                'memory_sufficient': self.system_env.available_memory_gb >= 16,
                'device_acceleration': self.system_env.recommended_device != 'cpu'
            }
        }
    
    def _identify_github_critical_issues(self) -> List[str]:
        """GitHub ì¤‘ìš” ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        # ì‹œìŠ¤í…œ ìˆ˜ì¤€ ë¬¸ì œ
        if not self.system_env.torch_available:
            issues.append("ğŸ”¥ CRITICAL: PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - AI ëª¨ë¸ ì‹¤í–‰ ë¶ˆê°€")
        
        if not self.system_env.ai_models_root_exists:
            issues.append("ğŸ”¥ CRITICAL: AI ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ - ai_models í´ë” ìƒì„± í•„ìš”")
        
        if self.system_env.available_memory_gb < 8:
            issues.append("ğŸ”¥ CRITICAL: ë©”ëª¨ë¦¬ ë¶€ì¡± - AI ëª¨ë¸ ë¡œë”©ì— ë¬¸ì œ ë°œìƒ ê°€ëŠ¥")
        
        if not self.system_env.is_target_conda_env:
            issues.append("âš ï¸ WARNING: conda í™˜ê²½ì´ mycloset-ai-cleanì´ ì•„ë‹˜ - ì˜ì¡´ì„± ë¬¸ì œ ê°€ëŠ¥")
        
        # íŒŒì¼ ìˆ˜ì • ê´€ë ¨
        if self.system_env.syntax_errors_fixed < 8:
            unfixed_count = 8 - self.system_env.syntax_errors_fixed
            issues.append(f"ğŸ”§ SYNTAX: {unfixed_count}ê°œ Step íŒŒì¼ì´ ì•„ì§ ìˆ˜ì •ë˜ì§€ ì•ŠìŒ")
        
        if len(self.system_env.threading_imports_added) < 8:
            missing_count = 8 - len(self.system_env.threading_imports_added)
            issues.append(f"ğŸ§µ THREADING: {missing_count}ê°œ Step íŒŒì¼ì— threading import ëˆ„ë½")
        
        # Step ìˆ˜ì¤€ ë¬¸ì œ (ìš°ì„ ìˆœìœ„ë³„)
        critical_failed_steps = []
        high_failed_steps = []
        syntax_error_steps = []
        
        for name, analysis in self.step_analyses.items():
            if analysis.step_info.priority == "critical" and analysis.status != GitHubStepStatus.SUCCESS:
                critical_failed_steps.append(name)
            elif analysis.step_info.priority == "high" and analysis.status != GitHubStepStatus.SUCCESS:
                high_failed_steps.append(name)
            
            if analysis.status == GitHubStepStatus.SYNTAX_ERROR:
                syntax_error_steps.append(name)
        
        if critical_failed_steps:
            issues.append(f"ğŸ”¥ CRITICAL STEPS ì‹¤íŒ¨: {', '.join(critical_failed_steps)}")
        
        if high_failed_steps:
            issues.append(f"âš ï¸ HIGH PRIORITY STEPS ì‹¤íŒ¨: {', '.join(high_failed_steps)}")
        
        if syntax_error_steps:
            issues.append(f"ğŸ”§ SYNTAX ERROR STEPS: {', '.join(syntax_error_steps)}")
        
        # VirtualFittingStep íŠ¹ë³„ ì²´í¬ (ê°€ì¥ ì¤‘ìš”)
        virtual_fitting = self.step_analyses.get('VirtualFittingStep')
        if virtual_fitting and virtual_fitting.status != GitHubStepStatus.SUCCESS:
            issues.append("ğŸ”¥ CRITICAL: VirtualFittingStep ì‹¤íŒ¨ - í•µì‹¬ ê°€ìƒ í”¼íŒ… ê¸°ëŠ¥ ë¶ˆê°€")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¬¸ì œ
        corrupted_checkpoints = []
        missing_models = []
        
        for analysis in self.step_analyses.values():
            for cp in analysis.checkpoint_analyses:
                if cp.status == CheckpointLoadingStatus.CORRUPTED:
                    corrupted_checkpoints.append(cp.file_path.name)
            
            if not analysis.detected_model_files and analysis.step_info.expected_files:
                missing_models.append(analysis.step_info.step_name)
        
        if corrupted_checkpoints:
            issues.append(f"ğŸ’¾ ì†ìƒëœ ì²´í¬í¬ì¸íŠ¸: {', '.join(corrupted_checkpoints[:3])}")
        
        if missing_models:
            issues.append(f"ğŸ“ AI ëª¨ë¸ ëˆ„ë½: {', '.join(missing_models)}")
        
        # GitHub í†µí•© ë¬¸ì œ
        github_integrations = self.system_env.github_integrations
        failed_integrations = [k for k, v in github_integrations.items() if not v]
        
        if len(failed_integrations) > 2:
            issues.append(f"ğŸ”— GitHub í†µí•© ë¬¸ì œ: {', '.join(failed_integrations[:3])}")
        
        return issues
    
    def _generate_github_actionable_recommendations(self) -> List[str]:
        """GitHub ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‹œìŠ¤í…œ ê°œì„ 
        if not self.system_env.torch_available:
            if self.system_env.is_m3_max:
                recommendations.append("ğŸ“¦ M3 Max PyTorch ì„¤ì¹˜: pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu")
            else:
                recommendations.append("ğŸ“¦ PyTorch ì„¤ì¹˜: conda install pytorch torchvision -c pytorch")
        
        if not self.system_env.is_target_conda_env:
            recommendations.append("ğŸ Conda í™˜ê²½ í™œì„±í™”: conda activate mycloset-ai-clean")
        
        if not self.system_env.ai_models_root_exists:
            recommendations.append(f"ğŸ“ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±: mkdir -p {ai_models_root}")
        
        # íŒŒì¼ ìˆ˜ì • ê´€ë ¨ ì¶”ì²œì‚¬í•­
        if self.system_env.syntax_errors_fixed > 0:
            recommendations.append(f"ğŸ”§ ìˆ˜ì •ëœ {self.system_env.syntax_errors_fixed}ê°œ Step íŒŒì¼ í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰")
        
        if len(self.system_env.threading_imports_added) > 0:
            recommendations.append(f"ğŸ§µ threading importê°€ ì¶”ê°€ëœ {len(self.system_env.threading_imports_added)}ê°œ íŒŒì¼ ì¬ë¡œë“œ")
        
        # Stepë³„ ê°œì„ ì‚¬í•­
        for name, analysis in self.step_analyses.items():
            if analysis.status == GitHubStepStatus.SYNTAX_ERROR:
                recommendations.append(f"ğŸ”§ {name} syntax error ìˆ˜ë™ í™•ì¸ ë° ìˆ˜ì •")
            elif analysis.status == GitHubStepStatus.THREADING_MISSING:
                recommendations.append(f"ğŸ§µ {name} threading import ìˆ˜ë™ ì¶”ê°€")
            elif analysis.status == GitHubStepStatus.IMPORT_FAILED:
                recommendations.append(f"ğŸ”§ {name} ëª¨ë“ˆ ê²½ë¡œ ì¬í™•ì¸: {analysis.step_info.module_path}")
            elif analysis.status == GitHubStepStatus.AI_MODELS_FAILED:
                expected_files = ', '.join(analysis.step_info.expected_files[:2])
                recommendations.append(f"ğŸ“¥ {name} AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {expected_files}")
            elif analysis.status == GitHubStepStatus.CENTRAL_HUB_FAILED:
                recommendations.append(f"ğŸ”— {name} Central Hub ì˜ì¡´ì„± ì£¼ì… í™•ì¸")
        
        # ì„±ëŠ¥ ìµœì í™”
        total_model_size = sum(analysis.total_model_size_gb for analysis in self.step_analyses.values())
        if total_model_size > 100:  # 100GB ì´ìƒ
            recommendations.append(f"ğŸ’¾ ëŒ€ìš©ëŸ‰ ëª¨ë¸ ìµœì í™”: {total_model_size:.1f}GB - ëª¨ë¸ ë¶„í•  ë¡œë”© ê³ ë ¤")
        
        if self.system_env.is_m3_max and not self.system_env.mps_available:
            recommendations.append("âš¡ M3 Max MPS í™œì„±í™”: PyTorch MPS ë°±ì—”ë“œ ì„¤ì • í™•ì¸")
        
        # GitHub íŠ¹í™” ì¶”ì²œì‚¬í•­
        if not self.system_env.github_integrations.get('auto_model_detector', False):
            recommendations.append("ğŸ” AutoModelDetector ì„¤ì •: ëª¨ë¸ ìë™ ê°ì§€ ê¸°ëŠ¥ í™œì„±í™”")
        
        if not self.system_env.github_integrations.get('step_factory', False):
            recommendations.append("ğŸ­ StepFactory í†µí•©: ë™ì  Step ìƒì„± ê¸°ëŠ¥ í™œì„±í™”")
        
        # VirtualFittingStep íŠ¹ë³„ ì¶”ì²œ (ê°€ì¥ ì¤‘ìš”)
        virtual_fitting = self.step_analyses.get('VirtualFittingStep')
        if virtual_fitting and virtual_fitting.status != GitHubStepStatus.SUCCESS:
            recommendations.append("ğŸ¯ VirtualFittingStep ìš°ì„  ìˆ˜ì •: OOTDiffusion ëª¨ë¸ ë° ì˜ì¡´ì„± í™•ì¸")
        
        # ë°±ì—… íŒŒì¼ ì •ë¦¬
        if len(self.system_env.step_files_fixed) > 0:
            recommendations.append("ğŸ—‚ï¸ ë°±ì—… íŒŒì¼ ì •ë¦¬: *.py.backup íŒŒì¼ë“¤ í™•ì¸ í›„ ì‚­ì œ")
        
        return recommendations
    
    def _calculate_github_performance_metrics(self) -> Dict[str, Any]:
        """GitHub ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        total_analysis_time = time.time() - self.start_time
        
        import_times = [analysis.import_time for analysis in self.step_analyses.values() 
                       if analysis.import_time > 0]
        init_times = [analysis.initialization_time for analysis in self.step_analyses.values() 
                     if analysis.initialization_time > 0]
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        current_memory = psutil.Process().memory_info().rss / (1024**3)
        
        # ëª¨ë¸ ë¡œë”© ì„±ê³µë¥ 
        total_checkpoints = sum(len(analysis.checkpoint_analyses) for analysis in self.step_analyses.values())
        successful_checkpoints = sum(
            sum(1 for cp in analysis.checkpoint_analyses 
                if cp.status in [CheckpointLoadingStatus.SUCCESS, CheckpointLoadingStatus.SAFETENSORS_SUCCESS])
            for analysis in self.step_analyses.values()
        )
        
        # ìˆ˜ì • íš¨ìœ¨ì„±
        fix_efficiency = 0.0
        if len(self.system_env.step_files_fixed) > 0:
            successful_after_fix = sum(1 for analysis in self.step_analyses.values() 
                                     if analysis.syntax_error_fixed and analysis.status == GitHubStepStatus.SUCCESS)
            fix_efficiency = (successful_after_fix / len(self.system_env.step_files_fixed)) * 100
        
        return {
            'analysis_time': {
                'total_seconds': total_analysis_time,
                'average_import_time': sum(import_times) / len(import_times) if import_times else 0,
                'average_init_time': sum(init_times) / len(init_times) if init_times else 0,
                'efficiency_rating': 'excellent' if total_analysis_time < 300 else 'good' if total_analysis_time < 600 else 'slow'
            },
            'memory_usage': {
                'current_gb': current_memory,
                'efficiency': 'good' if current_memory < 8 else 'moderate' if current_memory < 16 else 'high'
            },
            'model_loading': {
                'checkpoint_success_rate': (successful_checkpoints / total_checkpoints * 100) if total_checkpoints > 0 else 0,
                'total_checkpoints_tested': total_checkpoints,
                'loading_efficiency': 'excellent' if successful_checkpoints / total_checkpoints > 0.8 else 'good' if successful_checkpoints / total_checkpoints > 0.6 else 'needs_improvement'
            },
            'file_fixes': {
                'files_fixed': len(self.system_env.step_files_fixed),
                'threading_imports_added': len(self.system_env.threading_imports_added),
                'syntax_errors_fixed': self.system_env.syntax_errors_fixed,
                'fix_efficiency_percent': fix_efficiency,
                'fix_success_rating': 'excellent' if fix_efficiency > 80 else 'good' if fix_efficiency > 60 else 'needs_improvement'
            },
            'github_integration': {
                'integration_score': sum(self.system_env.github_integrations.values()) / len(self.system_env.github_integrations) * 100,
                'readiness_level': 'production' if sum(self.system_env.github_integrations.values()) >= 4 else 'development'
            }
        }
    
    def _print_github_debug_results(self, debug_result: Dict[str, Any]):
        """GitHub ë””ë²„ê¹… ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 100)
        print("ğŸ“Š Ultimate GitHub AI Model Loading Debug Results v6.0")
        print("=" * 100)
        
        # ì „ì²´ ìš”ì•½
        summary = debug_result['overall_summary']
        print(f"\nğŸ¯ GitHub í”„ë¡œì íŠ¸ ì „ì²´ ìš”ì•½:")
        print(f"   Step ì„±ê³µë¥ : {summary['steps']['success_rate']:.1f}% ({summary['steps']['successful']}/{summary['steps']['total']})")
        print(f"   Critical Step ì„±ê³µë¥ : {summary['steps']['critical_steps_success']}/{summary['steps']['critical_steps_total']}")
        print(f"   VirtualFittingStep: {'âœ…' if summary['steps']['virtual_fitting_ready'] else 'âŒ'}")
        print(f"   ì²´í¬í¬ì¸íŠ¸ ì„±ê³µë¥ : {summary['checkpoints']['success_rate']:.1f}% ({summary['checkpoints']['successful']}/{summary['checkpoints']['total']})")
        print(f"   AI ëª¨ë¸ í¬ê¸°: {summary['models']['total_size_gb']:.1f}GB / {summary['models']['expected_size_gb']:.1f}GB (ì»¤ë²„ë¦¬ì§€: {summary['models']['size_coverage']:.1f}%)")
        print(f"   í‰ê·  ê±´ê°•ë„: {summary['health']['average_score']:.1f}/100")
        print(f"   AI íŒŒì´í”„ë¼ì¸ ì¤€ë¹„: {'âœ…' if summary['health']['ai_pipeline_ready'] else 'âŒ'}")
        print(f"   ìµœì  í™˜ê²½ ì„¤ì •: {'âœ…' if summary['environment']['optimal_setup'] else 'âŒ'}")
        
        # íŒŒì¼ ìˆ˜ì • ê²°ê³¼
        print(f"\nğŸ”§ íŒŒì¼ ìˆ˜ì • ê²°ê³¼:")
        print(f"   Step íŒŒì¼ ìˆ˜ì •: {summary['fixes']['files_fixed']}ê°œ")
        print(f"   threading import ì¶”ê°€: {summary['fixes']['threading_imports_added']}ê°œ")
        print(f"   syntax error ìˆ˜ì •: {summary['fixes']['syntax_errors_fixed']}ê°œ")
        print(f"   ì´ ìˆ˜ì •ì‚¬í•­: {summary['fixes']['total_fixes_applied']}ê°œ")
        
        # Stepë³„ ìƒì„¸ ê²°ê³¼
        print(f"\nğŸš€ GitHub 8ë‹¨ê³„ AI Step ë¶„ì„ ê²°ê³¼ (ìˆ˜ì • í›„):")
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        sorted_steps = sorted(self.step_analyses.items(), 
                            key=lambda x: (x[1].step_info.step_id))
        
        for step_name, analysis in sorted_steps:
            status_icon = "âœ…" if analysis.status == GitHubStepStatus.SUCCESS else "âŒ"
            priority_icon = "ğŸ”¥" if analysis.step_info.priority == "critical" else "âš¡" if analysis.step_info.priority == "high" else "ğŸ“"
            
            # ìˆ˜ì • ìƒíƒœ ì•„ì´ì½˜
            fix_icons = ""
            if analysis.syntax_error_fixed:
                fix_icons += "ğŸ”§"
            if analysis.threading_import_added:
                fix_icons += "ğŸ§µ"
            
            print(f"   {status_icon} {priority_icon} {fix_icons} Step {analysis.step_info.step_id}: {step_name} (ê±´ê°•ë„: {analysis.health_score:.0f}/100)")
            print(f"      Import: {'âœ…' if analysis.import_success else 'âŒ'} | "
                  f"ì¸ìŠ¤í„´ìŠ¤: {'âœ…' if analysis.instance_created else 'âŒ'} | "
                  f"ì´ˆê¸°í™”: {'âœ…' if analysis.initialization_success else 'âŒ'} | "
                  f"Central Hub: {'âœ…' if analysis.central_hub_connected else 'âŒ'}")
            
            if analysis.detected_model_files:
                print(f"      AI ëª¨ë¸: {len(analysis.detected_model_files)}ê°œ ({analysis.total_model_size_gb:.1f}GB, ì„±ê³µë¥ : {analysis.model_loading_success_rate:.1f}%)")
            
            if analysis.recommendations:
                print(f"      ì¶”ì²œ: {analysis.recommendations[0]}")
        
        # GitHub í†µí•© ìƒíƒœ
        github_insights = debug_result['github_specific_insights']
        print(f"\nğŸ”— GitHub í†µí•© ìƒíƒœ:")
        print(f"   AutoModelDetector: {github_insights.get('auto_model_detector_status', 'unknown')}")
        print(f"   StepFactory: {'âœ…' if github_insights.get('step_factory_availability') else 'âŒ'}")
        print(f"   RealAIImplementationManager: {'âœ…' if github_insights.get('real_ai_implementation_manager') else 'âŒ'}")
        print(f"   Central Hub: {'âœ…' if github_insights.get('central_hub_readiness') else 'âŒ'}")
        print(f"   ModelLoader v5.1: {'âœ…' if github_insights.get('model_loader_v5_compatibility') else 'âŒ'}")
        
        # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
        if 'advanced_analyses' in debug_result:
            advanced = debug_result['advanced_analyses']
            
            # DetailedDataSpec í†µí•© ìƒíƒœ
            if 'detailed_data_spec' in advanced:
                dataspec_analysis = advanced['detailed_data_spec']
                print(f"\nğŸ“Š DetailedDataSpec v5.3 í†µí•© ìƒíƒœ:")
                print(f"   ì „ì²´ í†µí•© ì ìˆ˜: {dataspec_analysis['overall_integration_score']:.1f}%")
                print(f"   API ë§¤í•‘ ì¤€ë¹„: {dataspec_analysis['api_mapping_ready_count']}/8 Step")
                print(f"   ë°ì´í„° ë³€í™˜ ì¤€ë¹„: {dataspec_analysis['data_conversion_ready_count']}/8 Step")
                print(f"   Emergency Fallback: {dataspec_analysis['emergency_fallback_count']}/8 Step")
            
            # DI Container í†µí•© ìƒíƒœ
            if 'di_container' in advanced:
                di_analysis = advanced['di_container']
                print(f"\nğŸ”— DI Container v7.0 í†µí•© ìƒíƒœ:")
                print(f"   í†µí•© ì ìˆ˜: {di_analysis['integration_score']:.1f}%")
                print(f"   ì„œë¹„ìŠ¤ ë“±ë¡: {len(di_analysis['services_registered'])}ê°œ")
                print(f"   Central Hub ì—°ê²°: {'âœ…' if di_analysis['central_hub_connected'] else 'âŒ'}")
                print(f"   Step ì£¼ì…: {'âœ…' if di_analysis['step_injection_working'] else 'âŒ'}")
            
            # StepFactory í†µí•© ìƒíƒœ
            if 'step_factory' in advanced:
                factory_analysis = advanced['step_factory']
                print(f"\nğŸ­ StepFactory v11.2 í†µí•© ìƒíƒœ:")
                print(f"   í†µí•© ì ìˆ˜: {factory_analysis['integration_score']:.1f}%")
                print(f"   ë²„ì „: {factory_analysis['step_factory_version']}")
                print(f"   Step ìƒì„±: {'âœ…' if factory_analysis['step_creation_working'] else 'âŒ'}")
                print(f"   GitHub í˜¸í™˜ì„±: {'âœ…' if factory_analysis['github_compatibility'] else 'âŒ'}")
        
        # ì¤‘ìš” ë¬¸ì œì 
        if debug_result['critical_issues']:
            print(f"\nğŸ”¥ ì¤‘ìš” ë¬¸ì œì :")
            for issue in debug_result['critical_issues']:
                print(f"   {issue}")
        
        # ì¶”ì²œì‚¬í•­
        if debug_result['actionable_recommendations']:
            print(f"\nğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­:")
            for i, rec in enumerate(debug_result['actionable_recommendations'], 1):
                print(f"   {i}. {rec}")
        
        # ì„±ëŠ¥ ì§€í‘œ
        metrics = debug_result['performance_metrics']
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
        print(f"   ì „ì²´ ë¶„ì„ ì‹œê°„: {metrics['analysis_time']['total_seconds']:.1f}ì´ˆ ({metrics['analysis_time']['efficiency_rating']})")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {metrics['memory_usage']['current_gb']:.1f}GB ({metrics['memory_usage']['efficiency']})")
        print(f"   ëª¨ë¸ ë¡œë”© íš¨ìœ¨ì„±: {metrics['model_loading']['loading_efficiency']} ({metrics['model_loading']['checkpoint_success_rate']:.1f}%)")
        print(f"   íŒŒì¼ ìˆ˜ì • íš¨ìœ¨ì„±: {metrics['file_fixes']['fix_success_rating']} ({metrics['file_fixes']['fix_efficiency_percent']:.1f}%)")
        print(f"   GitHub í†µí•© ì ìˆ˜: {metrics['github_integration']['integration_score']:.1f}% ({metrics['github_integration']['readiness_level']})")
    
    def _save_github_debug_results(self, debug_result: Dict[str, Any]):
        """GitHub ë””ë²„ê¹… ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = int(time.time())
            
            # JSON ê²°ê³¼ ì €ì¥
            results_file = Path(f"github_ai_debug_results_v6_{timestamp}.json")
            serializable_result = self._make_json_serializable(debug_result)
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_result, f, indent=2, ensure_ascii=False)
            
            # GitHub íŠ¹í™” ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
            summary_file = Path(f"github_ai_debug_summary_v6_{timestamp}.md")
            self._save_github_summary_report(summary_file, debug_result)
            
            print(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼: {results_file}")
            print(f"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")
            
        except Exception as e:
            print(f"\nâš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _make_json_serializable(self, obj):
        """JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, Path):
            return str(obj)
        elif hasattr(obj, '__dict__'):
            return self._make_json_serializable(obj.__dict__)
        elif isinstance(obj, Enum):
            return obj.value
        else:
            return obj
    
    def _save_github_summary_report(self, file_path: Path, debug_result: Dict[str, Any]):
        """GitHub ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("# ğŸ”¥ Ultimate GitHub AI Model Loading Debug Report v6.0\n\n")
                f.write(f"**ìƒì„± ì‹œê°„**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n")
                f.write(f"**GitHub í”„ë¡œì íŠ¸**: MyCloset AI Pipeline\n")
                f.write(f"**ë¶„ì„ ì†Œìš” ì‹œê°„**: {debug_result['total_debug_time']:.1f}ì´ˆ\n\n")
                
                # ì‹œìŠ¤í…œ í™˜ê²½
                env = debug_result['system_environment']
                f.write("## ğŸ“Š ì‹œìŠ¤í…œ í™˜ê²½\n\n")
                f.write(f"- **í•˜ë“œì›¨ì–´**: {env['hardware']['cpu_cores']}ì½”ì–´, {env['hardware']['total_memory_gb']:.1f}GB ë©”ëª¨ë¦¬\n")
                f.write(f"- **M3 Max**: {'âœ…' if env['hardware']['is_m3_max'] else 'âŒ'}\n")
                f.write(f"- **PyTorch**: {env['pytorch']['torch_version']} (ë””ë°”ì´ìŠ¤: {env['pytorch']['recommended_device']})\n")
                f.write(f"- **Conda í™˜ê²½**: {env['software']['conda_env']} {'âœ…' if env['software']['is_target_conda_env'] else 'âŒ'}\n")
                f.write(f"- **AI ëª¨ë¸**: {env['project_structure']['ai_models_size_gb']:.1f}GB\n\n")
                
                # íŒŒì¼ ìˆ˜ì • ê²°ê³¼
                fixes = env['fixes_applied']
                f.write("## ğŸ”§ íŒŒì¼ ìˆ˜ì • ê²°ê³¼\n\n")
                f.write(f"- **Step íŒŒì¼ ìˆ˜ì •**: {fixes['files_fixed']}ê°œ\n")
                f.write(f"- **threading import ì¶”ê°€**: {fixes['threading_imports_added']}ê°œ\n")
                f.write(f"- **syntax error ìˆ˜ì •**: {fixes['syntax_errors_fixed']}ê°œ\n\n")
                
                # ì „ì²´ ìš”ì•½
                summary = debug_result['overall_summary']
                f.write("## ğŸ¯ ë¶„ì„ ê²°ê³¼ ìš”ì•½\n\n")
                f.write(f"- **Step ì„±ê³µë¥ **: {summary['steps']['success_rate']:.1f}%\n")
                f.write(f"- **Critical Step**: {summary['steps']['critical_steps_success']}/{summary['steps']['critical_steps_total']} ì„±ê³µ\n")
                f.write(f"- **VirtualFittingStep**: {'ì¤€ë¹„ë¨' if summary['steps']['virtual_fitting_ready'] else 'ë¬¸ì œìˆìŒ'}\n")
                f.write(f"- **ì²´í¬í¬ì¸íŠ¸ ì„±ê³µë¥ **: {summary['checkpoints']['success_rate']:.1f}%\n")
                f.write(f"- **AI íŒŒì´í”„ë¼ì¸**: {'ì¤€ë¹„ë¨' if summary['health']['ai_pipeline_ready'] else 'ë¬¸ì œìˆìŒ'}\n\n")
                
                # ì¤‘ìš” ë¬¸ì œì 
                if debug_result['critical_issues']:
                    f.write("## ğŸ”¥ ì¤‘ìš” ë¬¸ì œì \n\n")
                    for issue in debug_result['critical_issues']:
                        f.write(f"- {issue}\n")
                    f.write("\n")
                
                # ì¶”ì²œì‚¬í•­
                if debug_result['actionable_recommendations']:
                    f.write("## ğŸ’¡ ì¶”ì²œì‚¬í•­\n\n")
                    for i, rec in enumerate(debug_result['actionable_recommendations'], 1):
                        f.write(f"{i}. {rec}\n")
                    f.write("\n")
                
                # Stepë³„ ìƒì„¸ ì •ë³´
                f.write("## ğŸš€ Stepë³„ ë¶„ì„ ê²°ê³¼\n\n")
                for step_name, step_data in debug_result['step_analyses'].items():
                    if isinstance(step_data, dict) and 'step_info' in step_data:
                        step_info = step_data['step_info']
                        f.write(f"### Step {step_info['step_id']}: {step_name}\n\n")
                        f.write(f"- **ìš°ì„ ìˆœìœ„**: {step_info['priority']}\n")
                        f.write(f"- **ìƒíƒœ**: {step_data['status']}\n")
                        f.write(f"- **ê±´ê°•ë„**: {step_data['performance']['health_score']:.0f}/100\n")
                        f.write(f"- **AI ëª¨ë¸**: {len(step_data['ai_models']['detected_files'])}ê°œ ({step_data['ai_models']['total_size_gb']:.1f}GB)\n")
                        
                        # ìˆ˜ì • ìƒíƒœ
                        fixes = step_data['file_fixes']
                        if fixes['syntax_error_fixed'] or fixes['threading_import_added']:
                            f.write(f"- **íŒŒì¼ ìˆ˜ì •**: ")
                            if fixes['syntax_error_fixed']:
                                f.write("ğŸ”§ syntax error ìˆ˜ì • ")
                            if fixes['threading_import_added']:
                                f.write("ğŸ§µ threading import ì¶”ê°€")
                            f.write("\n")
                        
                        if step_data['recommendations']:
                            f.write(f"- **ì¶”ì²œì‚¬í•­**: {step_data['recommendations'][0]}\n")
                        f.write("\n")
                
        except Exception as e:
            print(f"ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 12. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def quick_github_step_check(step_name: str) -> bool:
    """ë¹ ë¥¸ GitHub Step í™•ì¸"""
    try:
        step_configs = {config.step_name: config for config in GITHUB_STEP_CONFIGS}
        
        if step_name not in step_configs:
            return False
        
        config = step_configs[step_name]
        module = importlib.import_module(config.module_path)
        step_class = getattr(module, config.step_class)
        instance = step_class(device='cpu', strict_mode=False)
        return True
        
    except Exception:
        return False

def quick_github_checkpoint_check(checkpoint_name: str) -> bool:
    """ë¹ ë¥¸ GitHub ì²´í¬í¬ì¸íŠ¸ í™•ì¸"""
    try:
        if not ai_models_root.exists():
            return False
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰
        for ext in ['.pth', '.pt', '.safetensors', '.bin', '.ckpt']:
            files = list(ai_models_root.rglob(f"*{checkpoint_name}*{ext}"))
            if files:
                analyzer = GitHubCheckpointAnalyzer()
                result = analyzer.analyze_checkpoint(files[0])
                return result.status in [CheckpointLoadingStatus.SUCCESS, CheckpointLoadingStatus.SAFETENSORS_SUCCESS]
        
        return False
        
    except Exception:
        return False

def get_github_system_readiness_score() -> float:
    """GitHub ì‹œìŠ¤í…œ ì¤€ë¹„ë„ ì ìˆ˜ (0-100)"""
    try:
        analyzer = GitHubSystemAnalyzer()
        env = analyzer.analyze_github_environment()
        
        score = 0.0
        
        # PyTorch í™˜ê²½ (25ì )
        if env.torch_available:
            score += 20
            if env.mps_available or env.cuda_available:
                score += 5
        
        # í”„ë¡œì íŠ¸ êµ¬ì¡° (25ì )
        if env.project_root_exists:
            score += 8
        if env.backend_root_exists:
            score += 8
        if env.ai_models_root_exists:
            score += 9
        
        # ë©”ëª¨ë¦¬ (20ì )
        if env.available_memory_gb >= 16:
            score += 20
        elif env.available_memory_gb >= 8:
            score += 15
        elif env.available_memory_gb >= 4:
            score += 10
        
        # conda í™˜ê²½ (15ì )
        if env.is_target_conda_env:
            score += 15
        elif env.conda_env != 'none':
            score += 8
        
        # GitHub í†µí•© (15ì )
        integration_score = sum(env.github_integrations.values()) / len(env.github_integrations) * 15
        score += integration_score
        
        return min(100.0, score)
        
    except Exception:
        return 0.0

def run_github_quick_diagnosis() -> Dict[str, Any]:
    """GitHub ë¹ ë¥¸ ì§„ë‹¨"""
    try:
        print("ğŸ” GitHub í”„ë¡œì íŠ¸ ë¹ ë¥¸ ì§„ë‹¨ ì‹œì‘...")
        
        # ê¸°ë³¸ í™˜ê²½ ì²´í¬
        results = {
            'pytorch_available': False,
            'ai_models_exist': ai_models_root.exists(),
            'conda_env_correct': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean',
            'critical_steps_working': 0,
            'total_model_size_gb': 0.0,
            'readiness_score': 0.0
        }
        
        # PyTorch í™•ì¸
        try:
            import torch
            results['pytorch_available'] = True
        except ImportError:
            pass
        
        # Critical Step í™•ì¸
        critical_steps = ['HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep', 'VirtualFittingStep']
        working_count = 0
        
        for step_name in critical_steps:
            if quick_github_step_check(step_name):
                working_count += 1
        
        results['critical_steps_working'] = working_count
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚°
        if results['ai_models_exist']:
            total_size = 0
            for model_file in ai_models_root.rglob('*'):
                if model_file.is_file() and model_file.suffix in ['.pth', '.pt', '.safetensors', '.bin']:
                    total_size += model_file.stat().st_size
            results['total_model_size_gb'] = total_size / (1024**3)
        
        # ì¤€ë¹„ë„ ì ìˆ˜
        results['readiness_score'] = get_github_system_readiness_score()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"   PyTorch: {'âœ…' if results['pytorch_available'] else 'âŒ'}")
        print(f"   AI ëª¨ë¸: {'âœ…' if results['ai_models_exist'] else 'âŒ'} ({results['total_model_size_gb']:.1f}GB)")
        print(f"   Conda í™˜ê²½: {'âœ…' if results['conda_env_correct'] else 'âŒ'}")
        print(f"   Critical Steps: {results['critical_steps_working']}/4 ì‘ë™")
        print(f"   ì¤€ë¹„ë„: {results['readiness_score']:.1f}/100")
        
        return results
        
    except Exception as e:
        print(f"ë¹ ë¥¸ ì§„ë‹¨ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# =============================================================================
# ğŸ”¥ 13. ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.WARNING,
        format='%(levelname)s: %(message)s',
        force=True
    )
    
    print(f"ğŸ”¥ Ultimate AI Model Loading Debugger v6.0")
    print(f"ğŸ”¥ GitHub í”„ë¡œì íŠ¸: MyCloset AI Pipeline")
    print(f"ğŸ”¥ Target: ëª¨ë“  ì˜¤ë¥˜ ì™„ì „ í•´ê²° + 8ë‹¨ê³„ AI Step + 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„")
    print(f"ğŸ”¥ ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”¥ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    try:
        # ë¹ ë¥¸ ì§„ë‹¨ ë¨¼ì € ì‹¤í–‰
        print("\nğŸ” ë¹ ë¥¸ ì§„ë‹¨ ì‹¤í–‰...")
        quick_results = run_github_quick_diagnosis()
        
        if quick_results.get('readiness_score', 0) < 30:
            print(f"\nâš ï¸ ì‹œìŠ¤í…œ ì¤€ë¹„ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({quick_results.get('readiness_score', 0):.1f}/100). ì „ì²´ ë¶„ì„ì„ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
            response = input("ê³„ì†í•˜ë ¤ë©´ 'y' ì…ë ¥ (Enterì‹œ ìë™ ì§„í–‰): ").lower().strip()
            if response and response != 'y':
                print("ë¹ ë¥¸ ì§„ë‹¨ ê²°ê³¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return quick_results
        
        # GitHub ë””ë²„ê±° ìƒì„± ë° ì‹¤í–‰
        debugger = UltimateGitHubAIDebuggerV6()
        debug_result = debugger.run_ultimate_github_debugging()
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        overall_summary = debug_result.get('overall_summary', {})
        ai_ready = overall_summary.get('health', {}).get('ai_pipeline_ready', False)
        system_ready = overall_summary.get('health', {}).get('system_ready', False)
        fixes_applied = overall_summary.get('fixes', {}).get('total_fixes_applied', 0)
        
        if ai_ready and system_ready:
            print(f"\nğŸ‰ SUCCESS: GitHub AI íŒŒì´í”„ë¼ì¸ì´ ì™„ì „ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"   - 8ë‹¨ê³„ AI Step ë³µêµ¬ ì™„ë£Œ")
            print(f"   - {fixes_applied}ê°œ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ")
            print(f"   - 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„ ì™„ë£Œ")
            print(f"   - threading import ë° syntax error í•´ê²°")
            print(f"   - M3 Max + MPS ìµœì í™” ì ìš©")
            print(f"   - Central Hub DI Container ì—°ë™ ì™„ë£Œ")
        else:
            print(f"\nâš ï¸ WARNING: ì¼ë¶€ ë¬¸ì œê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            print(f"   - AI íŒŒì´í”„ë¼ì¸: {'âœ…' if ai_ready else 'âŒ'}")
            print(f"   - ì‹œìŠ¤í…œ í™˜ê²½: {'âœ…' if system_ready else 'âŒ'}")
            print(f"   - ìˆ˜ì •ëœ ì˜¤ë¥˜: {fixes_applied}ê°œ")
            print(f"   - ìœ„ì˜ ì¶”ì²œì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        return debug_result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        print(f"ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        print(f"\nğŸ‘‹ Ultimate GitHub AI Model Debugger v6.0 ì¢…ë£Œ")

if __name__ == "__main__":
    main()