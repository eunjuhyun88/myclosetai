#!/usr/bin/env python3
"""
ğŸ”¥ ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ v3.1 - PyTorch í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ í•´ê²°
backend/enhanced_model_loading_validator.py

âœ… PyTorch 2.7 weights_only ë¬¸ì œ í•´ê²°
âœ… Legacy .tar í¬ë§· ì™„ì „ ì§€ì›
âœ… TorchScript ì•„ì¹´ì´ë¸Œ ì§€ì›
âœ… 3ë‹¨ê³„ ì•ˆì „ ë¡œë”© ì‹œìŠ¤í…œ
âœ… Safetensors ì™„ì „ ì§€ì›
"""

import sys
from fix_pytorch_loading import apply_pytorch_patch; apply_pytorch_patch()

import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
import hashlib
import json
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import weakref
import gc
from contextlib import contextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

# =============================================================================
# ğŸ”¥ 1. PyTorch í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ í•´ê²°
# =============================================================================

def setup_pytorch_compatibility():
    """PyTorch weights_only ë¬¸ì œ ì™„ì „ í•´ê²°"""
    try:
        import torch
        
        # PyTorch 2.6+ weights_only ê¸°ë³¸ê°’ ë¬¸ì œ í•´ê²°
        if hasattr(torch, 'load'):
            original_torch_load = torch.load
            
            def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
                """ì•ˆì „í•œ torch.load - weights_only ê¸°ë³¸ê°’ì„ Falseë¡œ ì„¤ì •"""
                if weights_only is None:
                    weights_only = False
                return original_torch_load(f, map_location=map_location, 
                                         pickle_module=pickle_module, 
                                         weights_only=weights_only, **kwargs)
            
            # torch.load í•¨ìˆ˜ íŒ¨ì¹˜
            torch.load = safe_torch_load
            print("âœ… PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
            
        return True
    except ImportError:
        print("âŒ PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False

# PyTorch í˜¸í™˜ì„± ì„¤ì •
TORCH_AVAILABLE = setup_pytorch_compatibility()

# =============================================================================
# ğŸ”¥ 2. 3ë‹¨ê³„ ì•ˆì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë”
# =============================================================================

class SafeCheckpointLoader:
    """3ë‹¨ê³„ ì•ˆì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def load_checkpoint_safe(file_path: Path) -> Tuple[Optional[Any], str]:
        """
        3ë‹¨ê³„ ì•ˆì „ ë¡œë”©:
        1. weights_only=True (ìµœê³  ë³´ì•ˆ)
        2. weights_only=False (í˜¸í™˜ì„±)  
        3. Legacy ëª¨ë“œ (ì™„ì „ í˜¸í™˜)
        
        Returns:
            (checkpoint_data, loading_method)
        """
        if not TORCH_AVAILABLE:
            return None, "no_pytorch"
        
        import torch
        
        # ê²½ê³  ë©”ì‹œì§€ ì–µì œ
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            
            # ğŸ”¥ 1ë‹¨ê³„: ìµœê³  ë³´ì•ˆ ëª¨ë“œ
            try:
                checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                return checkpoint, "secure_mode"
            except Exception as e1:
                pass
            
            # ğŸ”¥ 2ë‹¨ê³„: í˜¸í™˜ì„± ëª¨ë“œ
            try:
                checkpoint = torch.load(file_path, map_location='cpu', weights_only=False)
                return checkpoint, "compatible_mode"
            except Exception as e2:
                pass
            
            # ğŸ”¥ 3ë‹¨ê³„: Legacy ëª¨ë“œ (ì¸ì ì—†ìŒ)
            try:
                checkpoint = torch.load(file_path, map_location='cpu')
                return checkpoint, "legacy_mode"
            except Exception as e3:
                return None, f"all_failed: {str(e3)[:100]}"

    @staticmethod
    def load_safetensors_safe(file_path: Path) -> Tuple[Optional[Any], str]:
        """Safetensors ì•ˆì „ ë¡œë”©"""
        try:
            from safetensors.torch import load_file
            checkpoint = load_file(file_path)
            return checkpoint, "safetensors"
        except ImportError:
            return None, "no_safetensors_lib"
        except Exception as e:
            return None, f"safetensors_failed: {str(e)[:100]}"

# =============================================================================
# ğŸ”¥ 3. ì›ë³¸ ì½”ë“œ ìˆ˜ì • (ModelLoadingDetailsëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€)
# =============================================================================

@dataclass
class ModelLoadingDetails:
    """ëª¨ë¸ ë¡œë”© ì„¸ë¶€ ì •ë³´"""
    name: str
    path: Path
    exists: bool
    size_mb: float
    file_type: str
    step_assignment: str
    
    # ë¡œë”© ìƒíƒœ
    checkpoint_loaded: bool = False
    model_created: bool = False
    weights_loaded: bool = False
    inference_ready: bool = False
    
    # ë¡œë”© ì„¸ë¶€ì‚¬í•­
    checkpoint_keys: List[str] = None
    model_layers: List[str] = None
    device_compatible: bool = False
    memory_usage_mb: float = 0.0
    load_time_seconds: float = 0.0
    loading_method: str = ""  # ğŸ”¥ ì¶”ê°€: ë¡œë”© ë°©ë²• ê¸°ë¡
    
    # ì˜¤ë¥˜ ì •ë³´
    errors: List[str] = None
    warnings: List[str] = None
    
    def __post_init__(self):
        if self.checkpoint_keys is None:
            self.checkpoint_keys = []
        if self.model_layers is None:
            self.model_layers = []
        if self.errors is None:
            self.errors = []
        if self.warnings is None:
            self.warnings = []

# =============================================================================
# ğŸ”¥ 4. ìˆ˜ì •ëœ ëª¨ë¸ ë¶„ì„ê¸° (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
# =============================================================================

class EnhancedModelAnalyzer:
    """ê°•í™”ëœ AI ëª¨ë¸ ë¶„ì„ê¸° - PyTorch í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°"""
    
    def __init__(self):
        self.model_files: List[ModelLoadingDetails] = []
        self.step_reports: Dict[str, Any] = {}
        self.analysis_start_time = time.time()
        
        # PyTorch ê´€ë ¨ ì²´í¬
        self.torch_available = TORCH_AVAILABLE
        self.device_info = {}
        self._check_pytorch_status()
        
    def _check_pytorch_status(self):
        """PyTorch ìƒíƒœ í™•ì¸"""
        if not self.torch_available:
            print("âŒ PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
            
        try:
            import torch
            
            self.device_info = {
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'mps_available': torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
                'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
                'default_device': 'mps' if (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()) else 'cuda' if torch.cuda.is_available() else 'cpu'
            }
            
            print(f"âœ… PyTorch {torch.__version__} ì‚¬ìš© ê°€ëŠ¥ (í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©)")
            print(f"   ğŸ–¥ï¸ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {self.device_info['default_device']}")
            print(f"   ğŸ MPS ì‚¬ìš© ê°€ëŠ¥: {self.device_info['mps_available']}")
            print(f"   ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {self.device_info['cuda_available']}")
            
        except Exception as e:
            print(f"âŒ PyTorch ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            self.torch_available = False
    
    def _analyze_checkpoint_details(self, details: ModelLoadingDetails):
        """ì²´í¬í¬ì¸íŠ¸ ì„¸ë¶€ ë¶„ì„ - PyTorch 2.7 í˜¸í™˜ì„± í•´ê²°"""
        if not self.torch_available:
            details.warnings.append("PyTorch ì—†ìŒ - ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê±´ë„ˆëœ€")
            return
            
        try:
            import torch
            
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            # ğŸ”¥ 3ë‹¨ê³„ ì•ˆì „ ë¡œë”© ì‹œìŠ¤í…œ (PyTorch 2.7 ì™„ì „ í˜¸í™˜)
            with safety.safe_execution(f"{details.name} ì²´í¬í¬ì¸íŠ¸ ë¡œë”©", timeout=30):
                
                checkpoint = None
                loading_method = ""
                
                # 1ë‹¨ê³„: ìµœì‹  ì•ˆì „ ëª¨ë“œ (weights_only=True)
                try:
                    if details.file_type in ['pth', 'pt']:
                        checkpoint = torch.load(details.path, map_location='cpu', weights_only=True)
                    elif details.file_type == 'safetensors':
                        try:
                            from safetensors.torch import load_file
                            checkpoint = load_file(details.path)
                        except ImportError:
                            details.warnings.append("safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                            return
                    
                    if checkpoint is not None:
                        loading_method = "safe_mode"
                        print(f"    âœ… ì•ˆì „ ëª¨ë“œ ë¡œë”© ì„±ê³µ")
                        
                except Exception as safe_error:
                    # Legacy .tar í¬ë§·ì´ë‚˜ TorchScript íŒŒì¼ì¼ ê°€ëŠ¥ì„±
                    print(f"    âš ï¸ ì•ˆì „ ëª¨ë“œ ì‹¤íŒ¨: {str(safe_error)[:50]}...")
                    
                    # 2ë‹¨ê³„: í˜¸í™˜ì„± ëª¨ë“œ (weights_only=False)
                    try:
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")  # ê²½ê³  ë¬´ì‹œ
                            
                            if details.file_type in ['pth', 'pt']:
                                checkpoint = torch.load(details.path, map_location='cpu', weights_only=False)
                            elif details.file_type == 'safetensors':
                                try:
                                    from safetensors.torch import load_file
                                    checkpoint = load_file(details.path)
                                except ImportError:
                                    details.warnings.append("safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                                    return
                        
                        if checkpoint is not None:
                            loading_method = "compatible_mode"
                            print(f"    âœ… í˜¸í™˜ì„± ëª¨ë“œ ë¡œë”© ì„±ê³µ")
                            
                    except Exception as compat_error:
                        print(f"    âš ï¸ í˜¸í™˜ì„± ëª¨ë“œ ì‹¤íŒ¨: {str(compat_error)[:50]}...")
                        
                        # 3ë‹¨ê³„: Legacy ëª¨ë“œ (íŒŒë¼ë¯¸í„° ì—†ìŒ)
                        try:
                            with warnings.catch_warnings():
                                warnings.filterwarnings("ignore")
                                
                                if details.file_type in ['pth', 'pt']:
                                    checkpoint = torch.load(details.path, map_location='cpu')
                                elif details.file_type == 'safetensors':
                                    try:
                                        from safetensors.torch import load_file
                                        checkpoint = load_file(details.path)
                                    except ImportError:
                                        details.warnings.append("safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                                        return
                            
                            if checkpoint is not None:
                                loading_method = "legacy_mode"
                                print(f"    âœ… Legacy ëª¨ë“œ ë¡œë”© ì„±ê³µ")
                                
                        except Exception as legacy_error:
                            print(f"    âŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {str(legacy_error)[:50]}...")
                            details.errors.append(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ (ëª¨ë“  ë°©ë²•): {legacy_error}")
                            return
                
                # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì§„í–‰
                if checkpoint is not None:
                    details.checkpoint_loaded = True
                    details.loading_method = loading_method  # ë¡œë”© ë°©ë²• ê¸°ë¡
                    
                    # State dict ë¶„ì„
                    state_dict = checkpoint
                    if isinstance(checkpoint, dict):
                        if 'model' in checkpoint:
                            state_dict = checkpoint['model']
                        elif 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                    
                    if isinstance(state_dict, dict):
                        details.checkpoint_keys = list(state_dict.keys())[:20]  # ì²˜ìŒ 20ê°œë§Œ
                        
                        # ëª¨ë¸ êµ¬ì¡° ì¶”ì •
                        self._estimate_model_structure(details, state_dict)
                        
                        # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬
                        self._check_device_compatibility(details, state_dict)
                    
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    details.load_time_seconds = end_time - start_time
                    details.memory_usage_mb = end_memory - start_memory
                    
                    print(f"    âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ ({details.load_time_seconds:.2f}ì´ˆ, {loading_method})")
                    
                else:
                    details.errors.append("ì²´í¬í¬ì¸íŠ¸ê°€ None")
                    
        except Exception as e:
            details.errors.append(f"ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            print(f"    âŒ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")

    # ModelLoadingDetails í´ë˜ìŠ¤ì— loading_method í•„ë“œ ì¶”ê°€
    @dataclass
    class ModelLoadingDetails:
        """ëª¨ë¸ ë¡œë”© ì„¸ë¶€ ì •ë³´"""
        name: str
        path: Path
        exists: bool
        size_mb: float
        file_type: str
        step_assignment: str
        
        # ë¡œë”© ìƒíƒœ
        checkpoint_loaded: bool = False
        model_created: bool = False
        weights_loaded: bool = False
        inference_ready: bool = False
        
        # ë¡œë”© ì„¸ë¶€ì‚¬í•­
        checkpoint_keys: List[str] = None
        model_layers: List[str] = None
        device_compatible: bool = False
        memory_usage_mb: float = 0.0
        load_time_seconds: float = 0.0
        loading_method: str = ""  # ğŸ”¥ ë¡œë”© ë°©ë²• ì¶”ê°€
        
        # ì˜¤ë¥˜ ì •ë³´
        errors: List[str] = None
        warnings: List[str] = None
        
        def __post_init__(self):
            if self.checkpoint_keys is None:
                self.checkpoint_keys = []
            if self.model_layers is None:
                self.model_layers = []
            if self.errors is None:
                self.errors = []
            if self.warnings is None:
                self.warnings = []


    def _estimate_model_structure(self, details: ModelLoadingDetails, state_dict: dict):
        """ëª¨ë¸ êµ¬ì¡° ì¶”ì •"""
        try:
            # ë ˆì´ì–´ íŒ¨í„´ ë¶„ì„
            layer_patterns = {}
            for key in state_dict.keys():
                if '.' in key:
                    layer_name = key.split('.')[0]
                    if layer_name not in layer_patterns:
                        layer_patterns[layer_name] = 0
                    layer_patterns[layer_name] += 1
            
            details.model_layers = list(layer_patterns.keys())[:10]  # ì²˜ìŒ 10ê°œë§Œ
            
            # ëª¨ë¸ ìœ í˜• ì¶”ì •
            if any('backbone' in key for key in state_dict.keys()):
                details.warnings.append("ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ë¡œ ì¶”ì •")
            elif any('pose' in key.lower() for key in state_dict.keys()):
                details.warnings.append("í¬ì¦ˆ ì¶”ì • ëª¨ë¸ë¡œ ì¶”ì •")
            elif any('diffusion' in key.lower() for key in state_dict.keys()):
                details.warnings.append("ë””í“¨ì „ ëª¨ë¸ë¡œ ì¶”ì •")
                
        except Exception as e:
            details.warnings.append(f"ëª¨ë¸ êµ¬ì¡° ì¶”ì • ì‹¤íŒ¨: {e}")
    
    def _check_device_compatibility(self, details: ModelLoadingDetails, state_dict: dict):
        """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬"""
        try:
            import torch
            
            # ìƒ˜í”Œ í…ì„œë¡œ ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸
            sample_key = next(iter(state_dict.keys()))
            sample_tensor = state_dict[sample_key]
            
            if torch.is_tensor(sample_tensor):
                # CPUë¡œ ì´ë™ í…ŒìŠ¤íŠ¸
                cpu_tensor = sample_tensor.to('cpu')
                
                # MPS í…ŒìŠ¤íŠ¸ (M3 Max)
                if self.device_info.get('mps_available'):
                    try:
                        mps_tensor = cpu_tensor.to('mps')
                        details.device_compatible = True
                        details.warnings.append("MPS í˜¸í™˜ í™•ì¸")
                    except Exception:
                        details.warnings.append("MPS í˜¸í™˜ ë¶ˆê°€")
                
                # CUDA í…ŒìŠ¤íŠ¸
                elif self.device_info.get('cuda_available'):
                    try:
                        cuda_tensor = cpu_tensor.to('cuda')
                        details.device_compatible = True
                        details.warnings.append("CUDA í˜¸í™˜ í™•ì¸")
                    except Exception:
                        details.warnings.append("CUDA í˜¸í™˜ ë¶ˆê°€")
                else:
                    details.device_compatible = True
                    details.warnings.append("CPU í˜¸í™˜ í™•ì¸")
                    
        except Exception as e:
            details.warnings.append(f"ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 5. ë‚˜ë¨¸ì§€ ì½”ë“œ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ì•ˆì „ì„± í–¥ìƒ)
# =============================================================================

class EnhancedSafetyManager:
    """ê°•í™”ëœ ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.timeout_duration = 60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.max_memory_mb = 4096   # 4GB ë©”ëª¨ë¦¬ ì œí•œ (ì¦ê°€)
        self.active_operations = []
        
    @contextmanager
    def safe_execution(self, description: str, timeout: int = None):
        """ì•ˆì „í•œ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        timeout = timeout or self.timeout_duration
        
        print(f"ğŸ”’ {description} ì•ˆì „ ì‹¤í–‰ ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ)")
        
        try:
            yield
            
        except Exception as e:
            print(f"âŒ {description} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"   ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}")
            if hasattr(e, '__traceback__'):
                tb_lines = traceback.format_tb(e.__traceback__)
                if tb_lines:
                    print(f"   ìŠ¤íƒ ì¶”ì : {tb_lines[-1].strip()}")
            
        finally:
            elapsed = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_used = end_memory - start_memory
            
            print(f"âœ… {description} ì™„ë£Œ ({elapsed:.2f}ì´ˆ, ë©”ëª¨ë¦¬: +{memory_used:.1f}MB)")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if memory_used > 200:  # 200MB ì´ìƒ ì‚¬ìš©ì‹œ ì •ë¦¬
                gc.collect()
                if TORCH_AVAILABLE:
                    import torch
                    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()

# ì „ì—­ ì•ˆì „ ë§¤ë‹ˆì €
safety = EnhancedSafetyManager()

# =============================================================================
# ğŸ”¥ 6. ë©”ì¸ ê²€ì¦ ì‹œìŠ¤í…œ (ê¸°ì¡´ê³¼ ë™ì¼)
# =============================================================================

class EnhancedModelValidator:
    """ê°•í™”ëœ ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.analyzer = EnhancedModelAnalyzer()
        self.start_time = time.time()
        
    def run_enhanced_validation(self) -> Dict[str, Any]:
        """ê°•í™”ëœ ê²€ì¦ ì‹¤í–‰"""
        
        print("ğŸ”¥ ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ v3.1 ì‹œì‘ (PyTorch í˜¸í™˜ì„± í•´ê²°)")
        print("=" * 80)
        
        validation_result = {
            'timestamp': time.time(),
            'pytorch_compatibility': 'fixed',
            'loading_methods': ['secure_mode', 'compatible_mode', 'legacy_mode', 'safetensors'],
            'system_info': self._get_system_info(),
            'pytorch_info': self.analyzer.device_info,
            'model_files_analysis': {},
            'step_loading_reports': {},
            'overall_summary': {},
            'recommendations': []
        }
        
        # 1. ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        print("\nğŸ“Š 1. ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„")
        with safety.safe_execution("ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„"):
            validation_result['system_info'] = self._get_system_info()
            self._print_system_info(validation_result['system_info'])
        
        # 2. ëª¨ë¸ íŒŒì¼ ë¶„ì„ (í•µì‹¬ ê°œì„  ë¶€ë¶„)
        print("\nğŸ“ 2. AI ëª¨ë¸ íŒŒì¼ ìƒì„¸ ë¶„ì„ (3ë‹¨ê³„ ì•ˆì „ ë¡œë”©)")
        with safety.safe_execution("AI ëª¨ë¸ íŒŒì¼ ë¶„ì„"):
            validation_result['model_files_analysis'] = self._analyze_all_model_files()
        
        # 3. ì „ì²´ ìš”ì•½ ìƒì„±
        print("\nğŸ“Š 3. ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        validation_result['overall_summary'] = self._generate_overall_summary(validation_result)
        validation_result['recommendations'] = self._generate_recommendations(validation_result)
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_validation_results(validation_result)
        
        # ì™„ë£Œ
        total_time = time.time() - self.start_time
        print(f"\nğŸ‰ ê°•í™”ëœ AI ëª¨ë¸ ê²€ì¦ ì™„ë£Œ! (PyTorch í˜¸í™˜ì„± í•´ê²°, ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        return validation_result
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        try:
            return {
                'platform': {
                    'system': platform.system(),
                    'release': platform.release(),
                    'machine': platform.machine(),
                    'processor': platform.processor()
                },
                'memory': {
                    'total_gb': psutil.virtual_memory().total / (1024**3),
                    'available_gb': psutil.virtual_memory().available / (1024**3),
                    'used_percent': psutil.virtual_memory().percent
                },
                'cpu': {
                    'core_count': psutil.cpu_count(),
                    'usage_percent': psutil.cpu_percent(interval=1)
                },
                'python': {
                    'version': sys.version.split()[0],
                    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none')
                }
            }
        except Exception as e:
            return {'error': str(e)}
    
    def _print_system_info(self, system_info: dict):
        """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
        if 'error' in system_info:
            print(f"   âŒ ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {system_info['error']}")
            return
            
        platform_info = system_info.get('platform', {})
        memory_info = system_info.get('memory', {})
        python_info = system_info.get('python', {})
        
        print(f"   ğŸ–¥ï¸ ì‹œìŠ¤í…œ: {platform_info.get('system')} {platform_info.get('release')}")
        print(f"   ğŸ”§ ì•„í‚¤í…ì²˜: {platform_info.get('machine')}")
        print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {memory_info.get('available_gb', 0):.1f}GB ì‚¬ìš©ê°€ëŠ¥ / {memory_info.get('total_gb', 0):.1f}GB ì´ëŸ‰")
        print(f"   ğŸ Python: {python_info.get('version')} (conda: {python_info.get('conda_env')})")
        
        if self.analyzer.torch_available:
            device_info = self.analyzer.device_info
            print(f"   ğŸ”¥ PyTorch: {device_info.get('torch_version')} (í˜¸í™˜ì„± íŒ¨ì¹˜)")
            print(f"   ğŸ–¥ï¸ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {device_info.get('default_device')}")
    
    def _analyze_all_model_files(self) -> Dict[str, Any]:
        """ğŸ”¥ í•µì‹¬ ê°œì„ : ëª¨ë“  ëª¨ë¸ íŒŒì¼ ë¶„ì„ - 3ë‹¨ê³„ ì•ˆì „ ë¡œë”© ì ìš©"""
        
        analysis_result = {
            'total_files': 0,
            'total_size_gb': 0.0,
            'analyzed_files': 0,
            'successful_loads': 0,  # ğŸ”¥ ì¶”ê°€
            'loading_methods_used': {},  # ğŸ”¥ ì¶”ê°€
            'large_models': [],
            'step_distribution': {},
            'loading_test_results': []
        }
        
        # ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰
        search_paths = [
            Path("ai_models"),
            Path("backend/ai_models"),
            Path("models")
        ]
        
        step_keywords = {
            'step_01_human_parsing': ['human', 'parsing', 'graphonomy', 'schp', 'atr', 'lip'],
            'step_02_pose_estimation': ['pose', 'openpose', 'yolo', 'hrnet', 'mediapipe'],
            'step_03_cloth_segmentation': ['cloth', 'segment', 'sam', 'u2net'],
            'step_04_geometric_matching': ['geometric', 'matching', 'gmm'],
            'step_05_cloth_warping': ['warping', 'realvis'],
            'step_06_virtual_fitting': ['fitting', 'diffusion', 'stable'],
            'step_07_post_processing': ['esrgan', 'post'],
            'step_08_quality_assessment': ['clip', 'quality']
        }
        
        for search_path in search_paths:
            if not search_path.exists():
                continue
                
            print(f"   ğŸ“ ê²€ìƒ‰ ì¤‘: {search_path}")
            
            for ext in ["*.pth", "*.pt", "*.safetensors", "*.bin"]:
                try:
                    found_files = list(search_path.rglob(ext))
                    
                    for file_path in found_files:
                        try:
                            size_bytes = file_path.stat().st_size
                            size_mb = size_bytes / (1024 * 1024)
                            
                            analysis_result['total_files'] += 1
                            analysis_result['total_size_gb'] += size_mb / 1024
                            
                            # Step í• ë‹¹
                            step_assignment = 'unknown'
                            path_str = str(file_path).lower()
                            for step, keywords in step_keywords.items():
                                if any(keyword in path_str for keyword in keywords):
                                    step_assignment = step
                                    break
                            
                            # ëŒ€í˜• ëª¨ë¸ (100MB ì´ìƒ)ë§Œ ìƒì„¸ ë¶„ì„
                            if size_mb >= 100:
                                analysis_result['analyzed_files'] += 1
                                
                                # ğŸ”¥ í•µì‹¬ ê°œì„ : ì•ˆì „í•œ ëª¨ë¸ ë¶„ì„
                                model_details = ModelLoadingDetails(
                                    name=file_path.name,
                                    path=file_path,
                                    exists=True,
                                    size_mb=size_mb,
                                    file_type=file_path.suffix[1:],
                                    step_assignment=step_assignment
                                )
                                
                                # ìƒì„¸ ë¶„ì„ ìˆ˜í–‰
                                self.analyzer._analyze_checkpoint_details(model_details)
                                
                                # í†µê³„ ì—…ë°ì´íŠ¸
                                if model_details.checkpoint_loaded:
                                    analysis_result['successful_loads'] += 1
                                    method = model_details.loading_method
                                    if method in analysis_result['loading_methods_used']:
                                        analysis_result['loading_methods_used'][method] += 1
                                    else:
                                        analysis_result['loading_methods_used'][method] = 1
                                
                                analysis_result['large_models'].append({
                                    'name': file_path.name,
                                    'size_mb': size_mb,
                                    'step': step_assignment,
                                    'checkpoint_loaded': model_details.checkpoint_loaded,
                                    'loading_method': model_details.loading_method,
                                    'device_compatible': model_details.device_compatible,
                                    'errors': len(model_details.errors),
                                    'warnings': len(model_details.warnings)
                                })
                            
                            # Stepë³„ ë¶„í¬
                            if step_assignment not in analysis_result['step_distribution']:
                                analysis_result['step_distribution'][step_assignment] = {
                                    'count': 0,
                                    'total_size_mb': 0.0
                                }
                            analysis_result['step_distribution'][step_assignment]['count'] += 1
                            analysis_result['step_distribution'][step_assignment]['total_size_mb'] += size_mb
                            
                        except Exception as e:
                            print(f"     âš ï¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_path.name} - {e}")
                            
                except Exception as e:
                    print(f"     âš ï¸ {ext} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ëŒ€í˜• ëª¨ë¸ ì •ë ¬
        analysis_result['large_models'].sort(key=lambda x: x['size_mb'], reverse=True)
        
        return analysis_result
    
    def _generate_overall_summary(self, validation_result: dict) -> Dict[str, Any]:
        """ì „ì²´ ìš”ì•½ ìƒì„±"""
        
        model_analysis = validation_result.get('model_files_analysis', {})
        
        # ëª¨ë¸ í†µê³„
        total_models = model_analysis.get('total_files', 0)
        analyzed_models = model_analysis.get('analyzed_files', 0)
        successful_loads = model_analysis.get('successful_loads', 0)
        large_models = len(model_analysis.get('large_models', []))
        
        return {
            'models': {
                'total_files': total_models,
                'large_models': large_models,
                'analyzed_models': analyzed_models,
                'successful_loads': successful_loads,
                'load_success_rate': (successful_loads / analyzed_models * 100) if analyzed_models > 0 else 0,
                'total_size_gb': model_analysis.get('total_size_gb', 0),
                'loading_methods_used': model_analysis.get('loading_methods_used', {})
            },
            'system_health': {
                'pytorch_available': self.analyzer.torch_available,
                'pytorch_compatibility_fixed': True,
                'device_acceleration': self.analyzer.device_info.get('default_device', 'cpu') != 'cpu',
                'memory_sufficient': validation_result.get('system_info', {}).get('memory', {}).get('available_gb', 0) > 2
            }
        }
    
    def _generate_recommendations(self, validation_result: dict) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        summary = validation_result['overall_summary']
        
        # ëª¨ë¸ ê´€ë ¨
        model_stats = summary['models']
        if model_stats['load_success_rate'] >= 90:
            recommendations.append(f"ğŸ‰ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_stats['load_success_rate']:.1f}%")
        elif model_stats['load_success_rate'] >= 70:
            recommendations.append(f"âœ… ëª¨ë¸ ë¡œë”© ì–‘í˜¸: {model_stats['load_success_rate']:.1f}% ì„±ê³µ")
        else:
            recommendations.append(f"âš ï¸ ëª¨ë¸ ë¡œë”© ê°œì„  í•„ìš”: {model_stats['load_success_rate']:.1f}% ì„±ê³µ")
        
        # ë¡œë”© ë°©ë²• í†µê³„
        loading_methods = model_stats.get('loading_methods_used', {})
        if loading_methods:
            method_summary = ", ".join([f"{k}: {v}ê°œ" for k, v in loading_methods.items()])
            recommendations.append(f"ğŸ“Š ì‚¬ìš©ëœ ë¡œë”© ë°©ë²•: {method_summary}")
        
        # ì‹œìŠ¤í…œ ê´€ë ¨
        system_health = summary['system_health']
        if not system_health['pytorch_available']:
            recommendations.append("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        else:
            recommendations.append("âœ… PyTorch í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ë¨")
        
        if not system_health['device_acceleration']:
            recommendations.append("âš ï¸ GPU ê°€ì† ì‚¬ìš© ë¶ˆê°€ - CPUë§Œ ì‚¬ìš© ì¤‘")
        else:
            recommendations.append("âœ… GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥")
        
        # ì´ ìš©ëŸ‰ ê´€ë ¨
        total_size = model_stats['total_size_gb']
        if total_size > 100:
            recommendations.append(f"ğŸ“Š ëŒ€ìš©ëŸ‰ AI ëª¨ë¸ í™˜ê²½: {total_size:.1f}GB")
        
        return recommendations
    
    def _print_validation_results(self, validation_result: dict):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ê²€ì¦ ê²°ê³¼ (PyTorch í˜¸í™˜ì„± í•´ê²°)")
        print("=" * 80)
        
        # ëª¨ë¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        model_analysis = validation_result['model_files_analysis']
        print(f"\nğŸ“ AI ëª¨ë¸ íŒŒì¼ ë¶„ì„:")
        print(f"   ğŸ“¦ ì´ íŒŒì¼: {model_analysis.get('total_files', 0)}ê°œ")
        print(f"   ğŸ’¾ ì´ í¬ê¸°: {model_analysis.get('total_size_gb', 0):.1f}GB")
        print(f"   ğŸ” ìƒì„¸ ë¶„ì„: {model_analysis.get('analyzed_files', 0)}ê°œ (100MB ì´ìƒ)")
        print(f"   âœ… ì„±ê³µ ë¡œë”©: {model_analysis.get('successful_loads', 0)}ê°œ")
        
        # ë¡œë”© ë°©ë²• í†µê³„
        loading_methods = model_analysis.get('loading_methods_used', {})
        if loading_methods:
            print(f"\n   ğŸ”§ ì‚¬ìš©ëœ ë¡œë”© ë°©ë²•:")
            for method, count in loading_methods.items():
                print(f"      {method}: {count}ê°œ")
        
        # ëŒ€í˜• ëª¨ë¸ ìƒìœ„ 5ê°œ
        large_models = model_analysis.get('large_models', [])[:5]
        if large_models:
            print(f"\n   ğŸ”¥ ëŒ€í˜• ëª¨ë¸ (ìƒìœ„ 5ê°œ):")
            for i, model in enumerate(large_models, 1):
                status = "âœ…" if model['checkpoint_loaded'] else "âŒ"
                device = "ğŸ–¥ï¸" if model['device_compatible'] else "âš ï¸"
                method = f"({model['loading_method']})" if model['loading_method'] else ""
                print(f"      {i}. {model['name']}: {model['size_mb']/1024:.1f}GB {status} {device} {method}")
        
        # ì „ì²´ ìš”ì•½
        summary = validation_result['overall_summary']
        print(f"\nğŸ“Š ì „ì²´ ìš”ì•½:")
        print(f"   ğŸ”¥ ëª¨ë¸ ë¡œë”© ì„±ê³µë¥ : {summary['models']['load_success_rate']:.1f}% ({summary['models']['successful_loads']}/{summary['models']['analyzed_models']})")
        print(f"   ğŸ–¥ï¸ PyTorch: {'âœ… (í˜¸í™˜ì„± í•´ê²°)' if summary['system_health']['pytorch_available'] else 'âŒ'}")
        print(f"   âš¡ ê°€ì†: {'âœ…' if summary['system_health']['device_acceleration'] else 'âŒ'}")
        
        # ì¶”ì²œì‚¬í•­
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        recommendations = validation_result['recommendations']
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")

# =============================================================================
# ğŸ”¥ 7. ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì•ˆì „í•œ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.WARNING,
        format='%(levelname)s: %(message)s',
        force=True
    )
    
    try:
        # ê²€ì¦ ì‹œìŠ¤í…œ ìƒì„± ë° ì‹¤í–‰
        validator = EnhancedModelValidator()
        
        # ê°•í™”ëœ ê²€ì¦ ì‹¤í–‰
        validation_result = validator.run_enhanced_validation()
        
        # JSON ê²°ê³¼ ì €ì¥
        try:
            results_file = Path("enhanced_model_validation_fixed.json")
            
            # ì‹œê°„ ì •ë³´ ì¶”ê°€
            validation_result['validation_completed_at'] = time.time()
            validation_result['total_validation_time'] = time.time() - validator.start_time
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(validation_result, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ“„ ìƒì„¸ ê²€ì¦ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as save_e:
            print(f"\nâš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")
        
        return validation_result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        if TORCH_AVAILABLE:
            import torch
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
        print(f"\nğŸ‘‹ ê°•í™”ëœ AI ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ ì¢…ë£Œ")

if __name__ == "__main__":
    main()