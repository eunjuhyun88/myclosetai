#!/usr/bin/env python3
"""
π”¥ MyCloset AI - Step 01: μ‹¤μ  μ‘λ™ κ°€λ¥ν• Human Parsing
===============================================================================

β οΈ ν„μ‹¤μ μΈ μ ‘κ·Όλ²•:
1. μ‹¤μ  λ¨λΈ κµ¬μ΅°λ¥Ό μ •ν™•ν λ¶„μ„
2. μ²΄ν¬ν¬μΈνΈ νμΌ κµ¬μ΅°λ¥Ό λ¨Όμ € μ΄ν•΄
3. νΈν™ κ°€λ¥ν• λ¨λΈ κµ¬μ΅°λ¥Ό μ—­κ³µν•™μ μΌλ΅ κµ¬ν„
4. λ‹¨κ³„μ  κ²€μ¦ μ‹μ¤ν…

μ‹¤μ  νμΌλ“¤:
- graphonomy.pth (1173MB) - μ‹¤μ  κµ¬μ΅° λ¶„μ„ ν•„μ”
- exp-schp-201908301523-atr.pth (255MB) - ATR λ°μ΄ν„°μ…‹ μ²΄ν¬ν¬μΈνΈ
- atr_model.pth (255MB) - λ³„λ„ ATR λ¨λΈ
- lip_model.pth (255MB) - LIP λ°μ΄ν„°μ…‹ μ²΄ν¬ν¬μΈνΈ

ν•µμ‹¬: λ¨λΈ κµ¬μ΅°λ¥Ό μ¶”μΈ΅ν•μ§€ λ§κ³  μ‹¤μ  μ²΄ν¬ν¬μΈνΈμ—μ„ μ—­μ¶”μ 
"""

import os
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Union, Tuple, List
from PIL import Image
import cv2
import json
import time
from dataclasses import dataclass

# ==============================================
# π” 1. μ‹¤μ  μ²΄ν¬ν¬μΈνΈ λ¶„μ„κΈ° (ν•µμ‹¬!)
# ==============================================

class ModelCheckpointAnalyzer:
    """μ‹¤μ  μ²΄ν¬ν¬μΈνΈ νμΌμ„ λ¶„μ„ν•΄μ„ μ •ν™•ν• λ¨λΈ κµ¬μ΅° νμ•…"""
    
    def __init__(self, model_path: Path):
        self.model_path = model_path
        self.logger = logging.getLogger(self.__class__.__name__)
        self.checkpoint = None
        self.layer_info = {}
        
    def analyze_checkpoint(self) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ νμΌ μ™„μ „ λ¶„μ„"""
        try:
            # 1. μ²΄ν¬ν¬μΈνΈ λ΅λ”©
            self.checkpoint = torch.load(self.model_path, map_location='cpu')
            
            analysis = {
                "file_path": str(self.model_path),
                "file_size_mb": round(self.model_path.stat().st_size / (1024**2), 2),
                "checkpoint_structure": self._analyze_structure(),
                "state_dict_info": self._analyze_state_dict(),
                "model_metadata": self._extract_metadata(),
                "layer_compatibility": self._check_layer_compatibility()
            }
            
            self.logger.info(f"π“‹ μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ™„λ£: {self.model_path.name}")
            return analysis
            
        except Exception as e:
            self.logger.error(f"β μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ‹¤ν¨: {e}")
            return {"error": str(e)}
    
    def _analyze_structure(self) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ μµμƒμ„ κµ¬μ΅° λ¶„μ„"""
        if not isinstance(self.checkpoint, dict):
            return {"type": "direct_state_dict", "keys": ["tensor_data"]}
        
        structure = {
            "type": "checkpoint_dict",
            "top_level_keys": list(self.checkpoint.keys()),
            "has_state_dict": "state_dict" in self.checkpoint,
            "has_model": "model" in self.checkpoint,
            "has_optimizer": "optimizer" in self.checkpoint,
            "has_epoch": "epoch" in self.checkpoint
        }
        
        # κ° ν‚¤μ νƒ€μ… ν™•μΈ
        for key, value in self.checkpoint.items():
            structure[f"{key}_type"] = type(value).__name__
            if isinstance(value, dict):
                structure[f"{key}_subkeys"] = list(value.keys())[:10]  # μ²μ 10κ°λ§
        
        return structure
    
    def _analyze_state_dict(self) -> Dict[str, Any]:
        """state_dict μƒμ„Έ λ¶„μ„"""
        # state_dict μ„μΉ μ°ΎκΈ°
        state_dict = None
        if isinstance(self.checkpoint, dict):
            if 'state_dict' in self.checkpoint:
                state_dict = self.checkpoint['state_dict']
            elif 'model' in self.checkpoint:
                state_dict = self.checkpoint['model']
            elif all(isinstance(v, torch.Tensor) for v in self.checkpoint.values()):
                state_dict = self.checkpoint
        else:
            # μ²΄ν¬ν¬μΈνΈ μμ²΄κ°€ ν…μ„λ“¤μΈ κ²½μ°
            state_dict = self.checkpoint
        
        if state_dict is None:
            return {"error": "state_dictλ¥Ό μ°Ύμ„ μ μ—†μ"}
        
        # λ μ΄μ–΄ λ¶„μ„
        layer_info = {}
        for name, param in state_dict.items():
            if isinstance(param, torch.Tensor):
                layer_info[name] = {
                    "shape": list(param.shape),
                    "dtype": str(param.dtype),
                    "requires_grad": param.requires_grad,
                    "parameters": param.numel()
                }
        
        # μ•„ν‚¤ν…μ² ν¨ν„΄ λ¶„μ„
        layer_names = list(layer_info.keys())
        architecture_info = self._infer_architecture_from_layers(layer_names)
        
        return {
            "total_layers": len(layer_info),
            "total_parameters": sum(info["parameters"] for info in layer_info.values()),
            "layer_details": layer_info,
            "architecture_inference": architecture_info,
            "sample_layers": dict(list(layer_info.items())[:5])  # μ²μ 5κ° λ μ΄μ–΄
        }
    
    def _infer_architecture_from_layers(self, layer_names: List[str]) -> Dict[str, Any]:
        """λ μ΄μ–΄λ…μΌλ΅λ¶€ν„° μ•„ν‚¤ν…μ² μ¶”λ΅ """
        analysis = {
            "backbone_type": "unknown",
            "num_stages": 0,
            "has_fpn": False,
            "has_classifier": False,
            "num_classes": 0,
            "specific_patterns": []
        }
        
        layer_str = " ".join(layer_names).lower()
        
        # λ°±λ³Έ νƒ€μ… κ°μ§€
        if "resnet" in layer_str or "layer1" in layer_str:
            analysis["backbone_type"] = "ResNet"
            # ResNet μ¤ν…μ΄μ§€ μ κ³„μ‚°
            for i in range(1, 6):
                if f"layer{i}" in layer_str:
                    analysis["num_stages"] = max(analysis["num_stages"], i)
        
        elif "mobilenet" in layer_str or "inverted_residual" in layer_str:
            analysis["backbone_type"] = "MobileNet"
        
        elif "efficientnet" in layer_str:
            analysis["backbone_type"] = "EfficientNet"
        
        # FPN κ°μ§€
        if "fpn" in layer_str or "lateral" in layer_str:
            analysis["has_fpn"] = True
        
        # λ¶„λ¥κΈ° κ°μ§€
        classifier_keywords = ["classifier", "fc", "head", "cls"]
        for keyword in classifier_keywords:
            if keyword in layer_str:
                analysis["has_classifier"] = True
                break
        
        # νΉμ ν¨ν„΄ κ°μ§€
        if "graph" in layer_str:
            analysis["specific_patterns"].append("graph_neural_network")
        if "attention" in layer_str:
            analysis["specific_patterns"].append("attention_mechanism")
        if "decoder" in layer_str:
            analysis["specific_patterns"].append("encoder_decoder")
        
        # ν΄λμ¤ μ μ¶”μ • (λ§μ§€λ§‰ λ¶„λ¥ λ μ΄μ–΄μ—μ„)
        for name in reversed(layer_names):
            if any(keyword in name.lower() for keyword in classifier_keywords):
                # μ‹¤μ λ΅λ” μ²΄ν¬ν¬μΈνΈλ¥Ό λ΅λ“ν•΄μ„ ν™•μΈν•΄μ•Ό ν•¨
                break
        
        return analysis
    
    def _extract_metadata(self) -> Dict[str, Any]:
        """λ©”νƒ€λ°μ΄ν„° μ¶”μ¶"""
        metadata = {}
        
        if isinstance(self.checkpoint, dict):
            # μΌλ°μ μΈ λ©”νƒ€λ°μ΄ν„° ν‚¤λ“¤
            meta_keys = ['epoch', 'best_acc', 'best_iou', 'optimizer', 'lr_scheduler', 'config']
            for key in meta_keys:
                if key in self.checkpoint:
                    value = self.checkpoint[key]
                    # λ³µμ΅ν• κ°μ²΄λ” λ¬Έμμ—΄λ΅ λ³€ν™
                    if isinstance(value, (dict, list, tuple)):
                        metadata[key] = str(value)[:200]  # μ²μ 200μλ§
                    else:
                        metadata[key] = value
        
        return metadata
    
    def _check_layer_compatibility(self) -> Dict[str, Any]:
        """ν‘μ¤€ μ•„ν‚¤ν…μ²μ™€μ νΈν™μ„± ν™•μΈ"""
        compatibility = {
            "torchvision_resnet": False,
            "segmentation_models": False,
            "custom_architecture": False,
            "suggested_base_model": None
        }
        
        # μ‹¤μ  μ²΄ν¬ν¬μΈνΈμ—μ„ state_dict κ°€μ Έμ¤κΈ°
        state_dict = self._get_state_dict()
        if state_dict is None:
            return compatibility
        
        layer_names = list(state_dict.keys())
        layer_str = " ".join(layer_names)
        
        # torchvision ResNet νΈν™μ„±
        resnet_patterns = ["conv1.weight", "bn1.weight", "layer1.0.conv1.weight"]
        if all(pattern in layer_str for pattern in resnet_patterns[:2]):
            compatibility["torchvision_resnet"] = True
            compatibility["suggested_base_model"] = "torchvision.models.resnet"
        
        # segmentation models νΈν™μ„±  
        seg_patterns = ["encoder", "decoder", "segmentation_head"]
        if any(pattern in layer_str for pattern in seg_patterns):
            compatibility["segmentation_models"] = True
            compatibility["suggested_base_model"] = "segmentation_models"
        
        # μ™„μ „ν μ»¤μ¤ν…€μΈ κ²½μ°
        if not any(compatibility.values()):
            compatibility["custom_architecture"] = True
            compatibility["suggested_base_model"] = "custom_implementation_needed"
        
        return compatibility
    
    def _get_state_dict(self) -> Optional[Dict]:
        """μ²΄ν¬ν¬μΈνΈμ—μ„ state_dict μ¶”μ¶"""
        if isinstance(self.checkpoint, dict):
            if 'state_dict' in self.checkpoint:
                return self.checkpoint['state_dict']
            elif 'model' in self.checkpoint:
                return self.checkpoint['model']
            elif all(isinstance(v, torch.Tensor) for v in self.checkpoint.values()):
                return self.checkpoint
        
        return None

# ==============================================
# π”§ 2. νΈν™ κ°€λ¥ν• λ¨λΈ λΉλ” (μ—­κ³µν•™)
# ==============================================

class CompatibleModelBuilder:
    """μ²΄ν¬ν¬μΈνΈ λ¶„μ„ κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ νΈν™ κ°€λ¥ν• λ¨λΈ μƒμ„±"""
    
    def __init__(self, checkpoint_analysis: Dict[str, Any]):
        self.analysis = checkpoint_analysis
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def build_compatible_model(self, model_type: str = "graphonomy") -> Optional[nn.Module]:
        """νΈν™ κ°€λ¥ν• λ¨λΈ μƒμ„±"""
        try:
            if "error" in self.analysis:
                self.logger.error(f"λ¶„μ„ μ¤λ¥λ΅ μΈν•΄ λ¨λΈ μƒμ„± λ¶κ°€: {self.analysis['error']}")
                return None
            
            # μ•„ν‚¤ν…μ² μ •λ³΄ μ¶”μ¶
            state_dict_info = self.analysis.get("state_dict_info", {})
            architecture_info = state_dict_info.get("architecture_inference", {})
            layer_details = state_dict_info.get("layer_details", {})
            
            # νΈν™μ„± μ •λ³΄
            compatibility = self.analysis.get("layer_compatibility", {})
            
            if model_type == "graphonomy":
                return self._build_graphonomy_compatible(architecture_info, layer_details, compatibility)
            elif model_type == "atr":
                return self._build_atr_compatible(architecture_info, layer_details, compatibility)
            else:
                return self._build_generic_compatible(architecture_info, layer_details, compatibility)
                
        except Exception as e:
            self.logger.error(f"νΈν™ λ¨λΈ μƒμ„± μ‹¤ν¨: {e}")
            return None
    
    def _build_graphonomy_compatible(self, arch_info: Dict, layer_details: Dict, compatibility: Dict) -> nn.Module:
        """Graphonomy νΈν™ λ¨λΈ μƒμ„±"""
        
        class GraphonomyCompatible(nn.Module):
            def __init__(self, num_classes=20):
                super().__init__()
                
                # μ‹¤μ  λ μ΄μ–΄ κµ¬μ΅°λ¥Ό λ¶„μ„ν•΄μ„ μƒμ„±
                self.layers = nn.ModuleDict()
                
                # μ²΄ν¬ν¬μΈνΈμ—μ„ λ°κ²¬λ λ μ΄μ–΄λ“¤μ„ λ°”νƒ•μΌλ΅ κµ¬μ΅° μƒμ„±
                self._build_from_layer_analysis(layer_details)
                
                # κΈ°λ³Έ μ¶λ ¥ ν—¤λ“ (μ—†μΌλ©΄ μ¶”κ°€)
                if not self._has_output_layer(layer_details):
                    self.output_head = nn.Conv2d(512, num_classes, 1)  # κΈ°λ³Έκ°’
            
            def _build_from_layer_analysis(self, layer_details):
                """μ‹¤μ  λ μ΄μ–΄ μ •λ³΄λ΅λ¶€ν„° λ¨λΈ κµ¬μ΅° μƒμ„±"""
                for layer_name, layer_info in layer_details.items():
                    # μ‹¤μ  λ μ΄μ–΄ μƒμ„± λ΅μ§
                    # μ΄ λ¶€λ¶„μ€ μ‹¤μ  μ²΄ν¬ν¬μΈνΈ λ¶„μ„ κ²°κ³Όμ— λ”°λΌ λ‹¬λΌμ§
                    pass
            
            def _has_output_layer(self, layer_details):
                """μ¶λ ¥ λ μ΄μ–΄ μ΅΄μ¬ μ—¬λ¶€ ν™•μΈ"""
                output_keywords = ['classifier', 'head', 'output', 'cls']
                return any(keyword in name.lower() for name in layer_details.keys() 
                          for keyword in output_keywords)
            
            def forward(self, x):
                # κ°„λ‹¨ν• forward pass
                # μ‹¤μ λ΅λ” μ²΄ν¬ν¬μΈνΈ κµ¬μ΅°μ— λ§μ¶° κµ¬ν„ν•΄μ•Ό ν•¨
                return x  # placeholder
        
        return GraphonomyCompatible()
    
    def _build_atr_compatible(self, arch_info: Dict, layer_details: Dict, compatibility: Dict) -> nn.Module:
        """ATR νΈν™ λ¨λΈ μƒμ„±"""
        
        class ATRCompatible(nn.Module):
            def __init__(self):
                super().__init__()
                # ATR λ¨λΈ κµ¬μ΅° (μ‹¤μ  λ¶„μ„ κ²°κ³Ό κΈ°λ°)
                pass
            
            def forward(self, x):
                return x
        
        return ATRCompatible()
    
    def _build_generic_compatible(self, arch_info: Dict, layer_details: Dict, compatibility: Dict) -> nn.Module:
        """λ²”μ© νΈν™ λ¨λΈ μƒμ„±"""
        
        class GenericCompatible(nn.Module):
            def __init__(self):
                super().__init__()
                # λ²”μ© κµ¬μ΅°
                pass
            
            def forward(self, x):
                return x
        
        return GenericCompatible()

# ==============================================
# π”§ 3. μ•μ „ν• λ¨λΈ λ΅λ”
# ==============================================

class SafeModelLoader:
    """μ•μ „ν•κ³  κ²€μ¦λ λ¨λΈ λ΅λ”©"""
    
    def __init__(self, model_path: Path, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # 1λ‹¨κ³„: μ²΄ν¬ν¬μΈνΈ λ¶„μ„
        self.analyzer = ModelCheckpointAnalyzer(model_path)
        self.analysis = None
        self.model = None
        
    def load_with_verification(self) -> bool:
        """κ²€μ¦λ λ¨λΈ λ΅λ”©"""
        try:
            # 1. μ²΄ν¬ν¬μΈνΈ λ¶„μ„
            self.logger.info(f"π” μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ‹μ‘: {self.model_path.name}")
            self.analysis = self.analyzer.analyze_checkpoint()
            
            if "error" in self.analysis:
                self.logger.error(f"β μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ‹¤ν¨: {self.analysis['error']}")
                return False
            
            # 2. νΈν™ λ¨λΈ μƒμ„±
            self.logger.info("π”§ νΈν™ λ¨λΈ μƒμ„± μ¤‘...")
            builder = CompatibleModelBuilder(self.analysis)
            self.model = builder.build_compatible_model()
            
            if self.model is None:
                self.logger.error("β νΈν™ λ¨λΈ μƒμ„± μ‹¤ν¨")
                return False
            
            # 3. κ°€μ¤‘μΉ λ΅λ”© μ‹λ„
            self.logger.info("β–οΈ κ°€μ¤‘μΉ λ΅λ”© μ‹λ„...")
            loading_result = self._try_load_weights()
            
            # 4. λ¨λΈ κ²€μ¦
            if loading_result["success"]:
                self.logger.info("β… λ¨λΈ λ΅λ”© λ° κ²€μ¦ μ™„λ£")
                self._print_loading_summary(loading_result)
                return True
            else:
                self.logger.warning(f"β οΈ λ¶€λ¶„μ  λ΅λ”©: {loading_result['message']}")
                return False
                
        except Exception as e:
            self.logger.error(f"β λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _try_load_weights(self) -> Dict[str, Any]:
        """κ°€μ¤‘μΉ λ΅λ”© μ‹λ„ λ° κ²€μ¦"""
        state_dict = self.analyzer._get_state_dict()
        if state_dict is None:
            return {"success": False, "message": "state_dictλ¥Ό μ°Ύμ„ μ μ—†μ"}
        
        # λ¨λΈκ³Ό μ²΄ν¬ν¬μΈνΈ ν‚¤ λΉ„κµ
        model_keys = set(self.model.state_dict().keys())
        checkpoint_keys = set(state_dict.keys())
        
        matching_keys = model_keys & checkpoint_keys
        missing_keys = model_keys - checkpoint_keys
        unexpected_keys = checkpoint_keys - model_keys
        
        result = {
            "model_keys_count": len(model_keys),
            "checkpoint_keys_count": len(checkpoint_keys),
            "matching_keys_count": len(matching_keys),
            "missing_keys_count": len(missing_keys),
            "unexpected_keys_count": len(unexpected_keys),
            "matching_keys": list(matching_keys)[:10],  # μ²μ 10κ°λ§
            "missing_keys": list(missing_keys)[:10],
            "unexpected_keys": list(unexpected_keys)[:10]
        }
        
        # λ§¤μΉ­λ¥  κ³„μ‚°
        if len(model_keys) > 0:
            match_rate = len(matching_keys) / len(model_keys)
            result["match_rate"] = match_rate
            
            # λ΅λ”© μ‹λ„
            if match_rate > 0.1:  # 10% μ΄μƒ λ§¤μΉ­λλ©΄ λ΅λ”© μ‹λ„
                try:
                    # λ§¤μΉ­λλ” ν‚¤λ§ λ΅λ”©
                    filtered_state_dict = {k: v for k, v in state_dict.items() if k in matching_keys}
                    self.model.load_state_dict(filtered_state_dict, strict=False)
                    
                    result["success"] = True
                    result["message"] = f"λ¶€λ¶„ λ΅λ”© μ„±κ³µ ({match_rate:.1%} λ§¤μΉ­)"
                    
                except Exception as e:
                    result["success"] = False
                    result["message"] = f"λ΅λ”© μ‹¤ν¨: {str(e)}"
            else:
                result["success"] = False
                result["message"] = f"λ§¤μΉ­λ¥  λ„λ¬΄ λ‚®μ ({match_rate:.1%})"
        else:
            result["success"] = False
            result["message"] = "λ¨λΈμ— λ μ΄μ–΄κ°€ μ—†μ"
        
        return result
    
    def _print_loading_summary(self, result: Dict[str, Any]):
        """λ΅λ”© κ²°κ³Ό μ”μ•½ μ¶λ ¥"""
        print("\n" + "="*60)
        print(f"π“‹ λ¨λΈ λ΅λ”© κ²°κ³Ό: {self.model_path.name}")
        print("="*60)
        print(f"π“ μ²΄ν¬ν¬μΈνΈ ν¬κΈ°: {self.analysis['file_size_mb']}MB")
        print(f"π”Ά μ „μ²΄ νλΌλ―Έν„°: {self.analysis['state_dict_info']['total_parameters']:,}κ°")
        print(f"π—οΈ μ¶”μ • μ•„ν‚¤ν…μ²: {self.analysis['state_dict_info']['architecture_inference']['backbone_type']}")
        print(f"β–οΈ ν‚¤ λ§¤μΉ­λ¥ : {result['match_rate']:.1%}")
        print(f"β… λ΅λ”©λ λ μ΄μ–΄: {result['matching_keys_count']}/{result['model_keys_count']}")
        print(f"π“‹ μƒνƒ: {result['message']}")
        print("="*60)

# ==============================================
# π”§ 4. μ‹¤μ  μ‚¬μ© μμ‹
# ==============================================

async def test_realistic_model_loading():
    """ν„μ‹¤μ μΈ λ¨λΈ λ΅λ”© ν…μ¤νΈ"""
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    print("π” μ‹¤μ  λ¨λΈ νμΌ κΈ°λ° λ¶„μ„ λ° λ΅λ”© ν…μ¤νΈ")
    print("="*60)
    
    # μ‹¤μ  λ¨λΈ νμΌλ“¤
    model_files = {
        "graphonomy": "ai_models/step_01_human_parsing/graphonomy.pth",
        "schp_atr": "ai_models/step_01_human_parsing/exp-schp-201908301523-atr.pth",
        "atr": "ai_models/step_01_human_parsing/atr_model.pth",
        "lip": "ai_models/step_01_human_parsing/lip_model.pth"
    }
    
    success_count = 0
    
    for model_name, model_path in model_files.items():
        path_obj = Path(model_path)
        
        if not path_obj.exists():
            logger.warning(f"β οΈ {model_name} νμΌ μ—†μ: {model_path}")
            continue
        
        try:
            print(f"\nπ”¬ {model_name.upper()} λ¨λΈ λ¶„μ„ μ¤‘...")
            
            # μ•μ „ν• λ¨λΈ λ΅λ” μƒμ„±
            loader = SafeModelLoader(path_obj)
            
            # λ΅λ”© μ‹λ„
            if loader.load_with_verification():
                success_count += 1
                logger.info(f"β… {model_name} λ΅λ”© μ„±κ³µ")
            else:
                logger.warning(f"β οΈ {model_name} λ΅λ”© μ‹¤ν¨")
                
        except Exception as e:
            logger.error(f"β {model_name} μ²λ¦¬ μ‹¤ν¨: {e}")
    
    print(f"\nπ“ μµμΆ… κ²°κ³Ό: {success_count}/{len(model_files)}κ° λ¨λΈ μ²λ¦¬ μ™„λ£")
    
    return success_count > 0

if __name__ == "__main__":
    import asyncio
    
    print("π”¥ MyCloset AI - ν„μ‹¤μ μΈ Step 01 Human Parsing")
    print("β οΈ μ‹¤μ  μ²΄ν¬ν¬μΈνΈ νμΌ λ¶„μ„ κΈ°λ°")
    print("β… νΈν™μ„± κ²€μ¦ λ° μ•μ „ν• λ΅λ”©")
    
    asyncio.run(test_realistic_model_loading())