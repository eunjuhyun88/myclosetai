#!/usr/bin/env python3
"""
π”¥ μµμΆ… AI λ¨λΈ νΈν™μ„± κ°μ„  λ„κµ¬
================================

λ¨λ“  AI λ¨λΈλ“¤μ νΈν™μ„±μ„ κ°μ„ ν•κ³  μ²΄ν¬ν¬μΈνΈ ν‚¤ λ§¤ν•‘μ„ μμ •ν•λ” λ„κµ¬

Author: MyCloset AI Team
Date: 2025-08-08
Version: 3.0
"""

import os
import sys
import json
import shutil
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

# PyTorch κ΄€λ ¨
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# SafeTensors κ΄€λ ¨
try:
    import safetensors
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

logger = logging.getLogger(__name__)

class FinalAIModelCompatibilityFixer:
    """μµμΆ… AI λ¨λΈ νΈν™μ„± κ°μ„  λ„κµ¬"""
    
    def __init__(self):
        self.fixed_models = []
        self.failed_models = []
        self.compatibility_issues = []
        
        # Stepλ³„ νΈν™μ„± λ§¤ν•‘ κ·μΉ™
        self.compatibility_mappings = {
            'step_01': {
                'graphonomy': {
                    'old_keys': ['backbone', 'decoder', 'classifier'],
                    'new_keys': ['hrnet_backbone', 'hrnet_decoder', 'hrnet_classifier'],
                    'architecture': 'hrnet'
                },
                'u2net': {
                    'old_keys': ['stage1', 'stage2', 'stage3', 'stage4'],
                    'new_keys': ['u2net_stage1', 'u2net_stage2', 'u2net_stage3', 'u2net_stage4'],
                    'architecture': 'u2net'
                },
                'deeplabv3plus': {
                    'old_keys': ['backbone', 'decoder', 'classifier'],
                    'new_keys': ['deeplab_backbone', 'deeplab_decoder', 'deeplab_classifier'],
                    'architecture': 'deeplabv3plus'
                }
            },
            'step_02': {
                'hrnet': {
                    'old_keys': ['hrnet', 'stage', 'transition'],
                    'new_keys': ['pose_hrnet', 'pose_stage', 'pose_transition'],
                    'architecture': 'hrnet'
                },
                'openpose': {
                    'old_keys': ['pose', 'body', 'hand'],
                    'new_keys': ['openpose_pose', 'openpose_body', 'openpose_hand'],
                    'architecture': 'openpose'
                }
            },
            'step_03': {
                'sam': {
                    'old_keys': ['image_encoder', 'prompt_encoder', 'mask_decoder'],
                    'new_keys': ['sam_image_encoder', 'sam_prompt_encoder', 'sam_mask_decoder'],
                    'architecture': 'sam'
                },
                'u2net': {
                    'old_keys': ['stage1', 'stage2', 'stage3', 'stage4'],
                    'new_keys': ['seg_u2net_stage1', 'seg_u2net_stage2', 'seg_u2net_stage3', 'seg_u2net_stage4'],
                    'architecture': 'u2net'
                }
            },
            'step_04': {
                'gmm': {
                    'old_keys': ['feature_extraction', 'regression', 'pretrained.model'],
                    'new_keys': ['gmm_feature_extraction', 'gmm_regression', 'gmm_backbone'],
                    'architecture': 'gmm'
                },
                'tps': {
                    'old_keys': ['localization_net', 'grid_generator', 'control_points'],
                    'new_keys': ['tps_localization_net', 'tps_grid_generator', 'tps_control_points'],
                    'architecture': 'tps'
                },
                'raft': {
                    'old_keys': ['feature_encoder', 'context_encoder', 'flow_head'],
                    'new_keys': ['raft_feature_encoder', 'raft_context_encoder', 'raft_flow_head'],
                    'architecture': 'raft'
                }
            },
            'step_05': {
                'tom': {
                    'old_keys': ['feature_extraction', 'regression'],
                    'new_keys': ['tom_feature_extraction', 'tom_regression'],
                    'architecture': 'tom'
                },
                'viton_hd': {
                    'old_keys': ['warping', 'generator'],
                    'new_keys': ['viton_warping', 'viton_generator'],
                    'architecture': 'viton_hd'
                },
                'dpt': {
                    'old_keys': ['dpt', 'depth'],
                    'new_keys': ['dpt_depth', 'dpt_backbone'],
                    'architecture': 'dpt'
                }
            },
            'step_06': {
                'stable_diffusion': {
                    'old_keys': ['unet', 'vae', 'text_encoder'],
                    'new_keys': ['sd_unet', 'sd_vae', 'sd_text_encoder'],
                    'architecture': 'stable_diffusion'
                },
                'ootd': {
                    'old_keys': ['unet_vton', 'unet_garm', 'vae'],
                    'new_keys': ['ootd_unet_vton', 'ootd_unet_garm', 'ootd_vae'],
                    'architecture': 'ootd'
                }
            },
            'step_07': {
                'real_esrgan': {
                    'old_keys': ['body', 'upsampling'],
                    'new_keys': ['esrgan_body', 'esrgan_upsampling'],
                    'architecture': 'real_esrgan'
                },
                'swinir': {
                    'old_keys': ['layers', 'patch_embed', 'norm'],
                    'new_keys': ['swinir_layers', 'swinir_patch_embed', 'swinir_norm'],
                    'architecture': 'swinir'
                }
            },
            'step_08': {
                'clip': {
                    'old_keys': ['visual', 'transformer', 'text_projection'],
                    'new_keys': ['clip_visual', 'clip_transformer', 'clip_text_projection'],
                    'architecture': 'clip'
                },
                'lpips': {
                    'old_keys': ['alex', 'net'],
                    'new_keys': ['lpips_alex', 'lpips_net'],
                    'architecture': 'lpips'
                }
            }
        }
    
    def find_all_ai_models(self) -> List[str]:
        """λ¨λ“  AI λ¨λΈ νμΌ μ°ΎκΈ°"""
        model_files = []
        
        # λ‹¤μ–‘ν• ν™•μ¥μ κ²€μƒ‰
        extensions = ["*.pth", "*.pt", "*.safetensors", "*.ckpt", "*.bin"]
        
        for ext in extensions:
            files = list(Path(".").rglob(ext))
            model_files.extend([str(f) for f in files])
        
        # μ¤‘λ³µ μ κ±° λ° μ •λ ¬
        model_files = sorted(list(set(model_files)))
        
        # λ¶ν•„μ”ν• νμΌ ν•„ν„°λ§
        exclude_patterns = [
            'distutils-precedence.pth',
            '__pycache__',
            '.git',
            'node_modules',
            'venv',
            'env',
            '.conda',
            '.backup',
            '_temp',
            '_old'
        ]
        
        filtered_files = []
        for file_path in model_files:
            if not any(pattern in file_path for pattern in exclude_patterns):
                filtered_files.append(file_path)
        
        return filtered_files
    
    def categorize_model(self, model_path: str) -> Tuple[str, str]:
        """λ¨λΈμ„ Stepλ³„λ΅ λ¶„λ¥"""
        model_path_lower = model_path.lower()
        
        # Stepλ³„ ν‚¤μ›λ“ λ§¤ν•‘
        step_keywords = {
            'step_01': ['human_parsing', 'graphonomy', 'u2net', 'deeplab'],
            'step_02': ['pose_estimation', 'hrnet', 'openpose', 'yolo'],
            'step_03': ['cloth_segmentation', 'sam', 'segmentation'],
            'step_04': ['geometric_matching', 'gmm', 'tps', 'raft'],
            'step_05': ['cloth_warping', 'tom', 'viton', 'dpt'],
            'step_06': ['virtual_fitting', 'stable_diffusion', 'ootd', 'diffusion'],
            'step_07': ['post_processing', 'real_esrgan', 'swinir', 'gfpgan'],
            'step_08': ['quality_assessment', 'clip', 'lpips', 'alex']
        }
        
        for step_key, keywords in step_keywords.items():
            for keyword in keywords:
                if keyword in model_path_lower:
                    return step_key, keyword
        
        return "unknown", "unknown"
    
    def detect_model_architecture(self, state_dict_keys: List[str]) -> str:
        """λ¨λΈ μ•„ν‚¤ν…μ² κ°μ§€"""
        architecture_keywords = {
            'graphonomy': ['backbone', 'decoder', 'classifier', 'hrnet'],
            'u2net': ['stage1', 'stage2', 'stage3', 'stage4', 'u2net'],
            'deeplabv3plus': ['backbone', 'decoder', 'classifier', 'aspp', 'deeplab'],
            'gmm': ['feature_extraction', 'regression', 'gmm', 'geometric'],
            'tps': ['localization_net', 'grid_generator', 'tps', 'transformation'],
            'raft': ['feature_encoder', 'context_encoder', 'flow_head', 'raft'],
            'sam': ['image_encoder', 'prompt_encoder', 'mask_decoder', 'sam'],
            'stable_diffusion': ['unet', 'vae', 'text_encoder', 'diffusion'],
            'ootd': ['unet_vton', 'unet_garm', 'vae', 'ootd'],
            'real_esrgan': ['body', 'upsampling', 'esrgan'],
            'swinir': ['layers', 'patch_embed', 'norm', 'swin'],
            'clip': ['visual', 'transformer', 'text_projection', 'clip'],
            'hrnet': ['hrnet', 'stage', 'transition'],
            'openpose': ['pose', 'body', 'hand', 'face'],
            'tom': ['feature_extraction', 'regression', 'tom'],
            'viton_hd': ['viton', 'vton', 'warping'],
            'dpt': ['dpt', 'depth', 'midas']
        }
        
        for arch_name, keywords in architecture_keywords.items():
            matches = sum(1 for keyword in keywords if any(keyword.lower() in key.lower() for key in state_dict_keys))
            if matches > 0:
                return arch_name
        
        return "unknown"
    
    def fix_model_compatibility(self, model_path: str) -> bool:
        """λ¨λΈ νΈν™μ„± μμ •"""
        try:
            print(f"\nπ”§ νΈν™μ„± μμ • μ‹λ„: {Path(model_path).name}")
            
            # λ°±μ—… μƒμ„±
            backup_path = f"{model_path}.backup"
            if not Path(backup_path).exists():
                shutil.copy2(model_path, backup_path)
                print(f"   π“¦ λ°±μ—… μƒμ„±: {backup_path}")
            
            # λ¨λΈ λ΅λ”©
            model_data = None
            
            # λ‹¤μ–‘ν• λ΅λ”© λ°©λ²• μ‹λ„
            for method in ['weights_only_true', 'weights_only_false', 'safetensors']:
                try:
                    if method == 'weights_only_true':
                        model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    elif method == 'weights_only_false':
                        model_data = torch.load(model_path, map_location='cpu', weights_only=False)
                    elif method == 'safetensors' and SAFETENSORS_AVAILABLE:
                        with safetensors.safe_open(model_path, framework="pt", device="cpu") as f:
                            keys = list(f.keys())
                            model_data = {key: f.get_tensor(key) for key in keys}
                    
                    if model_data is not None:
                        print(f"   β… {method}λ΅ λ΅λ”© μ„±κ³µ")
                        break
                except Exception as e:
                    print(f"   β {method} λ΅λ”© μ‹¤ν¨: {e}")
                    continue
            
            if model_data is None:
                print(f"   β λ¨λ“  λ΅λ”© λ°©λ²• μ‹¤ν¨")
                return False
            
            # Step λ¶„λ¥
            step_category, model_type = self.categorize_model(model_path)
            print(f"   π― Step λ¶„λ¥: {step_category} ({model_type})")
            
            # state_dict μ¶”μ¶
            state_dict = None
            if isinstance(model_data, dict):
                if 'state_dict' in model_data:
                    state_dict = model_data['state_dict']
                else:
                    state_dict = model_data
            else:
                print(f"   β state_dictλ¥Ό μ°Ύμ„ μ μ—†μ")
                return False
            
            # μ•„ν‚¤ν…μ² κ°μ§€
            architecture = self.detect_model_architecture(list(state_dict.keys()))
            print(f"   π—οΈ μ•„ν‚¤ν…μ²: {architecture}")
            
            # νΈν™μ„± λ§¤ν•‘ μ μ©
            fixed_state_dict = self._apply_compatibility_mapping(
                state_dict, step_category, architecture
            )
            
            # μμ •λ λ¨λΈ μ €μ¥
            if isinstance(model_data, dict) and 'state_dict' in model_data:
                model_data['state_dict'] = fixed_state_dict
            else:
                model_data = {'state_dict': fixed_state_dict}
            
            torch.save(model_data, model_path)
            print(f"   β… νΈν™μ„± μμ • μ™„λ£")
            
            return True
            
        except Exception as e:
            print(f"   β νΈν™μ„± μμ • μ‹¤ν¨: {e}")
            return False
    
    def _apply_compatibility_mapping(self, state_dict: Dict[str, torch.Tensor], 
                                   step_category: str, architecture: str) -> Dict[str, torch.Tensor]:
        """νΈν™μ„± λ§¤ν•‘ μ μ©"""
        fixed_state_dict = {}
        
        # Stepλ³„ λ§¤ν•‘ κ·μΉ™ μ μ©
        if step_category in self.compatibility_mappings:
            step_mappings = self.compatibility_mappings[step_category]
            
            for model_name, mapping_info in step_mappings.items():
                if model_name.lower() in architecture.lower():
                    old_keys = mapping_info['old_keys']
                    new_keys = mapping_info['new_keys']
                    
                    # ν‚¤ λ§¤ν•‘ μ μ©
                    for old_key, new_key in zip(old_keys, new_keys):
                        for state_key in state_dict.keys():
                            if old_key.lower() in state_key.lower():
                                new_state_key = state_key.replace(old_key, new_key)
                                fixed_state_dict[new_state_key] = state_dict[state_key]
                                print(f"   π”„ ν‚¤ λ§¤ν•‘: {state_key} β†’ {new_state_key}")
                                break
        
        # λ§¤ν•‘λμ§€ μ•μ€ ν‚¤λ“¤μ€ κ·Έλ€λ΅ μ μ§€
        for key, tensor in state_dict.items():
            if key not in fixed_state_dict:
                fixed_state_dict[key] = tensor
        
        return fixed_state_dict
    
    def fix_tps_compatibility_issues(self):
        """TPS νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •"""
        print("\nπ”§ TPS νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •")
        
        # TPS κ΄€λ ¨ λ¨λΈλ“¤ μ°ΎκΈ°
        tps_models = [
            "backend/ai_models/step_04_geometric_matching/tps_network.pth",
            "backend/ai_models/step_05_cloth_warping/tps_transformation.pth"
        ]
        
        for model_path in tps_models:
            if Path(model_path).exists():
                print(f"\nπ”§ TPS λ¨λΈ μμ •: {Path(model_path).name}")
                
                try:
                    # λ°±μ—… μƒμ„±
                    backup_path = f"{model_path}.backup"
                    if not Path(backup_path).exists():
                        shutil.copy2(model_path, backup_path)
                    
                    # λ¨λΈ λ΅λ”©
                    model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    
                    if isinstance(model_data, dict):
                        state_dict = model_data.get('state_dict', model_data)
                    else:
                        state_dict = model_data
                    
                    # TPS ν‚¤ λ§¤ν•‘ μμ •
                    fixed_state_dict = {}
                    for key, tensor in state_dict.items():
                        new_key = key
                        
                        # TPS κ΄€λ ¨ ν‚¤ λ§¤ν•‘
                        if 'control_points' in key:
                            new_key = key.replace('control_points', 'tps_control_points')
                        elif 'weights' in key:
                            new_key = key.replace('weights', 'tps_weights')
                        elif 'affine_params' in key:
                            new_key = key.replace('affine_params', 'tps_affine_params')
                        
                        fixed_state_dict[new_key] = tensor
                    
                    # μμ •λ λ¨λΈ μ €μ¥
                    if isinstance(model_data, dict) and 'state_dict' in model_data:
                        model_data['state_dict'] = fixed_state_dict
                    else:
                        model_data = {'state_dict': fixed_state_dict}
                    
                    torch.save(model_data, model_path)
                    print(f"   β… TPS νΈν™μ„± μμ • μ™„λ£")
                    self.fixed_models.append(model_path)
                    
                except Exception as e:
                    print(f"   β TPS μμ • μ‹¤ν¨: {e}")
                    self.failed_models.append(model_path)
    
    def fix_gmm_compatibility_issues(self):
        """GMM νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •"""
        print("\nπ”§ GMM νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •")
        
        # GMM κ΄€λ ¨ λ¨λΈλ“¤ μ°ΎκΈ°
        gmm_models = [
            "backend/ai_models/step_04_geometric_matching/gmm_final.pth"
        ]
        
        for model_path in gmm_models:
            if Path(model_path).exists():
                print(f"\nπ”§ GMM λ¨λΈ μμ •: {Path(model_path).name}")
                
                try:
                    # λ°±μ—… μƒμ„±
                    backup_path = f"{model_path}.backup"
                    if not Path(backup_path).exists():
                        shutil.copy2(model_path, backup_path)
                    
                    # λ¨λΈ λ΅λ”©
                    model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                    
                    if isinstance(model_data, dict):
                        state_dict = model_data.get('state_dict', model_data)
                    else:
                        state_dict = model_data
                    
                    # GMM ν‚¤ λ§¤ν•‘ μμ •
                    fixed_state_dict = {}
                    for key, tensor in state_dict.items():
                        new_key = key
                        
                        # GMM κ΄€λ ¨ ν‚¤ λ§¤ν•‘
                        if 'pretrained.model' in key:
                            new_key = key.replace('pretrained.model', 'gmm_backbone')
                        elif 'feature_extraction' in key:
                            new_key = key.replace('feature_extraction', 'gmm_feature_extraction')
                        elif 'regression' in key:
                            new_key = key.replace('regression', 'gmm_regression')
                        
                        fixed_state_dict[new_key] = tensor
                    
                    # μμ •λ λ¨λΈ μ €μ¥
                    if isinstance(model_data, dict) and 'state_dict' in model_data:
                        model_data['state_dict'] = fixed_state_dict
                    else:
                        model_data = {'state_dict': fixed_state_dict}
                    
                    torch.save(model_data, model_path)
                    print(f"   β… GMM νΈν™μ„± μμ • μ™„λ£")
                    self.fixed_models.append(model_path)
                    
                except Exception as e:
                    print(f"   β GMM μμ • μ‹¤ν¨: {e}")
                    self.failed_models.append(model_path)
    
    def verify_compatibility(self):
        """νΈν™μ„± κ²€μ¦"""
        print(f"\nπ” νΈν™μ„± κ²€μ¦:")
        
        verified_count = 0
        for model_path in self.fixed_models:
            print(f"\nπ” κ²€μ¦ μ¤‘: {Path(model_path).name}")
            
            try:
                # λ¨λΈ λ΅λ”© ν…μ¤νΈ
                model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                
                if isinstance(model_data, dict):
                    state_dict = model_data.get('state_dict', model_data)
                else:
                    state_dict = model_data
                
                # ν‚¤ κ°μ ν™•μΈ
                key_count = len(state_dict.keys())
                print(f"   β… λ΅λ”© μ„±κ³µ (ν‚¤ μ: {key_count})")
                
                # TPS ν‚¤ ν™•μΈ
                tps_keys = [key for key in state_dict.keys() if 'tps_' in key]
                if tps_keys:
                    print(f"   π” TPS ν‚¤ λ°κ²¬: {len(tps_keys)}κ°")
                
                # GMM ν‚¤ ν™•μΈ
                gmm_keys = [key for key in state_dict.keys() if 'gmm_' in key]
                if gmm_keys:
                    print(f"   π” GMM ν‚¤ λ°κ²¬: {len(gmm_keys)}κ°")
                
                verified_count += 1
                
            except Exception as e:
                print(f"   β κ²€μ¦ μ‹¤ν¨: {e}")
        
        print(f"\nπ“ κ²€μ¦ κ²°κ³Ό: {verified_count}/{len(self.fixed_models)}κ° μ„±κ³µ")
    
    def generate_compatibility_report(self):
        """νΈν™μ„± λ¦¬ν¬νΈ μƒμ„±"""
        report = []
        report.append("π”¥ AI λ¨λΈ νΈν™μ„± κ°μ„  λ¦¬ν¬νΈ")
        report.append("=" * 80)
        report.append(f"π“… κ°μ„  μ‹κ°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        report.append(f"π“ νΈν™μ„± κ°μ„  κ²°κ³Ό:")
        report.append(f"   β… μ„±κ³µ: {len(self.fixed_models)}κ°")
        report.append(f"   β μ‹¤ν¨: {len(self.failed_models)}κ°")
        report.append("")
        
        if self.fixed_models:
            report.append("β… νΈν™μ„± κ°μ„  μ™„λ£λ λ¨λΈλ“¤:")
            for model_path in self.fixed_models:
                report.append(f"   - {Path(model_path).name}")
            report.append("")
        
        if self.failed_models:
            report.append("β νΈν™μ„± κ°μ„  μ‹¤ν¨ν• λ¨λΈλ“¤:")
            for model_path in self.failed_models:
                report.append(f"   - {Path(model_path).name}")
            report.append("")
        
        if self.compatibility_issues:
            report.append("β οΈ λ°κ²¬λ νΈν™μ„± λ¬Έμ λ“¤:")
            for issue in self.compatibility_issues:
                report.append(f"   - {issue}")
            report.append("")
        
        return "\n".join(report)

def main():
    """λ©”μΈ ν•¨μ"""
    print("π”¥ μµμΆ… AI λ¨λΈ νΈν™μ„± κ°μ„  λ„κµ¬")
    print("=" * 80)
    
    # νΈν™μ„± κ°μ„ κΈ° μ΄κΈ°ν™”
    fixer = FinalAIModelCompatibilityFixer()
    
    # 1. λ¨λ“  AI λ¨λΈ νμΌ μ°ΎκΈ°
    print("\nπ“‹ 1λ‹¨κ³„: λ¨λ“  AI λ¨λΈ νμΌ κ²€μƒ‰")
    model_files = fixer.find_all_ai_models()
    print(f"   λ°κ²¬λ AI λ¨λΈ: {len(model_files)}κ°")
    
    # 2. TPS νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •
    print("\nπ”§ 2λ‹¨κ³„: TPS νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •")
    fixer.fix_tps_compatibility_issues()
    
    # 3. GMM νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •
    print("\nπ”§ 3λ‹¨κ³„: GMM νΈν™μ„± λ¬Έμ  νΉλ³„ μμ •")
    fixer.fix_gmm_compatibility_issues()
    
    # 4. μΌλ°μ μΈ νΈν™μ„± μμ •
    print("\nπ”§ 4λ‹¨κ³„: μΌλ°μ μΈ νΈν™μ„± μμ •")
    for i, model_path in enumerate(model_files[:10], 1):  # μ²μ 10κ°λ§ μ²λ¦¬
        print(f"\nπ”§ μ²λ¦¬ μ¤‘... ({i}/{min(10, len(model_files))})")
        if fixer.fix_model_compatibility(model_path):
            fixer.fixed_models.append(model_path)
        else:
            fixer.failed_models.append(model_path)
    
    # 5. νΈν™μ„± κ²€μ¦
    print("\nπ” 5λ‹¨κ³„: νΈν™μ„± κ²€μ¦")
    fixer.verify_compatibility()
    
    # 6. νΈν™μ„± λ¦¬ν¬νΈ μƒμ„±
    print("\nπ“‹ 6λ‹¨κ³„: νΈν™μ„± λ¦¬ν¬νΈ μƒμ„±")
    report = fixer.generate_compatibility_report()
    print(report)
    
    # 7. λ¦¬ν¬νΈ μ €μ¥
    with open("ai_model_compatibility_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\nπ’Ύ νΈν™μ„± λ¦¬ν¬νΈ μ €μ¥: ai_model_compatibility_report.txt")
    print("\nπ‰ AI λ¨λΈ νΈν™μ„± κ°μ„  μ™„λ£!")

if __name__ == "__main__":
    main()
