#!/bin/bash

echo "ðŸ PyTorch Segmentation Fault í•´ê²° ì¤‘ (Conda í™˜ê²½)..."

# 1. í˜„ìž¬ í™˜ê²½ í™•ì¸
echo "ðŸ” 1. í˜„ìž¬ í™˜ê²½ í™•ì¸..."
echo "Conda í™˜ê²½: ${CONDA_DEFAULT_ENV:-'ì—†ìŒ'}"
echo "Python ë²„ì „: $(python --version 2>/dev/null || echo 'í™•ì¸ë¶ˆê°€')"

# 2. PyTorch ì™„ì „ ì œê±°
echo "ðŸ—‘ï¸ 2. ê¸°ì¡´ PyTorch ì™„ì „ ì œê±° ì¤‘..."
conda remove pytorch torchvision torchaudio pytorch-cuda -y --force 2>/dev/null || true
pip uninstall torch torchvision torchaudio -y 2>/dev/null || true

# Apple Silicon ê°ì§€
ARCH=$(uname -m)
if [[ "$ARCH" == "arm64" ]]; then
    echo "ðŸŽ Apple Silicon (M1/M2/M3) ê°ì§€ë¨"
    IS_APPLE_SILICON=true
else
    echo "ðŸ–¥ï¸ Intel/AMD ì‹œìŠ¤í…œ ê°ì§€ë¨"
    IS_APPLE_SILICON=false
fi

# 3. NumPy í˜¸í™˜ì„± ë¨¼ì € í•´ê²°
echo "ðŸ“¦ 3. NumPy í˜¸í™˜ì„± í•´ê²° ì¤‘..."
conda install numpy=1.24.3 -y --force-reinstall

# 4. ì•ˆì „í•œ PyTorch ì„¤ì¹˜
echo "ðŸ”¥ 4. ì•ˆì „í•œ PyTorch ì„¤ì¹˜ ì¤‘..."

if [[ "$IS_APPLE_SILICON" == true ]]; then
    # Apple Siliconìš© PyTorch (CPU ë²„ì „ - ì•ˆì •ì„± ìš°ì„ )
    echo "ðŸŽ Apple Siliconìš© CPU PyTorch ì„¤ì¹˜ ì¤‘..."
    conda install pytorch torchvision -c pytorch -y
    
    # MPS ê´€ë ¨ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë¬¸ì œ ë°œìƒì‹œ ë¹„í™œì„±í™”)
    export PYTORCH_ENABLE_MPS_FALLBACK=1
    export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
    
else
    # Intel/AMDìš© PyTorch
    echo "ðŸ–¥ï¸ CPU ë²„ì „ PyTorch ì„¤ì¹˜ ì¤‘..."
    conda install pytorch torchvision cpuonly -c pytorch -y
fi

# 5. ì¶”ê°€ ì•ˆì •ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "ðŸ“¦ 5. ì•ˆì •ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
conda install -c conda-forge \
    pillow=10.0.0 \
    scipy=1.10.1 \
    scikit-image=0.21.0 \
    -y

# 6. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo "âš™ï¸ 6. ì•ˆì •ì„± í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì¤‘..."
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
export VECLIB_MAXIMUM_THREADS=1
export NUMEXPR_NUM_THREADS=1

# í™˜ê²½ë³€ìˆ˜ë¥¼ ~/.bashrcì— ì¶”ê°€
cat >> ~/.bashrc << 'EOF'

# PyTorch ì•ˆì •ì„± ì„¤ì •
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
export VECLIB_MAXIMUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export PYTORCH_ENABLE_MPS_FALLBACK=1
EOF

# 7. ê°„ë‹¨í•œ PyTorch í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo "ðŸ§ª 7. ì•ˆì „í•œ PyTorch í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."

cat > test_pytorch_safe.py << 'EOF'
#!/usr/bin/env python3
"""
ì•ˆì „í•œ PyTorch í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Segmentation fault ì—†ì´ PyTorch í…ŒìŠ¤íŠ¸
"""

import sys
import os

# ì•ˆì „ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

def test_imports():
    """ì•ˆì „í•œ import í…ŒìŠ¤íŠ¸"""
    print("ðŸ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        import numpy as np
        print(f"âœ… NumPy: {np.__version__}")
    except Exception as e:
        print(f"âŒ NumPy ì‹¤íŒ¨: {e}")
        return False
    
    try:
        # PyTorchë¥¼ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ import
        print("ðŸ”¥ PyTorch import ì‹œë„ ì¤‘...")
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
        
        # ê¸°ë³¸ í…ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
        x = torch.tensor([1.0, 2.0, 3.0])
        print(f"âœ… í…ì„œ ìƒì„± ì„±ê³µ: {x}")
        
        # ë””ë°”ì´ìŠ¤ ì •ë³´
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("âœ… Apple MPS ì‚¬ìš© ê°€ëŠ¥ (í•˜ì§€ë§Œ CPU ì‚¬ìš© ê¶Œìž¥)")
            device = "cpu"  # ì•ˆì •ì„±ì„ ìœ„í•´ CPU ì‚¬ìš©
        elif torch.cuda.is_available():
            print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
            device = "cpu"  # ì•ˆì •ì„±ì„ ìœ„í•´ CPU ì‚¬ìš©
        else:
            print("âœ… CPU ëª¨ë“œ")
            device = "cpu"
            
        print(f"ðŸŽ¯ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")
        
        # ê°„ë‹¨í•œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        y = x * 2
        print(f"âœ… ì—°ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {y}")
        
        return True
        
    except Exception as e:
        print(f"âŒ PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_basic_model():
    """ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    try:
        import torch
        import torch.nn as nn
        
        print("ðŸ§  ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
        model = nn.Linear(3, 1)
        
        # í…ŒìŠ¤íŠ¸ ìž…ë ¥
        x = torch.tensor([[1.0, 2.0, 3.0]])
        
        # Forward pass
        with torch.no_grad():
            output = model(x)
            
        print(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {output}")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    print("ðŸ PyTorch ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹œìž‘")
    print("=" * 40)
    
    # ì‹œìŠ¤í…œ ì •ë³´
    print(f"ðŸ”§ Python: {sys.version}")
    print(f"ðŸ’» í”Œëž«í¼: {sys.platform}")
    
    # Import í…ŒìŠ¤íŠ¸
    if not test_imports():
        print("\nâŒ Import í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    if not test_basic_model():
        print("\nâŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    print("\nðŸŽ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    print("âœ… PyTorchê°€ ì•ˆì „í•˜ê²Œ ìž‘ë™í•©ë‹ˆë‹¤.")
    print("âœ… ì´ì œ MyCloset AI ë°±ì—”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
EOF

# 8. ì•ˆì „í•œ main.py ìƒì„± (PyTorch ì—†ì´ë„ ì‹¤í–‰ ê°€ëŠ¥)
echo "ðŸ”§ 8. ì•ˆì „í•œ main.py ìƒì„± ì¤‘..."

cat > app/main_safe.py << 'EOF'
"""
MyCloset AI Backend - ì•ˆì „í•œ ì‹¤í–‰ ë²„ì „
PyTorch segfault ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ì‹œìž‘ì 
"""

import sys
import os
import logging
from pathlib import Path

# ì•ˆì „ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from datetime import datetime
import platform

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="MyCloset AI Backend (Safe Mode)",
    description="ì•ˆì „ ëª¨ë“œë¡œ ì‹¤í–‰ë˜ëŠ” AI ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ",
    version="1.0.0-safe"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# PyTorch ì•ˆì „ ë¡œë“œ í•¨ìˆ˜
def safe_load_pytorch():
    """PyTorchë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
    try:
        import torch
        logger.info(f"âœ… PyTorch ë¡œë“œ ì„±ê³µ: {torch.__version__}")
        
        # ë””ë°”ì´ìŠ¤ í™•ì¸ (ì•ˆì „ëª¨ë“œ)
        device = "cpu"  # ì•ˆì •ì„±ì„ ìœ„í•´ CPU ê°•ì œ ì‚¬ìš©
        
        return {
            "available": True,
            "version": torch.__version__,
            "device": device,
            "mode": "safe_cpu"
        }
    except Exception as e:
        logger.warning(f"âš ï¸ PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {
            "available": False,
            "error": str(e),
            "device": "none",
            "mode": "no_pytorch"
        }

# ì‹œìž‘ì‹œ PyTorch ìƒíƒœ í™•ì¸
pytorch_status = safe_load_pytorch()

@app.get("/")
async def root():
    return {
        "message": "MyCloset AI Backend (Safe Mode) ðŸ›¡ï¸",
        "version": "1.0.0-safe",
        "status": "running",
        "environment": {
            "conda_env": os.getenv("CONDA_DEFAULT_ENV", "unknown"),
            "python_version": platform.python_version(),
            "platform": platform.system(),
            "architecture": platform.machine()
        },
        "pytorch": pytorch_status,
        "docs": "/docs",
        "health": "/api/health"
    }

@app.get("/api/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    env_vars = {
        "OMP_NUM_THREADS": os.getenv("OMP_NUM_THREADS"),
        "MKL_NUM_THREADS": os.getenv("MKL_NUM_THREADS"),
        "PYTORCH_ENABLE_MPS_FALLBACK": os.getenv("PYTORCH_ENABLE_MPS_FALLBACK")
    }
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "mode": "safe",
        "pytorch": pytorch_status,
        "environment_variables": env_vars,
        "system": {
            "platform": platform.platform(),
            "python": platform.python_version(),
            "conda_env": os.getenv("CONDA_DEFAULT_ENV")
        }
    }

@app.get("/api/test-pytorch")
async def test_pytorch():
    """PyTorch í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    if not pytorch_status["available"]:
        raise HTTPException(
            status_code=503, 
            detail="PyTorchë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        )
    
    try:
        import torch
        
        # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        x = torch.tensor([1.0, 2.0, 3.0])
        y = x * 2
        
        return {
            "status": "success",
            "test_tensor": x.tolist(),
            "result": y.tolist(),
            "device": pytorch_status["device"],
            "message": "PyTorchê°€ ì •ìƒì ìœ¼ë¡œ ìž‘ë™í•©ë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"
        )

@app.on_event("startup")
async def startup_event():
    logger.info("ðŸš€ MyCloset AI Backend (Safe Mode) ì‹œìž‘ë¨")
    logger.info(f"ðŸ Conda í™˜ê²½: {os.getenv('CONDA_DEFAULT_ENV', 'unknown')}")
    logger.info(f"ðŸ”¥ PyTorch: {'ì‚¬ìš© ê°€ëŠ¥' if pytorch_status['available'] else 'ì‚¬ìš© ë¶ˆê°€'}")

if __name__ == "__main__":
    import uvicorn
    print("ðŸ›¡ï¸ ì•ˆì „ ëª¨ë“œë¡œ ì„œë²„ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
EOF

# 9. ì•ˆì „í•œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo "ðŸ“œ 9. ì•ˆì „í•œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."

cat > run_safe_server.sh << 'EOF'
#!/bin/bash

echo "ðŸ›¡ï¸ MyCloset AI Backend - ì•ˆì „ ëª¨ë“œ ì‹¤í–‰"
echo "======================================"

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
export VECLIB_MAXIMUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export PYTORCH_ENABLE_MPS_FALLBACK=1

# Conda í™˜ê²½ í™•ì¸
if [[ "$CONDA_DEFAULT_ENV" == "" ]]; then
    echo "âŒ Conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "conda activate mycloset"
    exit 1
fi

echo "âœ… Conda í™˜ê²½: $CONDA_DEFAULT_ENV"

# PyTorch í…ŒìŠ¤íŠ¸
echo "ðŸ§ª PyTorch ì•ˆì „ì„± í…ŒìŠ¤íŠ¸ ì¤‘..."
python test_pytorch_safe.py

if [[ $? -eq 0 ]]; then
    echo "âœ… PyTorch í…ŒìŠ¤íŠ¸ ì„±ê³µ"
    echo ""
    echo "ðŸŒ ì•ˆì „ ëª¨ë“œ ì„œë²„ ì‹œìž‘ ì¤‘..."
    echo "ðŸ“± ì ‘ì†: http://localhost:8000"
    echo "ðŸ“š API ë¬¸ì„œ: http://localhost:8000/docs"
    echo "ðŸ§ª PyTorch í…ŒìŠ¤íŠ¸: http://localhost:8000/api/test-pytorch"
    echo ""
    
    # ì•ˆì „í•œ main.py ì‹¤í–‰
    uvicorn app.main_safe:app --reload --host 0.0.0.0 --port 8000
else
    echo "âŒ PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
    echo "ê¸°ë³¸ ì›¹ì„œë²„ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤..."
    
    # ìµœì†Œí•œì˜ ì„œë²„ ì‹¤í–‰
    python -c "
from fastapi import FastAPI
import uvicorn

app = FastAPI(title='MyCloset AI - ìµœì†Œ ì„œë²„')

@app.get('/')
def root():
    return {'message': 'MyCloset AI Backend - ìµœì†Œ ëª¨ë“œ', 'status': 'pytorch_disabled'}

uvicorn.run(app, host='0.0.0.0', port=8000)
"
fi
EOF

chmod +x run_safe_server.sh
chmod +x test_pytorch_safe.py

echo ""
echo "ðŸŽ‰ PyTorch Segfault í•´ê²° ì™„ë£Œ!"
echo ""
echo "ðŸš€ ì‹¤í–‰ ë°©ë²•:"
echo "   1. ./run_safe_server.sh      # ì•ˆì „í•œ ì‹¤í–‰"
echo "   2. python test_pytorch_safe.py  # PyTorch í…ŒìŠ¤íŠ¸ë§Œ"
echo ""
echo "ðŸ”§ ë¬¸ì œê°€ ì§€ì†ë˜ë©´:"
echo "   1. conda activate mycloset"
echo "   2. conda clean --all"
echo "   3. conda install pytorch=2.0.1 cpuonly -c pytorch -y"
echo "   4. ./run_safe_server.sh"
echo ""
echo "ðŸ“± ì‹¤í–‰ í›„: http://localhost:8000"