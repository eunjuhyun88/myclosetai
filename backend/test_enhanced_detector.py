#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ v7.0
================================================================

âœ… RealWorldModelDetector v7.0 ì™„ì „ í…ŒìŠ¤íŠ¸
âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ íƒì§€ ë° ê²€ì¦
âœ… PyTorch ëª¨ë¸ ì‹¤ì œ ë¡œë”© í…ŒìŠ¤íŠ¸
âœ… M3 Max ìµœì í™” ê²€ì¦
âœ… Stepë³„ ëª¨ë¸ ë§¤í•‘ í…ŒìŠ¤íŠ¸
âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

ì‚¬ìš©ë²•:
    python test_enhanced_detector.py
    python test_enhanced_detector.py --quick     # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    python test_enhanced_detector.py --detailed  # ìƒì„¸ ë¶„ì„
    python test_enhanced_detector.py --step VirtualFittingStep  # íŠ¹ì • Stepë§Œ
"""

import os
import sys
import time
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).resolve().parent
backend_dir = current_dir
while backend_dir.name != 'backend' and backend_dir.parent != backend_dir:
    backend_dir = backend_dir.parent

if backend_dir.name == 'backend':
    sys.path.insert(0, str(backend_dir))
    print(f"âœ… í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€: {backend_dir}")
else:
    print(f"âš ï¸ backend ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_dir}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(f'test_detector_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)

def test_imports():
    """í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ” Import í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    import_results = {
        "torch": False,
        "numpy": False,
        "PIL": False,
        "cv2": False,
        "auto_model_detector": False
    }
    
    # PyTorch í…ŒìŠ¤íŠ¸
    try:
        import torch
        import_results["torch"] = True
        logger.info(f"âœ… PyTorch {torch.__version__} ë¡œë“œ ì„±ê³µ")
        
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            logger.info("ğŸ M3 Max MPS ì§€ì› í™•ì¸ë¨")
        elif torch.cuda.is_available():
            logger.info("ğŸ® CUDA GPU ì§€ì› í™•ì¸ë¨")
        else:
            logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ë™ì‘")
            
    except ImportError as e:
        logger.error(f"âŒ PyTorch import ì‹¤íŒ¨: {e}")
    
    # NumPy í…ŒìŠ¤íŠ¸
    try:
        import numpy as np
        import_results["numpy"] = True
        logger.info(f"âœ… NumPy {np.__version__} ë¡œë“œ ì„±ê³µ")
    except ImportError as e:
        logger.error(f"âŒ NumPy import ì‹¤íŒ¨: {e}")
    
    # PIL í…ŒìŠ¤íŠ¸
    try:
        from PIL import Image
        import_results["PIL"] = True
        logger.info("âœ… PIL ë¡œë“œ ì„±ê³µ")
    except ImportError as e:
        logger.error(f"âŒ PIL import ì‹¤íŒ¨: {e}")
    
    # OpenCV í…ŒìŠ¤íŠ¸
    try:
        import cv2
        import_results["cv2"] = True
        logger.info(f"âœ… OpenCV {cv2.__version__} ë¡œë“œ ì„±ê³µ")
    except ImportError as e:
        logger.error(f"âŒ OpenCV import ì‹¤íŒ¨: {e}")
    
    # ìë™ ëª¨ë¸ íƒì§€ê¸° í…ŒìŠ¤íŠ¸
    try:
        from app.ai_pipeline.utils.auto_model_detector import (
            RealWorldModelDetector,
            create_real_world_detector,
            DetectedModel,
            ModelCategory,
            ModelPriority
        )
        import_results["auto_model_detector"] = True
        logger.info("âœ… ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° import ì„±ê³µ")
    except ImportError as e:
        logger.error(f"âŒ ìë™ ëª¨ë¸ íƒì§€ê¸° import ì‹¤íŒ¨: {e}")
        logger.error("ğŸ’¡ í•´ê²°ë°©ë²•: auto_model_detector.py íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        return False
    
    return all(import_results.values())

def test_detector_creation():
    """íƒì§€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ”§ íƒì§€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸...")
    
    try:
        from app.ai_pipeline.utils.auto_model_detector import create_real_world_detector
        
        # ê¸°ë³¸ íƒì§€ê¸° ìƒì„±
        detector = create_real_world_detector()
        logger.info("âœ… ê¸°ë³¸ íƒì§€ê¸° ìƒì„± ì„±ê³µ")
        
        # ê³ ê¸‰ íƒì§€ê¸° ìƒì„± (ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”)
        enhanced_detector = create_real_world_detector(
            enable_deep_scan=True,
            enable_pytorch_validation=True,
            enable_performance_profiling=True,
            enable_memory_monitoring=True,
            enable_caching=True,
            max_workers=2,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì œí•œ
            scan_timeout=300
        )
        logger.info("âœ… ê°•í™”ëœ íƒì§€ê¸° ìƒì„± ì„±ê³µ")
        
        # íƒì§€ê¸° ì†ì„± í™•ì¸
        logger.info(f"   - ê²€ìƒ‰ ê²½ë¡œ: {len(enhanced_detector.search_paths)}ê°œ")
        logger.info(f"   - ë”¥ìŠ¤ìº”: {enhanced_detector.enable_deep_scan}")
        logger.info(f"   - PyTorch ê²€ì¦: {enhanced_detector.enable_pytorch_validation}")
        logger.info(f"   - ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§: {enhanced_detector.enable_performance_profiling}")
        logger.info(f"   - ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§: {enhanced_detector.enable_memory_monitoring}")
        
        return enhanced_detector
        
    except Exception as e:
        logger.error(f"âŒ íƒì§€ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def test_model_detection(detector, quick_mode: bool = False):
    """ì‹¤ì œ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ” ì‹¤ì œ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        start_time = time.time()
        
        # íƒì§€ ì‹¤í–‰
        detected_models = detector.detect_all_models(
            force_rescan=True,  # ê°•ì œ ì¬ìŠ¤ìº”
            min_confidence=0.3,
            enable_detailed_analysis=not quick_mode,
            max_models_per_category=10 if quick_mode else None
        )
        
        detection_time = time.time() - start_time
        
        logger.info(f"âœ… ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(detected_models)}ê°œ ë°œê²¬ ({detection_time:.2f}ì´ˆ)")
        
        # íƒì§€ í†µê³„ ì¶œë ¥
        stats = detector.scan_stats
        logger.info("ğŸ“Š íƒì§€ í†µê³„:")
        logger.info(f"   - ìŠ¤ìº”ëœ íŒŒì¼: {stats['total_files_scanned']:,}ê°œ")
        logger.info(f"   - PyTorch íŒŒì¼: {stats['pytorch_files_found']:,}ê°œ")
        logger.info(f"   - ê²€ì¦ëœ ëª¨ë¸: {stats['valid_pytorch_models']:,}ê°œ")
        logger.info(f"   - ì˜¤ë¥˜ ë°œìƒ: {stats['errors_encountered']:,}ê°œ")
        
        # ë°œê²¬ëœ ëª¨ë¸ë“¤ ìƒì„¸ ì •ë³´
        if detected_models:
            logger.info("\nğŸ¯ ë°œê²¬ëœ ëª¨ë¸ë“¤:")
            
            for i, (name, model) in enumerate(detected_models.items(), 1):
                logger.info(f"\n{i}. {name}")
                logger.info(f"   ğŸ“ ê²½ë¡œ: {model.path}")
                logger.info(f"   ğŸ“¦ í¬ê¸°: {model.file_size_mb:.1f}MB ({model.file_size_mb/1024:.2f}GB)")
                logger.info(f"   ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {model.category.value}")
                logger.info(f"   ğŸ¯ Step: {model.step_name}")
                logger.info(f"   â­ ì‹ ë¢°ë„: {model.confidence_score:.2f}")
                logger.info(f"   ğŸ”§ PyTorch ê²€ì¦: {'âœ…' if model.pytorch_valid else 'âŒ'}")
                
                if hasattr(model, 'parameter_count') and model.parameter_count > 0:
                    logger.info(f"   ğŸ“Š íŒŒë¼ë¯¸í„°: {model.parameter_count:,}ê°œ")
                
                if hasattr(model, 'architecture'):
                    logger.info(f"   ğŸ—ï¸ ì•„í‚¤í…ì²˜: {model.architecture.value}")
                
                if hasattr(model, 'device_compatibility'):
                    compatible_devices = [k for k, v in model.device_compatibility.items() if v]
                    logger.info(f"   ğŸ’» í˜¸í™˜ ë””ë°”ì´ìŠ¤: {', '.join(compatible_devices)}")
        else:
            logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            logger.info("ğŸ’¡ í•´ê²°ë°©ë²•:")
            logger.info("   1. ai_models ë””ë ‰í† ë¦¬ì— ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸")
            logger.info("   2. íŒŒì¼ í™•ì¥ìê°€ .pth, .pt, .bin, .safetensorsì¸ì§€ í™•ì¸")
            logger.info("   3. íŒŒì¼ í¬ê¸°ê°€ ìµœì†Œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸")
        
        return detected_models
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        import traceback
        logger.debug(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return {}

def test_step_specific_detection(detector, step_name: str):
    """íŠ¹ì • Stepë³„ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸"""
    logger.info(f"ğŸ¯ {step_name} ì „ìš© ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸...")
    
    try:
        # ì „ì²´ ëª¨ë¸ íƒì§€ (ì´ë¯¸ ì‹¤í–‰ëœ ê²½ìš° ìºì‹œ ì‚¬ìš©)
        all_models = detector.detected_models
        if not all_models:
            all_models = detector.detect_all_models()
        
        # Stepë³„ ëª¨ë¸ í•„í„°ë§
        step_models = detector.get_models_by_step(step_name)
        
        logger.info(f"âœ… {step_name}ìš© ëª¨ë¸: {len(step_models)}ê°œ ë°œê²¬")
        
        if step_models:
            for i, model in enumerate(step_models, 1):
                logger.info(f"{i}. {model.name}")
                logger.info(f"   í¬ê¸°: {model.file_size_mb:.1f}MB")
                logger.info(f"   ì‹ ë¢°ë„: {model.confidence_score:.2f}")
                logger.info(f"   ìš°ì„ ìˆœìœ„: {model.priority.name}")
            
            # ìµœì  ëª¨ë¸ ì„ íƒ
            best_model = detector.get_best_model_for_step(step_name)
            if best_model:
                logger.info(f"\nğŸ† {step_name} ìµœì  ëª¨ë¸: {best_model.name}")
                logger.info(f"   í¬ê¸°: {best_model.file_size_mb:.1f}MB")
                logger.info(f"   ì‹ ë¢°ë„: {best_model.confidence_score:.2f}")
                logger.info(f"   PyTorch ê²€ì¦: {'âœ…' if best_model.pytorch_valid else 'âŒ'}")
        else:
            logger.warning(f"âš ï¸ {step_name}ìš© ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return step_models
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return []

def test_pytorch_validation(detected_models):
    """PyTorch ëª¨ë¸ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ” PyTorch ëª¨ë¸ ê²€ì¦ í…ŒìŠ¤íŠ¸...")
    
    validated_models = []
    validation_failures = []
    
    for name, model in detected_models.items():
        if model.pytorch_valid:
            validated_models.append(model)
            logger.info(f"âœ… {name}: ê²€ì¦ ì„±ê³µ")
            
            # ìƒì„¸ ê²€ì¦ ì •ë³´
            if hasattr(model, 'validation_results') and model.validation_results:
                results = model.validation_results
                if 'parameter_count' in results:
                    logger.info(f"   íŒŒë¼ë¯¸í„°: {results['parameter_count']:,}ê°œ")
                if 'layer_types' in results:
                    layer_info = results['layer_types']
                    logger.info(f"   ë ˆì´ì–´ íƒ€ì…: {dict(list(layer_info.items())[:3])}")
        else:
            validation_failures.append(model)
            logger.warning(f"âŒ {name}: ê²€ì¦ ì‹¤íŒ¨")
    
    logger.info(f"\nğŸ“Š PyTorch ê²€ì¦ ê²°ê³¼:")
    logger.info(f"   âœ… ì„±ê³µ: {len(validated_models)}ê°œ")
    logger.info(f"   âŒ ì‹¤íŒ¨: {len(validation_failures)}ê°œ")
    logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ : {len(validated_models)/len(detected_models)*100:.1f}%")
    
    return validated_models

def test_performance_metrics(detector):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸...")
    
    try:
        stats = detector.scan_stats
        device_info = detector.device_info
        
        logger.info("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´:")
        logger.info(f"   ë””ë°”ì´ìŠ¤: {device_info.get('type', 'unknown')}")
        logger.info(f"   M3 Max: {'âœ…' if device_info.get('is_m3_max', False) else 'âŒ'}")
        logger.info(f"   ë©”ëª¨ë¦¬: {device_info.get('memory_total_gb', 0):.1f}GB ì´ / {device_info.get('memory_available_gb', 0):.1f}GB ì‚¬ìš©ê°€ëŠ¥")
        logger.info(f"   CPU ì½”ì–´: {device_info.get('cpu_count', 0)}ê°œ")
        
        logger.info("\nâš¡ ì„±ëŠ¥ í†µê³„:")
        logger.info(f"   ìŠ¤ìº” ì‹œê°„: {stats.get('scan_duration', 0):.2f}ì´ˆ")
        logger.info(f"   ì²˜ë¦¬ëœ íŒŒì¼: {stats.get('total_files_scanned', 0):,}ê°œ")
        logger.info(f"   ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰: {stats.get('total_files_scanned', 0) / max(stats.get('scan_duration', 1), 0.1):.1f} íŒŒì¼/ì´ˆ")
        
        if stats.get('memory_usage_delta_gb'):
            logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage_delta_gb']:.2f}GB")
        
        # ìµœì í™” ì œì•ˆ
        if device_info.get('optimization_hints'):
            logger.info(f"\nğŸ’¡ ìµœì í™” ì œì•ˆ:")
            for hint in device_info['optimization_hints']:
                logger.info(f"   - {hint}")
        
    except Exception as e:
        logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_cache_functionality(detector):
    """ìºì‹œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ’¾ ìºì‹œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
    
    try:
        # ì²« ë²ˆì§¸ ìŠ¤ìº” (ìºì‹œ ìƒì„±)
        start_time = time.time()
        models1 = detector.detect_all_models(force_rescan=True)
        first_scan_time = time.time() - start_time
        
        # ë‘ ë²ˆì§¸ ìŠ¤ìº” (ìºì‹œ ì‚¬ìš©)
        start_time = time.time()
        models2 = detector.detect_all_models(force_rescan=False)
        second_scan_time = time.time() - start_time
        
        logger.info(f"âœ… ìºì‹œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
        logger.info(f"   ì²« ìŠ¤ìº” (ìºì‹œ ìƒì„±): {first_scan_time:.2f}ì´ˆ")
        logger.info(f"   ë‘˜ì§¸ ìŠ¤ìº” (ìºì‹œ ì‚¬ìš©): {second_scan_time:.2f}ì´ˆ")
        
        if second_scan_time > 0:
            speedup = first_scan_time / second_scan_time
            logger.info(f"   ì†ë„ í–¥ìƒ: {speedup:.1f}ë°°")
        
        # ìºì‹œ í†µê³„
        cache_hits = detector.scan_stats.get('cache_hits', 0)
        cache_misses = detector.scan_stats.get('cache_misses', 0)
        
        if cache_hits + cache_misses > 0:
            hit_rate = cache_hits / (cache_hits + cache_misses) * 100
            logger.info(f"   ìºì‹œ ì ì¤‘ë¥ : {hit_rate:.1f}%")
        
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def generate_test_report(detected_models, detector, test_results):
    """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
    logger.info("ğŸ“‹ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±...")
    
    try:
        report = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "detector_version": "7.0",
                "test_duration": test_results.get('total_duration', 0)
            },
            "system_info": detector.device_info,
            "scan_statistics": detector.scan_stats,
            "detected_models": []
        }
        
        # ëª¨ë¸ ì •ë³´ ì¶”ê°€
        for name, model in detected_models.items():
            model_info = {
                "name": name,
                "path": str(model.path),
                "category": model.category.value,
                "step_name": model.step_name,
                "file_size_mb": model.file_size_mb,
                "confidence_score": model.confidence_score,
                "pytorch_valid": model.pytorch_valid,
                "priority": model.priority.name
            }
            
            # í™•ì¥ ì •ë³´ (ìˆëŠ” ê²½ìš°ë§Œ)
            if hasattr(model, 'parameter_count'):
                model_info["parameter_count"] = model.parameter_count
            if hasattr(model, 'architecture'):
                model_info["architecture"] = model.architecture.value
            if hasattr(model, 'device_compatibility'):
                model_info["device_compatibility"] = model.device_compatibility
            
            report["detected_models"].append(model_info)
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        report_filename = f"detector_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_filename}")
        return report_filename
        
    except Exception as e:
        logger.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ')
    parser.add_argument('--detailed', action='store_true', help='ìƒì„¸ ë¶„ì„ ëª¨ë“œ')
    parser.add_argument('--step', type=str, help='íŠ¹ì • Stepë§Œ í…ŒìŠ¤íŠ¸ (ì˜ˆ: VirtualFittingStep)')
    parser.add_argument('--no-cache', action='store_true', help='ìºì‹œ í…ŒìŠ¤íŠ¸ ë¹„í™œì„±í™”')
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ MyCloset AI - ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)
    
    test_results = {
        "start_time": time.time(),
        "tests_passed": 0,
        "tests_failed": 0
    }
    
    # 1. Import í…ŒìŠ¤íŠ¸
    logger.info("\n1ï¸âƒ£ Import í…ŒìŠ¤íŠ¸")
    if test_imports():
        test_results["tests_passed"] += 1
        logger.info("âœ… Import í…ŒìŠ¤íŠ¸ í†µê³¼")
    else:
        test_results["tests_failed"] += 1
        logger.error("âŒ Import í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
        return
    
    # 2. íƒì§€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸
    logger.info("\n2ï¸âƒ£ íƒì§€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸")
    detector = test_detector_creation()
    if detector:
        test_results["tests_passed"] += 1
        logger.info("âœ… íƒì§€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸ í†µê³¼")
    else:
        test_results["tests_failed"] += 1
        logger.error("âŒ íƒì§€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
        return
    
    # 3. ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸
    logger.info("\n3ï¸âƒ£ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸")
    detected_models = test_model_detection(detector, quick_mode=args.quick)
    if detected_models is not None:
        test_results["tests_passed"] += 1
        logger.info("âœ… ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸ í†µê³¼")
    else:
        test_results["tests_failed"] += 1
        logger.error("âŒ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    # 4. Stepë³„ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
    if args.step:
        logger.info(f"\n4ï¸âƒ£ {args.step} ì „ìš© í…ŒìŠ¤íŠ¸")
        step_models = test_step_specific_detection(detector, args.step)
        if step_models is not None:
            test_results["tests_passed"] += 1
        else:
            test_results["tests_failed"] += 1
    
    # 5. PyTorch ê²€ì¦ í…ŒìŠ¤íŠ¸
    if detected_models and not args.quick:
        logger.info("\n5ï¸âƒ£ PyTorch ê²€ì¦ í…ŒìŠ¤íŠ¸")
        validated_models = test_pytorch_validation(detected_models)
        if validated_models is not None:
            test_results["tests_passed"] += 1
        else:
            test_results["tests_failed"] += 1
    
    # 6. ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸
    logger.info("\n6ï¸âƒ£ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸")
    test_performance_metrics(detector)
    test_results["tests_passed"] += 1
    
    # 7. ìºì‹œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
    if not args.no_cache and not args.quick:
        logger.info("\n7ï¸âƒ£ ìºì‹œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        test_cache_functionality(detector)
        test_results["tests_passed"] += 1
    
    # ìµœì¢… ê²°ê³¼
    test_results["total_duration"] = time.time() - test_results["start_time"]
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    logger.info(f"âœ… í†µê³¼: {test_results['tests_passed']}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {test_results['tests_failed']}ê°œ")
    logger.info(f"â±ï¸ ì´ ì‹œê°„: {test_results['total_duration']:.2f}ì´ˆ")
    
    # í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
    if detected_models:
        report_file = generate_test_report(detected_models, detector, test_results)
        if report_file:
            logger.info(f"ğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
    
    # ì¢…ë£Œ ì½”ë“œ
    exit_code = 0 if test_results["tests_failed"] == 0 else 1
    logger.info(f"ğŸ”š ì¢…ë£Œ ì½”ë“œ: {exit_code}")
    sys.exit(exit_code)

if __name__ == "__main__":
    main()