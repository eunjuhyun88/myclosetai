#!/usr/bin/env python3
"""
κ° μ¤ν…λ³„ λ¨λΈ λ΅λ”© μƒνƒ ν™•μΈ μ¤ν¬λ¦½νΈ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

import torch
from pathlib import Path
import logging

# λ΅κΉ… μ„¤μ • (κ°„λ‹¨ν•κ²)
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def check_model_files():
    """λ¨λΈ νμΌ μ΅΄μ¬ μ—¬λ¶€ ν™•μΈ"""
    print("π” λ¨λΈ νμΌ μ΅΄μ¬ μ—¬λ¶€ ν™•μΈ:")
    print("=" * 50)
    
    model_paths = [
        "ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth",
        "ai_models/U2Net/u2net.pth",
        "ai_models/DeepLabV3+/deeplabv3plus.pth",
        "ai_models/SAM/sam_vit_h_4b8939.pth",
        "ai_models/OpenPose/pose_iter_584000.caffemodel",
        "ai_models/YOLOv8/yolov8n-pose.pt",
        "ai_models/GMM/gmm_final.pth",
        "ai_models/RealVisXL/realvisxl_v4.0.safetensors"
    ]
    
    for path in model_paths:
        full_path = Path(path)
        if full_path.exists():
            size_mb = full_path.stat().st_size / (1024 * 1024)
            print(f"β… {path} ({size_mb:.1f}MB)")
        else:
            print(f"β {path} (μ—†μ)")

def check_step_models():
    """κ° μ¤ν…λ³„ λ¨λΈ λ΅λ”© μƒνƒ ν™•μΈ"""
    print("\nπ§  κ° μ¤ν…λ³„ λ¨λΈ λ΅λ”© μƒνƒ:")
    print("=" * 50)
    
    try:
        from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
        from app.ai_pipeline.steps.step_02_pose_estimation import PoseEstimationStep
        from app.ai_pipeline.steps.step_03_cloth_segmentation import ClothSegmentationStep
        from app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
        from app.ai_pipeline.steps.step_05_cloth_warping import ClothWarpingStep
        from app.ai_pipeline.steps.step_06_virtual_fitting import VirtualFittingStep
        from app.ai_pipeline.steps.step_07_post_processing import PostProcessingStep
        from app.ai_pipeline.steps.step_08_quality_assessment import QualityAssessmentStep
        
        steps = [
            ("Step 1: Human Parsing", HumanParsingStep),
            ("Step 2: Pose Estimation", PoseEstimationStep),
            ("Step 3: Cloth Segmentation", ClothSegmentationStep),
            ("Step 4: Geometric Matching", GeometricMatchingStep),
            ("Step 5: Cloth Warping", ClothWarpingStep),
            ("Step 6: Virtual Fitting", VirtualFittingStep),
            ("Step 7: Post Processing", PostProcessingStep),
            ("Step 8: Quality Assessment", QualityAssessmentStep)
        ]
        
        for step_name, step_class in steps:
            try:
                print(f"\nπ” {step_name} ν™•μΈ μ¤‘...")
                step = step_class()
                
                # λ¨λΈ λ΅λ”© μ‹λ„
                if hasattr(step, '_load_ai_models_via_central_hub'):
                    result = step._load_ai_models_via_central_hub()
                    if result:
                        print(f"β… {step_name}: λ¨λΈ λ΅λ”© μ„±κ³µ")
                    else:
                        print(f"β {step_name}: λ¨λΈ λ΅λ”© μ‹¤ν¨")
                else:
                    print(f"β οΈ {step_name}: _load_ai_models_via_central_hub λ©”μ„λ“ μ—†μ")
                    
            except Exception as e:
                print(f"β {step_name}: μ¤λ¥ λ°μƒ - {str(e)[:100]}...")
                
    except Exception as e:
        print(f"β μ¤ν… μ„ν¬νΈ μ‹¤ν¨: {e}")

def check_checkpoint_loading():
    """μ²΄ν¬ν¬μΈνΈ λ΅λ”© μƒνƒ ν™•μΈ"""
    print("\nπ“¦ μ²΄ν¬ν¬μΈνΈ λ΅λ”© μƒνƒ:")
    print("=" * 50)
    
    try:
        checkpoint_path = "ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth"
        if Path(checkpoint_path).exists():
            print(f"π”„ {checkpoint_path} λ΅λ”© μ¤‘...")
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            if isinstance(checkpoint, dict):
                print(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ„±κ³µ: {len(checkpoint)}κ° ν‚¤")
                
                # ν‚¤ κµ¬μ΅° ν™•μΈ
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    print(f"π“ state_dict ν‚¤ μ: {len(state_dict)}")
                    
                    # ν‚¤ μƒν” μ¶λ ¥
                    sample_keys = list(state_dict.keys())[:5]
                    print(f"π” ν‚¤ μƒν”: {sample_keys}")
                    
                    # λ¨λΈ κµ¬μ΅° ν™•μΈ
                    total_params = sum(p.numel() for p in state_dict.values() if hasattr(p, 'numel'))
                    print(f"π“ μ΄ νλΌλ―Έν„° μ: {total_params:,}")
                else:
                    print(f"π“ μ§μ ‘ ν‚¤λ“¤: {list(checkpoint.keys())[:5]}")
            else:
                print(f"β οΈ μ²΄ν¬ν¬μΈνΈκ°€ λ”•μ…”λ„λ¦¬κ°€ μ•„λ‹: {type(checkpoint)}")
        else:
            print(f"β μ²΄ν¬ν¬μΈνΈ νμΌ μ—†μ: {checkpoint_path}")
            
    except Exception as e:
        print(f"β μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹¤ν¨: {e}")

if __name__ == "__main__":
    print("π MyCloset AI λ¨λΈ λ΅λ”© μƒνƒ ν™•μΈ")
    print("=" * 60)
    
    check_model_files()
    check_checkpoint_loading()
    check_step_models()
    
    print("\nβ… λ¨λΈ λ΅λ”© μƒνƒ ν™•μΈ μ™„λ£!") 