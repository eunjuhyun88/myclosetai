{
  "test_start_time": 1753938495.310133,
  "initial_memory_mb": 43.15625,
  "step_results": {
    "HumanParsingStep": {
      "step_name": "HumanParsingStep",
      "step_id": 1,
      "description": "Human Body Parsing & Segmentation",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 26.52840781211853,
      "memory_usage_mb": 1247.203125,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "dict",
        "expected_type": "parsing_mask",
        "size_info": "20 keys",
        "content_summary": [
          "parsing_map",
          "detected_parts",
          "body_masks",
          "clothing_analysis",
          "confidence"
        ],
        "valid_output": true,
        "ai_features": [
          "body_masks",
          "quality_score",
          "quality_metrics"
        ]
      },
      "ai_model_info": "Graphonomy/DenseASPP (104MB)"
    },
    "PoseEstimationStep": {
      "step_name": "PoseEstimationStep",
      "step_id": 2,
      "description": "Human Pose Estimation",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 7.653771877288818,
      "memory_usage_mb": 443.734375,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "dict",
        "expected_type": "pose_keypoints",
        "size_info": "16 keys",
        "content_summary": [
          "success",
          "keypoints",
          "confidence_scores",
          "pose_quality",
          "joint_angles"
        ],
        "valid_output": true,
        "ai_features": [
          "keypoints",
          "confidence_scores",
          "pose_quality"
        ]
      },
      "ai_model_info": "OpenPose/MediaPipe (3.4GB)"
    },
    "ClothSegmentationStep": {
      "step_name": "ClothSegmentationStep",
      "step_id": 3,
      "description": "Clothing Segmentation",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 0.1363358497619629,
      "memory_usage_mb": 0.53125,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "dict",
        "expected_type": "cloth_mask",
        "size_info": "19 keys",
        "content_summary": [
          "cloth_mask",
          "segmented_clothing",
          "confidence",
          "method_used",
          "processing_time"
        ],
        "valid_output": true,
        "ai_features": [
          "cloth_mask",
          "segmented_clothing",
          "quality_score"
        ]
      },
      "ai_model_info": "U2Net/SAM/DeepLabV3+ Multi-model"
    },
    "GeometricMatchingStep": {
      "step_name": "GeometricMatchingStep",
      "step_id": 4,
      "description": "Geometric Matching & Alignment",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 1.7705562114715576,
      "memory_usage_mb": 3.109375,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "dict",
        "expected_type": "matched_points",
        "size_info": "14 keys",
        "content_summary": [
          "success",
          "transformation_matrix",
          "transformation_grid",
          "warped_clothing",
          "flow_field"
        ],
        "valid_output": true,
        "ai_features": [
          "warped_clothing",
          "quality_score",
          "matching_score"
        ]
      },
      "ai_model_info": "GMM/TPS Networks (1.3GB)"
    },
    "ClothWarpingStep": {
      "step_name": "ClothWarpingStep",
      "step_id": 5,
      "description": "Cloth Warping & Deformation",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 0.27221107482910156,
      "memory_usage_mb": 16.65625,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "dict",
        "expected_type": "warped_cloth",
        "size_info": "16 keys",
        "content_summary": [
          "warped_cloth",
          "warped_cloth_tensor",
          "ai_success",
          "enhanced_ai_inference",
          "confidence"
        ],
        "valid_output": true,
        "ai_features": [
          "warped_cloth",
          "warped_cloth_tensor",
          "quality_score"
        ]
      },
      "ai_model_info": "RealVisXL/Diffusion Models (7.0GB)"
    },
    "VirtualFittingStep": {
      "step_name": "VirtualFittingStep",
      "step_id": 6,
      "description": "ðŸ”¥ í•µì‹¬ Virtual Fitting",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 50.944234132766724,
      "memory_usage_mb": 155.5625,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "dict",
        "expected_type": "fitted_image",
        "size_info": "14 keys",
        "content_summary": [
          "fitted_image",
          "confidence",
          "method_used",
          "processing_time",
          "quality_score"
        ],
        "valid_output": true,
        "ai_features": [
          "fitted_image",
          "quality_score",
          "quality_metrics"
        ]
      },
      "ai_model_info": "OOTDiffusion (14GB) - UNetÃ—4 + VAE + TextEncoder"
    },
    "PostProcessingStep": {
      "step_name": "PostProcessingStep",
      "step_id": 7,
      "description": "Post-processing & Enhancement",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 0.06058979034423828,
      "memory_usage_mb": 0.03125,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "dict",
        "expected_type": "enhanced_image",
        "size_info": "11 keys",
        "content_summary": [
          "enhanced_image",
          "enhancement_quality",
          "enhancement_methods_used",
          "sr_enhancement",
          "face_enhancement"
        ],
        "valid_output": true,
        "ai_features": [
          "enhancement_quality"
        ]
      },
      "ai_model_info": "ESRGAN/SwinIR/RealESRGAN (1.5GB)"
    },
    "QualityAssessmentStep": {
      "step_name": "QualityAssessmentStep",
      "step_id": 8,
      "description": "Quality Assessment & Analysis",
      "import_success": true,
      "instance_created": true,
      "model_loaded": true,
      "inference_success": true,
      "output_generated": true,
      "execution_time": 0.08228182792663574,
      "memory_usage_mb": 0.015625,
      "errors": [],
      "warnings": [],
      "output_info": {
        "type": "coroutine",
        "expected_type": "quality_metrics",
        "size_info": "unknown",
        "content_summary": null,
        "valid_output": true,
        "ai_features": []
      },
      "ai_model_info": "CLIP/ViT Models (8.2GB)"
    }
  },
  "data_flow": {
    "step_1": {
      "input_from": [
        "from_step_01"
      ],
      "output_to": "step_2",
      "success": true
    },
    "step_2": {
      "input_from": [
        "from_step_01",
        "from_step_02"
      ],
      "output_to": "step_3",
      "success": true
    },
    "step_3": {
      "input_from": [
        "from_step_01",
        "from_step_02",
        "from_step_03"
      ],
      "output_to": "step_4",
      "success": true
    },
    "step_4": {
      "input_from": [
        "from_step_01",
        "from_step_02",
        "from_step_03",
        "from_step_04"
      ],
      "output_to": "step_5",
      "success": true
    },
    "step_5": {
      "input_from": [
        "from_step_01",
        "from_step_02",
        "from_step_03",
        "from_step_04",
        "from_step_05"
      ],
      "output_to": "step_6",
      "success": true
    },
    "step_6": {
      "input_from": [
        "from_step_01",
        "from_step_02",
        "from_step_03",
        "from_step_04",
        "from_step_05",
        "from_step_06"
      ],
      "output_to": "step_7",
      "success": true
    },
    "step_7": {
      "input_from": [
        "from_step_01",
        "from_step_02",
        "from_step_03",
        "from_step_04",
        "from_step_05",
        "from_step_06",
        "from_step_07"
      ],
      "output_to": "step_8",
      "success": true
    },
    "step_8": {
      "input_from": [
        "from_step_01",
        "from_step_02",
        "from_step_03",
        "from_step_04",
        "from_step_05",
        "from_step_06",
        "from_step_07",
        "from_step_08"
      ],
      "output_to": "final_result",
      "success": true
    }
  },
  "overall_performance": {
    "total_steps": 8,
    "successful_imports": 8,
    "successful_instances": 8,
    "successful_inferences": 8,
    "import_success_rate": 100.0,
    "instance_success_rate": 100.0,
    "inference_success_rate": 100.0,
    "total_execution_time": 87.44838857650757,
    "total_memory_usage_mb": 1866.84375,
    "final_memory_mb": 1384.375,
    "memory_increase_mb": 1341.21875,
    "ai_model_performance": {
      "HumanParsingStep": {
        "model": "Graphonomy/DenseASPP (104MB)",
        "success": true,
        "time": 26.52840781211853,
        "memory": 1247.203125
      },
      "PoseEstimationStep": {
        "model": "OpenPose/MediaPipe (3.4GB)",
        "success": true,
        "time": 7.653771877288818,
        "memory": 443.734375
      },
      "ClothSegmentationStep": {
        "model": "U2Net/SAM/DeepLabV3+ Multi-model",
        "success": true,
        "time": 0.1363358497619629,
        "memory": 0.53125
      },
      "GeometricMatchingStep": {
        "model": "GMM/TPS Networks (1.3GB)",
        "success": true,
        "time": 1.7705562114715576,
        "memory": 3.109375
      },
      "ClothWarpingStep": {
        "model": "RealVisXL/Diffusion Models (7.0GB)",
        "success": true,
        "time": 0.27221107482910156,
        "memory": 16.65625
      },
      "VirtualFittingStep": {
        "model": "OOTDiffusion (14GB) - UNetÃ—4 + VAE + TextEncoder",
        "success": true,
        "time": 50.944234132766724,
        "memory": 155.5625
      },
      "PostProcessingStep": {
        "model": "ESRGAN/SwinIR/RealESRGAN (1.5GB)",
        "success": true,
        "time": 0.06058979034423828,
        "memory": 0.03125
      },
      "QualityAssessmentStep": {
        "model": "CLIP/ViT Models (8.2GB)",
        "success": true,
        "time": 0.08228182792663574,
        "memory": 0.015625
      }
    },
    "average_step_time": 10.931048572063446,
    "average_memory_per_step": 233.35546875
  },
  "errors": [],
  "warnings": [],
  "pipeline_integrity": {
    "pipeline_complete": true,
    "successful_steps": 8,
    "failed_steps": 0,
    "data_flow_intact": true,
    "bottleneck_steps": [],
    "critical_failures": [],
    "success_rate": 100.0
  },
  "test_completed_at": 1753938583.630424,
  "total_test_time": 88.42160511016846
}