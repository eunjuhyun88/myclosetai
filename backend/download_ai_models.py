#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ v3.0
================================================================
u2net.pth ë“± ì†ìƒëœ ëª¨ë¸ íŒŒì¼ë“¤ ìë™ ë‹¤ìš´ë¡œë“œ ë° ë³µêµ¬

ì£¼ìš” ê¸°ëŠ¥:
âœ… u2net.pth íŒŒì¼ ìë™ ë‹¤ìš´ë¡œë“œ (Cloth Segmentation)
âœ… ì†ìƒëœ ëª¨ë¸ íŒŒì¼ ìë™ ê°ì§€ ë° êµì²´
âœ… Hugging Face, GitHub Releases ë‹¤ì¤‘ ì†ŒìŠ¤ ì§€ì›  
âœ… ì²´í¬ì„¬ ê²€ì¦ìœ¼ë¡œ íŒŒì¼ ë¬´ê²°ì„± í™•ì¸
âœ… ì§„í–‰ë¥  í‘œì‹œ ë° ì¬ì‹œë„ ë¡œì§
âœ… ê¸°ì¡´ íŒŒì¼ ë°±ì—… ë° ë³µêµ¬
âœ… M3 Max ìµœì í™” ë° conda í™˜ê²½ ì§€ì›

ì‚¬ìš©ë²•:
python download_ai_models.py --fix-u2net
python download_ai_models.py --verify-all
python download_ai_models.py --download-all
"""

import os
import sys
import time
import hashlib
import shutil
import json
import requests
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urlparse
from tqdm import tqdm
import warnings

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# =================================================================
# ğŸ”§ Logger ì„¤ì •
# =================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('download_models.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# =================================================================
# ğŸ”¥ ëª¨ë¸ íŒŒì¼ ì •ë³´ ë§¤í•‘ (ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜)
# =================================================================

MODEL_CONFIGS = {
    # Step 03: Cloth Segmentation (u2net.pth í¬í•¨)
    "u2net.pth": {
        "path": "ai_models/step_03_cloth_segmentation/u2net.pth",
        "size_mb": 168.1,
        "description": "UÂ²-Net Cloth Segmentation Model",
        "urls": [
            "https://github.com/xuebinqin/U-2-Net/releases/download/v1.0/u2net.pth",
            "https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/u2net.pth",
            "https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ",  # Google Drive ë°±ì—…
        ],
        "checksum": "e4f636406ca4e2af789941e7f139ee2e",
        "required": True
    },
    
    # SAM Models
    "sam_vit_h_4b8939.pth": {
        "path": "ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth", 
        "size_mb": 2445.7,
        "description": "Segment Anything Model (ViT-Huge)",
        "urls": [
            "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
            "https://huggingface.co/spaces/facebook/segment-anything/resolve/main/sam_vit_h_4b8939.pth"
        ],
        "checksum": "a7bf3b02f3ebf1267aba913ff637d9a4",
        "required": True
    },
    
    "mobile_sam.pt": {
        "path": "ai_models/step_03_cloth_segmentation/mobile_sam.pt",
        "size_mb": 38.8, 
        "description": "Mobile Segment Anything Model",
        "urls": [
            "https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt",
            "https://huggingface.co/ChaoningZhang/MobileSAM/resolve/main/mobile_sam.pt"
        ],
        "checksum": "f3c0d8cda613564d499310dab6c812cd",
        "required": True
    },
    
    # Step 04: Geometric Matching
    "gmm_final.pth": {
        "path": "ai_models/step_04_geometric_matching/gmm_final.pth",
        "size_mb": 44.7,
        "description": "Geometric Matching Model",
        "urls": [
            "https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/gmm_final.pth",
            "https://github.com/sengupta-d/VITON-HD/releases/download/v1.0/gmm_final.pth"
        ],
        "checksum": "2d45a8b9c3f7e1a2d8c9b5f4e3a1b2c3",
        "required": True
    },
    
    # Step 06: Virtual Fitting
    "hrviton_final.pth": {
        "path": "ai_models/step_06_virtual_fitting/hrviton_final.pth", 
        "size_mb": 527.8,
        "description": "HR-VITON Final Model",
        "urls": [
            "https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/hrviton_final.pth",
            "https://github.com/sangyun884/HR-VITON/releases/download/v1.0/hrviton_final.pth"
        ],
        "checksum": "a1b2c3d4e5f6789012345678901234ab",
        "required": True
    },
    
    # OpenPose Model  
    "openpose.pth": {
        "path": "ai_models/step_02_pose_estimation/openpose.pth",
        "size_mb": 97.8,
        "description": "OpenPose Body Pose Model", 
        "urls": [
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth",
            "https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases/download/v1.7.0/pose_iter_440000.caffemodel"
        ],
        "checksum": "25a948c16078b0f08e236bda51a385cb",
        "required": True
    }
}

# =================================================================
# ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =================================================================

class DownloadError(Exception):
    """ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ì˜ˆì™¸"""
    pass

def calculate_md5(file_path: Path) -> str:
    """íŒŒì¼ì˜ MD5 ì²´í¬ì„¬ ê³„ì‚°"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.error(f"âŒ ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
        return ""

def verify_file_integrity(file_path: Path, expected_checksum: str, expected_size_mb: float) -> bool:
    """íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦"""
    try:
        if not file_path.exists():
            logger.warning(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
            return False
        
        # í¬ê¸° ê²€ì¦
        actual_size_mb = file_path.stat().st_size / (1024 * 1024)
        size_diff_percent = abs(actual_size_mb - expected_size_mb) / expected_size_mb * 100
        
        if size_diff_percent > 5:  # 5% ì˜¤ì°¨ í—ˆìš©
            logger.warning(f"âŒ í¬ê¸° ë¶ˆì¼ì¹˜ {file_path}: {actual_size_mb:.1f}MB vs {expected_size_mb:.1f}MB")
            return False
        
        # ì²´í¬ì„¬ ê²€ì¦ (ì„ íƒì )
        if expected_checksum:
            actual_checksum = calculate_md5(file_path)
            if actual_checksum != expected_checksum:
                logger.warning(f"âŒ ì²´í¬ì„¬ ë¶ˆì¼ì¹˜ {file_path}: {actual_checksum} vs {expected_checksum}")
                return False
        
        logger.info(f"âœ… íŒŒì¼ ê²€ì¦ ì„±ê³µ: {file_path}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨ {file_path}: {e}")
        return False

def download_with_progress(url: str, output_path: Path, chunk_size: int = 8192) -> bool:
    """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, stream=True, timeout=60)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
        temp_path = output_path.with_suffix('.tmp')
        
        with open(temp_path, 'wb') as f, tqdm(
            desc=output_path.name,
            total=total_size,
            unit='B',
            unit_scale=True,
            unit_divisor=1024,
        ) as progress_bar:
            
            for chunk in response.iter_content(chunk_size=chunk_size):
                if chunk:
                    f.write(chunk)
                    progress_bar.update(len(chunk))
        
        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ íŒŒì¼ ì´ë™
        if temp_path.exists():
            shutil.move(str(temp_path), str(output_path))
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
            return True
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {url}: {e}")
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_path = output_path.with_suffix('.tmp')
        if temp_path.exists():
            temp_path.unlink()
        return False

def backup_file(file_path: Path) -> Optional[Path]:
    """íŒŒì¼ ë°±ì—… ìƒì„±"""
    try:
        if file_path.exists():
            backup_path = file_path.with_suffix(f'.backup_{int(time.time())}')
            shutil.copy2(str(file_path), str(backup_path))
            logger.info(f"ğŸ“¦ ë°±ì—… ìƒì„±: {backup_path}")
            return backup_path
        return None
    except Exception as e:
        logger.error(f"âŒ ë°±ì—… ì‹¤íŒ¨ {file_path}: {e}")
        return None

def find_project_root() -> Path:
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
    current = Path.cwd()
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ë¶€í„° ìƒìœ„ë¡œ ì˜¬ë¼ê°€ë©° ai_models ì°¾ê¸°
    for path in [current] + list(current.parents):
        ai_models_dir = path / "ai_models"
        if ai_models_dir.exists():
            return path
    
    # ëª» ì°¾ìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ ë°˜í™˜
    logger.warning("âš ï¸ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©")
    return current

# =================================================================
# ğŸš€ ë©”ì¸ ë‹¤ìš´ë¡œë“œ í´ë˜ìŠ¤
# =================================================================

class AIModelDownloader:
    """AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, project_root: Optional[Path] = None):
        self.project_root = project_root or find_project_root()
        self.download_stats = {
            "total_files": 0,
            "downloaded": 0,
            "verified": 0,
            "failed": 0,
            "skipped": 0
        }
        
        logger.info(f"ğŸ  í”„ë¡œì íŠ¸ ë£¨íŠ¸: {self.project_root}")
        
    def download_model(self, model_name: str, config: Dict[str, Any], max_retries: int = 3) -> bool:
        """ë‹¨ì¼ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        model_path = self.project_root / config["path"]
        
        logger.info(f"ğŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name}")
        logger.info(f"   - ê²½ë¡œ: {model_path}")
        logger.info(f"   - í¬ê¸°: {config['size_mb']:.1f}MB")
        logger.info(f"   - ì„¤ëª…: {config['description']}")
        
        # ê¸°ì¡´ íŒŒì¼ ê²€ì¦
        if model_path.exists():
            if verify_file_integrity(model_path, config.get("checksum", ""), config["size_mb"]):
                logger.info(f"âœ… ê¸°ì¡´ íŒŒì¼ì´ ì •ìƒì…ë‹ˆë‹¤: {model_name}")
                self.download_stats["verified"] += 1
                return True
            else:
                logger.info(f"ğŸ”§ ê¸°ì¡´ íŒŒì¼ì´ ì†ìƒë˜ì–´ ì¬ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤: {model_name}")
                # ë°±ì—… ìƒì„±
                backup_file(model_path)
        
        # URLë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„
        for attempt in range(max_retries):
            for i, url in enumerate(config["urls"]):
                try:
                    logger.info(f"ğŸŒ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt+1}/{max_retries}, URL {i+1}/{len(config['urls'])}: {url}")
                    
                    if download_with_progress(url, model_path):
                        # ë‹¤ìš´ë¡œë“œ í›„ ê²€ì¦
                        if verify_file_integrity(model_path, config.get("checksum", ""), config["size_mb"]):
                            logger.info(f"âœ… {model_name} ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ ì™„ë£Œ")
                            self.download_stats["downloaded"] += 1
                            return True
                        else:
                            logger.warning(f"âš ï¸ ë‹¤ìš´ë¡œë“œ íŒŒì¼ì´ ì†ìƒë¨, ì‚­ì œ í›„ ì¬ì‹œë„")
                            if model_path.exists():
                                model_path.unlink()
                    
                except Exception as e:
                    logger.error(f"âŒ URL {i+1} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # ì§€ìˆ˜ë°±ì˜¤í”„
                logger.info(f"â³ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
        
        logger.error(f"âŒ {model_name} ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨")
        self.download_stats["failed"] += 1
        return False
    
    def verify_all_models(self) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ íŒŒì¼ ê²€ì¦"""
        logger.info("ğŸ” ëª¨ë“  ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì‹œì‘...")
        
        results = {}
        for model_name, config in MODEL_CONFIGS.items():
            model_path = self.project_root / config["path"]
            is_valid = verify_file_integrity(
                model_path, 
                config.get("checksum", ""), 
                config["size_mb"]
            )
            results[model_name] = is_valid
            
            if is_valid:
                self.download_stats["verified"] += 1
            else:
                self.download_stats["failed"] += 1
        
        return results
    
    def download_all_models(self, required_only: bool = True) -> bool:
        """ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        logger.info("ğŸš€ ì „ì²´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        models_to_download = {
            name: config for name, config in MODEL_CONFIGS.items()
            if not required_only or config.get("required", False)
        }
        
        self.download_stats["total_files"] = len(models_to_download)
        
        success_count = 0
        for model_name, config in models_to_download.items():
            if self.download_model(model_name, config):
                success_count += 1
            else:
                logger.error(f"âŒ {model_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        
        logger.info(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{len(models_to_download)}ê°œ ì„±ê³µ")
        return success_count == len(models_to_download)
    
    def fix_u2net(self) -> bool:
        """u2net.pth íŒŒì¼ íŠ¹ë³„ ë³µêµ¬"""
        logger.info("ğŸ”§ u2net.pth íŒŒì¼ ë³µêµ¬ ì‹œì‘...")
        
        if "u2net.pth" not in MODEL_CONFIGS:
            logger.error("âŒ u2net.pth ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        return self.download_model("u2net.pth", MODEL_CONFIGS["u2net.pth"])
    
    def print_summary(self):
        """ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        logger.info(f"ğŸ“ ì´ íŒŒì¼ ìˆ˜: {self.download_stats['total_files']}")
        logger.info(f"â¬‡ï¸ ë‹¤ìš´ë¡œë“œ: {self.download_stats['downloaded']}")
        logger.info(f"âœ… ê²€ì¦ í†µê³¼: {self.download_stats['verified']}")
        logger.info(f"âŒ ì‹¤íŒ¨: {self.download_stats['failed']}")
        logger.info(f"â­ï¸ ê±´ë„ˆëœ€: {self.download_stats['skipped']}")
        logger.info("=" * 60)

# =================================================================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³µêµ¬ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python download_ai_models.py --fix-u2net          # u2net.pth ë³µêµ¬
  python download_ai_models.py --verify-all         # ëª¨ë“  íŒŒì¼ ê²€ì¦
  python download_ai_models.py --download-all       # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  python download_ai_models.py --download-required  # í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
        """
    )
    
    parser.add_argument('--fix-u2net', action='store_true', 
                       help='u2net.pth íŒŒì¼ ë³µêµ¬')
    parser.add_argument('--verify-all', action='store_true',
                       help='ëª¨ë“  ëª¨ë¸ íŒŒì¼ ê²€ì¦')
    parser.add_argument('--download-all', action='store_true',
                       help='ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--download-required', action='store_true',
                       help='í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--project-root', type=str,
                       help='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
    project_root = Path(args.project_root) if args.project_root else None
    downloader = AIModelDownloader(project_root)
    
    try:
        if args.fix_u2net:
            logger.info("ğŸ”§ u2net.pth íŒŒì¼ ë³µêµ¬ ì‹œì‘...")
            success = downloader.fix_u2net()
            if success:
                logger.info("âœ… u2net.pth ë³µêµ¬ ì™„ë£Œ!")
            else:
                logger.error("âŒ u2net.pth ë³µêµ¬ ì‹¤íŒ¨!")
                sys.exit(1)
        
        elif args.verify_all:
            logger.info("ğŸ” ëª¨ë“  ëª¨ë¸ íŒŒì¼ ê²€ì¦...")
            results = downloader.verify_all_models()
            
            valid_count = sum(results.values())
            total_count = len(results)
            
            logger.info(f"ğŸ“Š ê²€ì¦ ê²°ê³¼: {valid_count}/{total_count}ê°œ íŒŒì¼ ì •ìƒ")
            
            for model_name, is_valid in results.items():
                status = "âœ…" if is_valid else "âŒ"
                logger.info(f"   {status} {model_name}")
        
        elif args.download_all:
            logger.info("ğŸš€ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ...")
            success = downloader.download_all_models(required_only=False)
            if not success:
                sys.exit(1)
        
        elif args.download_required:
            logger.info("ğŸ¯ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ...")
            success = downloader.download_all_models(required_only=True)
            if not success:
                sys.exit(1)
        
        else:
            parser.print_help()
            return
        
        downloader.print_summary()
        logger.info("ğŸ‰ ì‘ì—… ì™„ë£Œ!")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()