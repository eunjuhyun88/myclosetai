#!/usr/bin/env python3
"""
Step 5 ì˜ë¥˜ ì›Œí•‘ AI ëª¨ë¸ ë‹¤ìš´ë¡œë”
âœ… Conda í™˜ê²½ ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ê´€ë¦¬
âœ… ìë™ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
âœ… ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì§€ì›
âœ… ì§„í–‰ë¥  í‘œì‹œ
"""

import os
import sys
import asyncio
import aiohttp
import aiofiles
import json
import time
import hashlib
import zipfile
import tarfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import logging
from concurrent.futures import ThreadPoolExecutor
import subprocess
import platform

# ì§„í–‰ë¥  í‘œì‹œ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("ğŸ’¡ ë” ë‚˜ì€ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ tqdmì„ ì„¤ì¹˜í•˜ì„¸ìš”: pip install tqdm")

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

try:
    from huggingface_hub import snapshot_download, hf_hub_download, login
    from huggingface_hub.utils import HfHubHTTPError
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸ Hugging Face Hubê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install huggingface-hub")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('step_05_download.log')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    name: str
    repo_id: str
    local_path: str
    size_mb: int
    required_files: List[str]
    optional_files: List[str] = None
    download_method: str = "huggingface"  # huggingface, direct, git
    url: str = None
    checksum: str = None
    
    def __post_init__(self):
        if self.optional_files is None:
            self.optional_files = []

class Step05AIDownloader:
    """Step 5 AI ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, base_dir: Optional[str] = None):
        """ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        if base_dir:
            self.base_dir = Path(base_dir)
        else:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
            current = Path(__file__).resolve()
            for parent in current.parents:
                if (parent / "backend").exists():
                    self.base_dir = parent / "backend" / "ai_models" / "step_05_cloth_warping"
                    break
            else:
                self.base_dir = Path.cwd() / "ai_models" / "step_05_cloth_warping"
        
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self.is_m3_max = self._detect_m3_max()
        self.max_workers = 8 if self.is_m3_max else 4
        self.chunk_size = 1024 * 1024  # 1MB chunks
        
        # ëª¨ë¸ ì •ì˜
        self.models = self._define_models()
        
        logger.info(f"ğŸš€ Step 5 AI ë‹¤ìš´ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ: {self.base_dir}")
        logger.info(f"ğŸ M3 Max ìµœì í™”: {self.is_m3_max}")
        logger.info(f"âš¡ ìµœëŒ€ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ: {self.max_workers}")
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == "Darwin":  # macOS
                if TORCH_AVAILABLE and torch.backends.mps.is_available():
                    return True
        except Exception:
            pass
        return False
    
    def _define_models(self) -> Dict[str, ModelConfig]:
        """í•„ìš”í•œ AI ëª¨ë¸ë“¤ ì •ì˜"""
        return {
            # 1. IDM-VTON (í•µì‹¬ ì˜ë¥˜ ì›Œí•‘ ëª¨ë¸)
            "idm_vton": ModelConfig(
                name="IDM-VTON",
                repo_id="yisol/IDM-VTON",
                local_path=str(self.base_dir / "idm_vton"),
                size_mb=8500,
                required_files=["model.safetensors", "config.json"],
                optional_files=["tokenizer.json", "scheduler.json"]
            ),
            
            # 2. SAM for Segmentation
            "sam_vit_large": ModelConfig(
                name="SAM-ViT-Large",
                repo_id="facebook/sam-vit-large",
                local_path=str(self.base_dir / "sam"),
                size_mb=2400,
                required_files=["pytorch_model.bin"],
                download_method="direct",
                url="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth"
            ),
            
            # 3. Stable Diffusion Inpainting
            "sd_inpainting": ModelConfig(
                name="Stable Diffusion Inpainting",
                repo_id="runwayml/stable-diffusion-inpainting",
                local_path=str(self.base_dir / "sd_inpainting"),
                size_mb=5100,
                required_files=["unet/diffusion_pytorch_model.safetensors", "vae/diffusion_pytorch_model.safetensors"]
            ),
            
            # 4. OpenPose for Pose Estimation
            "openpose": ModelConfig(
                name="OpenPose",
                repo_id="lllyasviel/Annotators", 
                local_path=str(self.base_dir / "openpose"),
                size_mb=1200,
                required_files=["body_pose_model.pth"],
                optional_files=["hand_pose_model.pth", "face_pose_model.pth"]
            ),
            
            # 5. CLIP for Feature Extraction
            "clip_vit": ModelConfig(
                name="CLIP-ViT-Large",
                repo_id="openai/clip-vit-large-patch14",
                local_path=str(self.base_dir / "clip"),
                size_mb=1700,
                required_files=["pytorch_model.bin", "config.json"]
            ),
            
            # 6. DensePose (ì˜ë¥˜ ë§¤í•‘ìš©)
            "densepose": ModelConfig(
                name="DensePose",
                repo_id="facebook/densepose",
                local_path=str(self.base_dir / "densepose"),
                size_mb=800,
                required_files=["model.pkl"],
                download_method="direct",
                url="https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x.pkl"
            ),
            
            # 7. Thin-Plate Spline (ê¸°í•˜í•™ì  ë³€í˜•ìš©)
            "tps_model": ModelConfig(
                name="TPS Transformation",
                repo_id="microsoft/DiT-XL-2-256",
                local_path=str(self.base_dir / "tps"),
                size_mb=3200,
                required_files=["diffusion_pytorch_model.safetensors"]
            ),
            
            # 8. Texture Synthesis Model
            "texture_synthesis": ModelConfig(
                name="Texture Synthesis",
                repo_id="stabilityai/stable-diffusion-2-inpainting",
                local_path=str(self.base_dir / "texture"),
                size_mb=4600,
                required_files=["unet/diffusion_pytorch_model.safetensors"]
            )
        }
    
    async def download_all_models(self, force_redownload: bool = False) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        print("ğŸš€ Step 5 AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        print("=" * 60)
        
        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        total_size_mb = sum(model.size_mb for model in self.models.values())
        if not self._check_disk_space(total_size_mb):
            logger.error(f"âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±! í•„ìš”: {total_size_mb/1024:.1f}GB")
            return {}
        
        print(f"ğŸ“Š ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_size_mb/1024:.1f}GB")
        print(f"ğŸ“‚ ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜: {self.base_dir}")
        print(f"âš¡ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìˆ˜: {self.max_workers}")
        print()
        
        # Hugging Face ë¡œê·¸ì¸ í™•ì¸ (ì„ íƒì )
        await self._check_hf_login()
        
        # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        results = {}
        
        if TQDM_AVAILABLE:
            progress_bar = tqdm(
                total=len(self.models),
                desc="ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ",
                unit="model",
                ncols=80
            )
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def download_with_semaphore(model_name: str, model_config: ModelConfig):
            async with semaphore:
                success = await self._download_single_model(model_name, model_config, force_redownload)
                if TQDM_AVAILABLE:
                    progress_bar.update(1)
                return model_name, success
        
        # ëª¨ë“  ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            download_with_semaphore(name, config)
            for name, config in self.models.items()
        ]
        
        # ë³‘ë ¬ ì‹¤í–‰
        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
        
        if TQDM_AVAILABLE:
            progress_bar.close()
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for result in completed_tasks:
            if isinstance(result, Exception):
                logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {result}")
            else:
                model_name, success = result
                results[model_name] = success
        
        # ê²°ê³¼ ìš”ì•½
        success_count = sum(results.values())
        total_count = len(results)
        
        print("\n" + "=" * 60)
        print(f"ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ")
        
        if success_count == total_count:
            print("âœ… ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            failed_models = [name for name, success in results.items() if not success]
            print(f"âš ï¸ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤: {', '.join(failed_models)}")
        
        # ê²€ì¦ ì‹¤í–‰
        print("\nğŸ” ëª¨ë¸ ê²€ì¦ ì‹œì‘...")
        verification_results = await self._verify_all_models()
        
        verified_count = sum(verification_results.values())
        print(f"âœ… ê²€ì¦ ì™„ë£Œ: {verified_count}/{total_count} í†µê³¼")
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        await self._generate_summary_report(results, verification_results)
        
        return results
    
    async def _download_single_model(
        self,
        model_name: str,
        model_config: ModelConfig,
        force_redownload: bool
    ) -> bool:
        """ë‹¨ì¼ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            model_path = Path(model_config.local_path)
            
            # ê¸°ì¡´ íŒŒì¼ í™•ì¸
            if not force_redownload and self._model_exists(model_config):
                logger.info(f"âœ… {model_config.name} - ì´ë¯¸ ì¡´ì¬í•¨")
                return True
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            model_path.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“¥ {model_config.name} ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            
            # ë‹¤ìš´ë¡œë“œ ë°©ë²•ì— ë”°ë¼ ë¶„ê¸°
            if model_config.download_method == "huggingface":
                success = await self._download_from_huggingface(model_config)
            elif model_config.download_method == "direct":
                success = await self._download_direct(model_config)
            elif model_config.download_method == "git":
                success = await self._download_from_git(model_config)
            else:
                logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë‹¤ìš´ë¡œë“œ ë°©ë²•: {model_config.download_method}")
                return False
            
            if success:
                # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
                if self._verify_model(model_config):
                    logger.info(f"âœ… {model_config.name} ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ ì™„ë£Œ")
                    return True
                else:
                    logger.warning(f"âš ï¸ {model_config.name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œë˜ì—ˆì§€ë§Œ ê²€ì¦ ì‹¤íŒ¨")
                    return False
            else:
                logger.error(f"âŒ {model_config.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ {model_config.name} ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def _download_from_huggingface(self, model_config: ModelConfig) -> bool:
        """Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ"""
        try:
            if not HF_AVAILABLE:
                logger.error("âŒ Hugging Face Hubê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return False
            
            # íŠ¹ì • íŒŒì¼ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ (ìš©ëŸ‰ ì ˆì•½)
            if model_config.required_files:
                for file_pattern in model_config.required_files:
                    try:
                        file_path = hf_hub_download(
                            repo_id=model_config.repo_id,
                            filename=file_pattern,
                            cache_dir=model_config.local_path,
                            local_dir=model_config.local_path,
                            resume_download=True
                        )
                        logger.info(f"  âœ… {file_pattern} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"  âš ï¸ {file_pattern} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                # ì „ì²´ ë¦¬í¬ì§€í† ë¦¬ ë‹¤ìš´ë¡œë“œ
                snapshot_download(
                    repo_id=model_config.repo_id,
                    cache_dir=model_config.local_path,
                    local_dir=model_config.local_path,
                    local_dir_use_symlinks=False,
                    resume_download=True
                )
            
            return True
            
        except HfHubHTTPError as e:
            if "401" in str(e):
                logger.error(f"âŒ {model_config.name}: ì¸ì¦ í•„ìš” (Hugging Face ë¡œê·¸ì¸)")
            elif "404" in str(e):
                logger.error(f"âŒ {model_config.name}: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                logger.error(f"âŒ {model_config.name}: HTTP ì˜¤ë¥˜ {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ {model_config.name} HF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _download_direct(self, model_config: ModelConfig) -> bool:
        """ì§ì ‘ URLì—ì„œ ë‹¤ìš´ë¡œë“œ"""
        try:
            if not model_config.url:
                logger.error(f"âŒ {model_config.name}: ë‹¤ìš´ë¡œë“œ URLì´ ì—†ìŒ")
                return False
            
            filename = Path(model_config.url).name
            file_path = Path(model_config.local_path) / filename
            
            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
            if file_path.exists() and file_path.stat().st_size > 1024:  # 1KB ì´ìƒ
                logger.info(f"  âœ… {filename} ì´ë¯¸ ì¡´ì¬í•¨")
                return True
            
            async with aiohttp.ClientSession() as session:
                logger.info(f"  ğŸ“¥ {filename} ë‹¤ìš´ë¡œë“œ ì¤‘...")
                
                async with session.get(model_config.url) as response:
                    if response.status != 200:
                        logger.error(f"âŒ HTTP {response.status}: {model_config.url}")
                        return False
                    
                    total_size = int(response.headers.get('content-length', 0))
                    downloaded = 0
                    
                    async with aiofiles.open(file_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(self.chunk_size):
                            await f.write(chunk)
                            downloaded += len(chunk)
                            
                            # ì§„í–‰ë¥  ë¡œê¹… (10MBë§ˆë‹¤)
                            if downloaded % (10 * 1024 * 1024) == 0:
                                if total_size > 0:
                                    progress = (downloaded / total_size) * 100
                                    logger.info(f"    ğŸ“Š {filename}: {progress:.1f}% ({downloaded//1024//1024}MB)")
            
            logger.info(f"  âœ… {filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({file_path.stat().st_size//1024//1024}MB)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {model_config.name} ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _download_from_git(self, model_config: ModelConfig) -> bool:
        """Git LFSë¡œ ë‹¤ìš´ë¡œë“œ"""
        try:
            model_path = Path(model_config.local_path)
            
            if model_path.exists() and any(model_path.iterdir()):
                logger.info(f"  âœ… {model_config.name} Git ë¦¬í¬ì§€í† ë¦¬ ì´ë¯¸ ì¡´ì¬")
                return True
            
            # Git clone with LFS
            cmd = [
                "git", "clone",
                f"https://huggingface.co/{model_config.repo_id}",
                str(model_path)
            ]
            
            logger.info(f"  ğŸ“¥ Git clone: {model_config.repo_id}")
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.info(f"  âœ… {model_config.name} Git clone ì™„ë£Œ")
                return True
            else:
                logger.error(f"âŒ Git clone ì‹¤íŒ¨: {stderr.decode()}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ {model_config.name} Git ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _model_exists(self, model_config: ModelConfig) -> bool:
        """ëª¨ë¸ ì¡´ì¬ í™•ì¸"""
        model_path = Path(model_config.local_path)
        
        if not model_path.exists():
            return False
        
        # í•„ìˆ˜ íŒŒì¼ë“¤ í™•ì¸
        for required_file in model_config.required_files:
            file_patterns = list(model_path.rglob(required_file))
            if not file_patterns:
                return False
        
        return True
    
    def _verify_model(self, model_config: ModelConfig) -> bool:
        """ëª¨ë¸ ê²€ì¦"""
        try:
            model_path = Path(model_config.local_path)
            
            # 1. ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
            if not model_path.exists():
                return False
            
            # 2. í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
            for required_file in model_config.required_files:
                file_patterns = list(model_path.rglob(required_file))
                if not file_patterns:
                    logger.warning(f"âš ï¸ í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {required_file}")
                    return False
            
            # 3. íŒŒì¼ í¬ê¸° í™•ì¸
            total_size = sum(
                f.stat().st_size for f in model_path.rglob("*") 
                if f.is_file()
            )
            
            expected_size = model_config.size_mb * 1024 * 1024
            size_ratio = total_size / expected_size
            
            if size_ratio < 0.5:  # 50% ë¯¸ë§Œì´ë©´ ë¬¸ì œ
                logger.warning(f"âš ï¸ í¬ê¸° ë¶€ì¡±: {total_size//1024//1024}MB < ì˜ˆìƒ {model_config.size_mb}MB")
                return False
            
            # 4. PyTorch ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì„ íƒì )
            if TORCH_AVAILABLE:
                model_files = list(model_path.rglob("*.bin")) + list(model_path.rglob("*.pth"))
                for model_file in model_files[:2]:  # ì²˜ìŒ 2ê°œë§Œ ê²€ì‚¬
                    try:
                        torch.load(model_file, map_location='cpu')
                    except Exception as e:
                        logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì†ìƒ ê°€ëŠ¥ì„±: {model_file.name} - {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return False
    
    async def _verify_all_models(self) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ê²€ì¦"""
        results = {}
        
        for model_name, model_config in self.models.items():
            verified = self._verify_model(model_config)
            results[model_name] = verified
            
            status = "âœ… ê²€ì¦ë¨" if verified else "âŒ ê²€ì¦ ì‹¤íŒ¨"
            logger.info(f"{status} {model_config.name}")
        
        return results
    
    def _check_disk_space(self, required_mb: int) -> bool:
        """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸"""
        try:
            import shutil
            free_space = shutil.disk_usage(self.base_dir).free
            free_space_mb = free_space // (1024 * 1024)
            
            # ì—¬ìœ  ê³µê°„ 50% ì¶”ê°€ ìš”êµ¬
            required_with_buffer = required_mb * 1.5
            
            logger.info(f"ğŸ’¾ ë””ìŠ¤í¬ ê³µê°„: {free_space_mb}MB ì—¬ìœ , {required_with_buffer:.0f}MB í•„ìš”")
            
            return free_space_mb >= required_with_buffer
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
            return True
    
    async def _check_hf_login(self):
        """Hugging Face ë¡œê·¸ì¸ í™•ì¸"""
        try:
            if HF_AVAILABLE:
                from huggingface_hub import whoami
                try:
                    user_info = whoami()
                    logger.info(f"ğŸ” Hugging Face ë¡œê·¸ì¸: {user_info.get('name', 'Unknown')}")
                except Exception:
                    logger.warning("âš ï¸ Hugging Face ë¡œê·¸ì¸ ì•ˆë¨ (ì¼ë¶€ ëª¨ë¸ ì ‘ê·¼ ì œí•œ ê°€ëŠ¥)")
        except Exception:
            pass
    
    async def _generate_summary_report(
        self,
        download_results: Dict[str, bool],
        verification_results: Dict[str, bool]
    ):
        """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "step": "05_cloth_warping",
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": {
                "platform": platform.system(),
                "is_m3_max": self.is_m3_max,
                "torch_available": TORCH_AVAILABLE,
                "hf_available": HF_AVAILABLE
            },
            "download_summary": {
                "total_models": len(self.models),
                "downloaded": sum(download_results.values()),
                "verified": sum(verification_results.values()),
                "failed": len(self.models) - sum(download_results.values())
            },
            "models": {}
        }
        
        # ê° ëª¨ë¸ ìƒì„¸ ì •ë³´
        for model_name, model_config in self.models.items():
            model_path = Path(model_config.local_path)
            actual_size = 0
            
            if model_path.exists():
                actual_size = sum(
                    f.stat().st_size for f in model_path.rglob("*") 
                    if f.is_file()
                ) // (1024 * 1024)  # MB
            
            report["models"][model_name] = {
                "name": model_config.name,
                "repo_id": model_config.repo_id,
                "downloaded": download_results.get(model_name, False),
                "verified": verification_results.get(model_name, False),
                "expected_size_mb": model_config.size_mb,
                "actual_size_mb": actual_size,
                "download_method": model_config.download_method
            }
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.base_dir / "download_report.json"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥: {report_path}")
            
            # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
            print(f"\nğŸ“Š ë‹¤ìš´ë¡œë“œ ìš”ì•½:")
            print(f"  âœ… ì„±ê³µ: {report['download_summary']['downloaded']}")
            print(f"  ğŸ” ê²€ì¦ë¨: {report['download_summary']['verified']}")
            print(f"  âŒ ì‹¤íŒ¨: {report['download_summary']['failed']}")
            print(f"  ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.base_dir}")
            print(f"  ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_path}")
            
        except Exception as e:
            logger.error(f"âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def download_specific_models(self, model_names: List[str]) -> Dict[str, bool]:
        """íŠ¹ì • ëª¨ë¸ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ"""
        selected_models = {
            name: config for name, config in self.models.items()
            if name in model_names
        }
        
        if not selected_models:
            logger.error("âŒ ì„ íƒëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        print(f"ğŸ“¥ ì„ íƒëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {', '.join(model_names)}")
        
        # ì„ì‹œë¡œ ëª¨ë¸ ëª©ë¡ êµì²´
        original_models = self.models
        self.models = selected_models
        
        try:
            results = await self.download_all_models()
            return results
        finally:
            self.models = original_models

    def list_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ Step 5 AI ëª¨ë¸ë“¤:")
        print("=" * 60)
        
        for model_name, config in self.models.items():
            status = "âœ… ë‹¤ìš´ë¡œë“œë¨" if self._model_exists(config) else "ğŸ“¥ ë‹¤ìš´ë¡œë“œ í•„ìš”"
            print(f"  {model_name:15} | {config.name:25} | {config.size_mb:>5}MB | {status}")
        
        print("=" * 60)
        total_size = sum(config.size_mb for config in self.models.values())
        print(f"ì´ í¬ê¸°: {total_size/1024:.1f}GB")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Step 5 AI ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    parser.add_argument("--models", nargs="+", help="ë‹¤ìš´ë¡œë“œí•  íŠ¹ì • ëª¨ë¸ë“¤")
    parser.add_argument("--list", action="store_true", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥")
    parser.add_argument("--force", action="store_true", help="ê°•ì œ ì¬ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--base-dir", help="ë‹¤ìš´ë¡œë“œ ê¸°ë³¸ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    # ë‹¤ìš´ë¡œë” ìƒì„±
    downloader = Step05AIDownloader(args.base_dir)
    
    if args.list:
        downloader.list_available_models()
        return
    
    try:
        if args.models:
            # íŠ¹ì • ëª¨ë¸ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ
            results = await downloader.download_specific_models(args.models)
        else:
            # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            results = await downloader.download_all_models(args.force)
        
        # ê²°ê³¼ í™•ì¸
        if results:
            success_count = sum(results.values())
            total_count = len(results)
            
            if success_count == total_count:
                print("\nğŸ‰ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                print("ì´ì œ Step 5 ì˜ë¥˜ ì›Œí•‘ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                print(f"\nâš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {success_count}/{total_count}")
                failed = [name for name, success in results.items() if not success]
                print(f"ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤: {', '.join(failed)}")
                
                print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
                print("1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
                print("2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸")
                print("3. Hugging Face ê³„ì • ë¡œê·¸ì¸ (ì¼ë¶€ ëª¨ë¸)")
                print("4. --force ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

if __name__ == "__main__":
    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ë‹¤ìš´ë¡œë“œê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        sys.exit(1)