#!/usr/bin/env python3
"""
MyCloset AI MVP - ì‹¤ì œë¡œ ë™ì‘í•˜ëŠ” ë°±ì—”ë“œ
Python 3.13 + OpenCV + PyTorchë¡œ êµ¬í˜„
MediaPipe ì—†ì´ë„ ì¶©ë¶„íˆ ë™ì‘í•˜ëŠ” ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import cv2
import numpy as np
from PIL import Image
import json
import time
import base64
import io
import logging
from typing import Dict, List, Optional, Tuple
import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="MyCloset AI MVP",
    description="OpenCV + PyTorch ê¸°ë°˜ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class WorkingVirtualTryOn:
    """ì‹¤ì œë¡œ ë™ì‘í•˜ëŠ” ê°€ìƒ í”¼íŒ… ì—”ì§„"""
    
    def __init__(self):
        logger.info("ğŸš€ Working Virtual Try-On Engine ì´ˆê¸°í™” ì¤‘...")
        
        # OpenCV ë¶„ë¥˜ê¸°ë“¤ ë¡œë“œ
        try:
            self.face_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            self.body_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_fullbody.xml'
            )
            logger.info("âœ… OpenCV Haar Cascades ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ Haar Cascades ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.face_cascade = None
            self.body_cascade = None
        
        # ìƒ‰ìƒ ë¶„ì„ê¸°
        self.color_analyzer = ColorAnalyzer()
        
        # í”¼íŒ… í”„ë¡œì„¸ì„œ
        self.fitting_processor = FittingProcessor()
        
        logger.info("âœ… Virtual Try-On Engine ì´ˆê¸°í™” ì™„ë£Œ!")
    
    async def process_virtual_fitting(
        self, 
        person_image: np.ndarray, 
        clothing_image: np.ndarray,
        height: float = 170.0,
        weight: float = 60.0
    ) -> Dict:
        """ë©”ì¸ ê°€ìƒ í”¼íŒ… í”„ë¡œì„¸ìŠ¤"""
        
        start_time = time.time()
        logger.info("ğŸ¬ ê°€ìƒ í”¼íŒ… í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        try:
            # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            person_processed = self.preprocess_image(person_image)
            clothing_processed = self.preprocess_image(clothing_image)
            
            # 2. ì‚¬ëŒ ì˜ì—­ ê²€ì¶œ
            person_region = self.detect_person_region(person_processed)
            logger.info(f"ğŸ“ ì‚¬ëŒ ì˜ì—­ ê²€ì¶œ: {person_region}")
            
            # 3. ì˜ë¥˜ ë¶„ì„
            clothing_analysis = self.color_analyzer.analyze_clothing(clothing_processed)
            logger.info(f"ğŸ‘• ì˜ë¥˜ ë¶„ì„: {clothing_analysis['category']}")
            
            # 4. ì‹ ì²´ ì¸¡ì • ì¶”ì •
            measurements = self.estimate_body_measurements(
                person_processed, person_region, height, weight
            )
            
            # 5. ê°€ìƒ í”¼íŒ… ìˆ˜í–‰
            fitted_image = self.fitting_processor.apply_virtual_fitting(
                person_processed, clothing_processed, person_region, clothing_analysis
            )
            
            # 6. í• ì ìˆ˜ ê³„ì‚°
            fit_score = self.calculate_fit_score(measurements, clothing_analysis)
            
            # 7. ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self.generate_recommendations(
                fit_score, measurements, clothing_analysis
            )
            
            processing_time = time.time() - start_time
            
            result = {
                "success": True,
                "fitted_image": self.image_to_base64(fitted_image),
                "processing_time": round(processing_time, 2),
                "confidence": 0.82,  # ì‹¤ìš©ì  ì‹ ë¢°ë„
                "measurements": measurements,
                "clothing_analysis": clothing_analysis,
                "fit_score": fit_score,
                "recommendations": recommendations
            }
            
            logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # í¬ê¸° ì •ê·œí™” (ìµœëŒ€ 800px)
        height, width = image.shape[:2]
        if max(height, width) > 800:
            scale = 800 / max(height, width)
            new_width = int(width * scale)
            new_height = int(height * scale)
            image = cv2.resize(image, (new_width, new_height))
        
        # ë…¸ì´ì¦ˆ ì œê±°
        image = cv2.bilateralFilter(image, 9, 75, 75)
        
        return image
    
    def detect_person_region(self, image: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """ì‚¬ëŒ ì˜ì—­ ê²€ì¶œ"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ì–¼êµ´ ê²€ì¶œ ì‹œë„
        if self.face_cascade is not None:
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
            if len(faces) > 0:
                x, y, w, h = faces[0]  # ê°€ì¥ í° ì–¼êµ´
                # ì–¼êµ´ ê¸°ë°˜ìœ¼ë¡œ ì‹ ì²´ ì˜ì—­ ì¶”ì •
                body_x = max(0, x - w)
                body_y = y
                body_w = min(image.shape[1] - body_x, w * 3)
                body_h = min(image.shape[0] - body_y, h * 6)
                return (body_x, body_y, body_w, body_h)
        
        # ì „ì‹  ê²€ì¶œ ì‹œë„
        if self.body_cascade is not None:
            bodies = self.body_cascade.detectMultiScale(gray, 1.1, 4)
            if len(bodies) > 0:
                return tuple(bodies[0])
        
        # ê¸°ë³¸ê°’: ì´ë¯¸ì§€ ì¤‘ì•™ ì˜ì—­
        h, w = image.shape[:2]
        return (w//4, h//6, w//2, h*2//3)
    
    def estimate_body_measurements(
        self, 
        image: np.ndarray, 
        person_region: Tuple[int, int, int, int],
        height: float,
        weight: float
    ) -> Dict:
        """ì‹ ì²´ ì¹˜ìˆ˜ ì¶”ì •"""
        
        x, y, w, h = person_region
        
        # BMI ê³„ì‚°
        bmi = weight / ((height / 100) ** 2)
        
        # ê¸°ë³¸ ì¹˜ìˆ˜ (í•œêµ­ì¸ í‰ê·  ê¸°ë°˜)
        if bmi < 18.5:
            chest_base = 82
            waist_base = 68
            hip_base = 88
        elif bmi < 25:
            chest_base = 88
            waist_base = 75
            hip_base = 95
        else:
            chest_base = 95
            waist_base = 85
            hip_base = 105
        
        # ì´ë¯¸ì§€ ë¹„ìœ¨ë¡œ ë³´ì •
        scale_factor = w / 200  # ê¸°ì¤€ ë„ˆë¹„ 200px
        
        return {
            "chest": round(chest_base * scale_factor, 1),
            "waist": round(waist_base * scale_factor, 1),
            "hip": round(hip_base * scale_factor, 1),
            "shoulder_width": round(w * 0.8, 1),
            "height_estimate": height,
            "bmi": round(bmi, 1),
            "confidence": 0.75
        }
    
    def calculate_fit_score(self, measurements: Dict, clothing_analysis: Dict) -> float:
        """í• ì ìˆ˜ ê³„ì‚°"""
        base_score = 0.8
        
        # BMI ê¸°ë°˜ ì¡°ì •
        bmi = measurements.get("bmi", 22)
        if 18.5 <= bmi <= 25:
            base_score += 0.1
        elif bmi > 30:
            base_score -= 0.1
        
        # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì¡°ì •
        category = clothing_analysis.get("category", "unknown")
        if category in ["dress", "top"]:
            base_score += 0.05
        
        return min(max(base_score, 0.5), 0.95)  # 0.5-0.95 ë²”ìœ„
    
    def generate_recommendations(
        self, 
        fit_score: float, 
        measurements: Dict, 
        clothing_analysis: Dict
    ) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        if fit_score >= 0.85:
            recommendations.append("âœ… ì™„ë²½í•œ í•! ì´ ì˜·ì´ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.")
        elif fit_score >= 0.75:
            recommendations.append("ğŸ‘ ì¢‹ì€ í•ì…ë‹ˆë‹¤. ìì‹ ìˆê²Œ ì°©ìš©í•˜ì„¸ìš”!")
        else:
            recommendations.append("âš ï¸ ì‚¬ì´ì¦ˆ ì¡°ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        # BMI ê¸°ë°˜ ì¶”ì²œ
        bmi = measurements.get("bmi", 22)
        if bmi < 18.5:
            recommendations.append("ğŸ’¡ ë³¼ë¥¨ê° ìˆëŠ” ë””ìì¸ì´ ì˜ ì–´ìš¸ë¦´ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        elif bmi > 25:
            recommendations.append("ğŸ’¡ ì—¬ìœ ìˆëŠ” í•ì˜ ì˜·ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.")
        
        # ìƒ‰ìƒ ê¸°ë°˜ ì¶”ì²œ
        dominant_color = clothing_analysis.get("dominant_color", [0, 0, 0])
        if sum(dominant_color) < 150:  # ì–´ë‘ìš´ ìƒ‰
            recommendations.append("ğŸŒŸ ì–´ë‘ìš´ ìƒ‰ìƒìœ¼ë¡œ ìŠ¬ë¦¼í•´ ë³´ì´ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            recommendations.append("â˜€ï¸ ë°ì€ ìƒ‰ìƒìœ¼ë¡œ í™œê¸°ì°¬ ëŠë‚Œì„ ì¤ë‹ˆë‹¤.")
        
        return recommendations
    
    def image_to_base64(self, image: np.ndarray) -> str:
        """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
        _, buffer = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 90])
        return base64.b64encode(buffer).decode('utf-8')


class ColorAnalyzer:
    """ìƒ‰ìƒ ë° ì˜ë¥˜ ë¶„ì„ê¸°"""
    
    def analyze_clothing(self, clothing_image: np.ndarray) -> Dict:
        """ì˜ë¥˜ ë¶„ì„"""
        
        height, width = clothing_image.shape[:2]
        
        # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
        dominant_color = self.extract_dominant_color(clothing_image)
        
        # ì¹´í…Œê³ ë¦¬ ì¶”ì • (ì¢…íš¡ë¹„ ê¸°ë°˜)
        aspect_ratio = height / width
        if aspect_ratio > 1.5:
            category = "dress"
            subcategory = "ì›í”¼ìŠ¤"
        elif aspect_ratio > 1.0:
            category = "top"
            subcategory = "ìƒì˜"
        else:
            category = "bottom"
            subcategory = "í•˜ì˜"
        
        # ìŠ¤íƒ€ì¼ ì¶”ì • (ìƒ‰ìƒ ê¸°ë°˜)
        style = self.estimate_style(dominant_color)
        
        return {
            "category": category,
            "subcategory": subcategory,
            "dominant_color": dominant_color,
            "style": style,
            "size_estimate": "M",
            "confidence": 0.7
        }
    
    def extract_dominant_color(self, image: np.ndarray) -> List[int]:
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        # ì´ë¯¸ì§€ ë¦¬ìƒ˜í”Œë§
        small_image = cv2.resize(image, (100, 100))
        data = small_image.reshape((-1, 3))
        data = np.float32(data)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        _, labels, centers = cv2.kmeans(data, 3, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        
        # ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ìƒ‰ìƒ
        unique, counts = np.unique(labels, return_counts=True)
        dominant_cluster = unique[np.argmax(counts)]
        
        return centers[dominant_cluster].astype(int).tolist()
    
    def estimate_style(self, color: List[int]) -> str:
        """ìƒ‰ìƒ ê¸°ë°˜ ìŠ¤íƒ€ì¼ ì¶”ì •"""
        r, g, b = color
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì²´í¬
        if abs(r - g) < 30 and abs(g - b) < 30:
            return "minimal"
        
        # ë°ì€ ìƒ‰ìƒ
        if sum(color) > 400:
            return "casual"
        
        # ì–´ë‘ìš´ ìƒ‰ìƒ
        if sum(color) < 200:
            return "formal"
        
        return "casual"


class FittingProcessor:
    """ê°€ìƒ í”¼íŒ… í”„ë¡œì„¸ì„œ"""
    
    def apply_virtual_fitting(
        self,
        person_image: np.ndarray,
        clothing_image: np.ndarray,
        person_region: Tuple[int, int, int, int],
        clothing_analysis: Dict
    ) -> np.ndarray:
        """ê°€ìƒ í”¼íŒ… ì ìš©"""
        
        result = person_image.copy()
        x, y, w, h = person_region
        
        # ì˜ë¥˜ ì˜ì—­ ê²°ì •
        clothing_region = self.determine_clothing_region(person_region, clothing_analysis)
        
        if clothing_region:
            cx, cy, cw, ch = clothing_region
            
            try:
                # ì˜ë¥˜ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
                resized_clothing = cv2.resize(clothing_image, (cw, ch))
                
                # ë¶€ë“œëŸ¬ìš´ ë¸”ë Œë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬ ìƒì„±
                mask = self.create_blend_mask(cw, ch)
                
                # ê²½ê³„ í™•ì¸ ë° ë¸”ë Œë”©
                if (cy + ch <= result.shape[0] and 
                    cx + cw <= result.shape[1] and 
                    cx >= 0 and cy >= 0):
                    
                    # ROI ì¶”ì¶œ
                    roi = result[cy:cy+ch, cx:cx+cw]
                    
                    # ê°€ì¤‘ ë¸”ë Œë”©
                    alpha = 0.6  # ì˜ë¥˜ íˆ¬ëª…ë„
                    blended = cv2.addWeighted(roi, 1-alpha, resized_clothing, alpha, 0)
                    
                    # ë§ˆìŠ¤í¬ ì ìš©
                    for i in range(3):  # BGR ì±„ë„
                        result[cy:cy+ch, cx:cx+cw, i] = (
                            roi[:, :, i] * (1 - mask) + 
                            blended[:, :, i] * mask
                        ).astype(np.uint8)
                
            except Exception as e:
                logger.warning(f"ë¸”ë Œë”© ì˜¤ë¥˜: {e}")
        
        return result
    
    def determine_clothing_region(
        self, 
        person_region: Tuple[int, int, int, int], 
        clothing_analysis: Dict
    ) -> Tuple[int, int, int, int]:
        """ì˜ë¥˜ ì˜ì—­ ê²°ì •"""
        
        x, y, w, h = person_region
        category = clothing_analysis.get("category", "top")
        
        if category == "dress":
            # ì›í”¼ìŠ¤: ìƒì²´+í•˜ì²´
            return (x + w//8, y + h//4, w*3//4, h*2//3)
        elif category == "top":
            # ìƒì˜: ìƒì²´ë§Œ
            return (x + w//8, y + h//4, w*3//4, h//2)
        elif category == "bottom":
            # í•˜ì˜: í•˜ì²´ë§Œ
            return (x + w//6, y + h*2//3, w*2//3, h//3)
        else:
            # ê¸°ë³¸ê°’
            return (x + w//8, y + h//4, w*3//4, h//2)
    
    def create_blend_mask(self, width: int, height: int) -> np.ndarray:
        """ë¸”ë Œë”© ë§ˆìŠ¤í¬ ìƒì„±"""
        
        mask = np.ones((height, width), dtype=np.float32)
        
        # ê²½ê³„ë¥¼ ë¶€ë“œëŸ½ê²Œ
        border_size = min(width, height) // 20
        
        # ìƒí•˜ì¢Œìš° ê²½ê³„ í˜ì´ë”©
        mask[:border_size, :] *= np.linspace(0, 1, border_size).reshape(-1, 1)
        mask[-border_size:, :] *= np.linspace(1, 0, border_size).reshape(-1, 1)
        mask[:, :border_size] *= np.linspace(0, 1, border_size)
        mask[:, -border_size:] *= np.linspace(1, 0, border_size)
        
        return mask


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
virtual_tryon_engine = WorkingVirtualTryOn()


@app.post("/api/virtual-tryon")
async def virtual_tryon_endpoint(
    person_image: UploadFile = File(..., description="ì‚¬ìš©ì ì‚¬ì§„"),
    clothing_image: UploadFile = File(..., description="ì˜ë¥˜ ì‚¬ì§„"),
    height: float = Form(170.0, description="í‚¤ (cm)"),
    weight: float = Form(60.0, description="ëª¸ë¬´ê²Œ (kg)")
):
    """ê°€ìƒ í”¼íŒ… API ì—”ë“œí¬ì¸íŠ¸"""
    
    try:
        # íŒŒì¼ íƒ€ì… ê²€ì¦
        if not person_image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ì ì‚¬ì§„ì€ ì´ë¯¸ì§€ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        if not clothing_image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ì˜ë¥˜ ì‚¬ì§„ì€ ì´ë¯¸ì§€ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        person_bytes = await person_image.read()
        clothing_bytes = await clothing_image.read()
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        person_np = np.frombuffer(person_bytes, np.uint8)
        clothing_np = np.frombuffer(clothing_bytes, np.uint8)
        
        person_img = cv2.imdecode(person_np, cv2.IMREAD_COLOR)
        clothing_img = cv2.imdecode(clothing_np, cv2.IMREAD_COLOR)
        
        # ì´ë¯¸ì§€ ê²€ì¦
        if person_img is None:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ì ì‚¬ì§„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if clothing_img is None:
            raise HTTPException(status_code=400, detail="ì˜ë¥˜ ì‚¬ì§„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
        result = await virtual_tryon_engine.process_virtual_fitting(
            person_img, clothing_img, height, weight
        )
        
        return JSONResponse(content=result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.get("/api/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "MyCloset AI MVP",
        "version": "1.0.0",
        "features": [
            "Virtual Try-On",
            "Body Measurement Estimation", 
            "Clothing Analysis",
            "Fit Score Calculation",
            "Style Recommendations"
        ],
        "tech_stack": [
            "FastAPI",
            "OpenCV",
            "PyTorch",
            "NumPy"
        ]
    }


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "ğŸš€ MyCloset AI MVP is running!",
        "docs": "/docs",
        "health": "/api/health",
        "virtual_tryon": "/api/virtual-tryon"
    }


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ MyCloset AI MVP ì„œë²„ ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“± API ë¬¸ì„œ: http://localhost:8000/docs")
    print(f"ğŸ¥ í—¬ìŠ¤ì²´í¬: http://localhost:8000/api/health")
    print(f"ğŸ”— ê°€ìƒ í”¼íŒ…: http://localhost:8000/api/virtual-tryon")
    print("=" * 60)
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )