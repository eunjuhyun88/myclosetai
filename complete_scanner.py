#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ì™„ì „ ê³ ë„í™”ëœ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸
=================================================================

M3 Max 128GB ìµœì í™”, conda í™˜ê²½ ìš°ì„ , ì™„ì „ ìë™í™” ì§€ì›

íŠ¹ì§•:
- ğŸ›¡ï¸ ê¶Œí•œ ì•ˆì „ì„± (macOS/Linux/Windows ëŒ€ì‘)
- ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” (16ì½”ì–´ í™œìš©)
- ğŸ§  AI ê¸°ë°˜ ëª¨ë¸ ë¶„ë¥˜ (8ë‹¨ê³„ + í”„ë ˆì„ì›Œí¬)
- ğŸ“Š ì‹¤ì‹œê°„ ì§„í–‰ë¥  ë° ìƒì„¸ ë¶„ì„
- ğŸ”„ ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì œê±°
- ğŸ“ ìë™ ì •ë¦¬ ë° ì´ë™ ê¸°ëŠ¥
- âš™ï¸ conda í™˜ê²½ ìš°ì„  ì„¤ì •
- ğŸ¯ MyCloset AI íŠ¹í™” ìµœì í™”

ì‚¬ìš©ë²•:
    python advanced_scanner.py                    # í‘œì¤€ ìŠ¤ìº”
    python advanced_scanner.py --deep            # ë”¥ ìŠ¤ìº”
    python advanced_scanner.py --organize        # ìŠ¤ìº” + ìë™ ì •ë¦¬
    python advanced_scanner.py --conda-first     # conda í™˜ê²½ ìš°ì„ 
    python advanced_scanner.py --repair          # ì†ìƒëœ ëª¨ë¸ ë³µêµ¬
"""

import os
import sys
import time
import json
import hashlib
import argparse
import subprocess
import platform
import threading
import asyncio
import sqlite3
import shutil
import glob
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Set, Union
from dataclasses import dataclass, asdict, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from datetime import datetime, timedelta
import re
import mimetypes
import pickle
from collections import defaultdict, Counter
import logging
import warnings

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì•ˆì „í•œ import)
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    class tqdm:
        def __init__(self, iterable=None, total=None, desc="", **kwargs):
            self.iterable = iterable or []
            self.total = total or (len(iterable) if hasattr(iterable, '__len__') else 0)
            self.desc = desc
            self.current = 0
            
        def __iter__(self):
            for item in self.iterable:
                yield item
                self.current += 1
                self._update()
            print()
            
        def update(self, n=1):
            self.current += n
            self._update()
            
        def _update(self):
            if self.total > 0:
                percent = (self.current / self.total) * 100
                print(f"\r{self.desc}: {self.current}/{self.total} ({percent:.1f}%)", end='', flush=True)

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_scanner.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ“Š ê³ ê¸‰ ë°ì´í„° ëª¨ë¸
# ==============================================

@dataclass
class ModelMetadata:
    """ê³ ê¸‰ ëª¨ë¸ ë©”íƒ€ë°ì´í„°"""
    architecture: str = "unknown"
    parameters: Optional[int] = None
    precision: str = "unknown"
    framework_version: str = "unknown"
    training_framework: str = "unknown"
    has_tokenizer: bool = False
    has_config: bool = False
    is_fine_tuned: bool = False
    base_model: Optional[str] = None
    license: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    
@dataclass
class ModelInfo:
    """ì™„ì „í•œ AI ëª¨ë¸ ì •ë³´"""
    # ê¸°ë³¸ ì •ë³´
    name: str
    path: str
    absolute_path: str
    size_bytes: int
    size_mb: float
    size_gb: float
    
    # íŒŒì¼ ì •ë³´
    extension: str
    mime_type: str
    created_time: datetime
    modified_time: datetime
    access_time: datetime
    checksum_md5: str
    checksum_sha256: str
    
    # AI ëª¨ë¸ ë¶„ë¥˜
    framework: str
    model_type: str
    step_candidate: str
    confidence: float
    architecture: str
    
    # ìœ„ì¹˜ ë° í™˜ê²½
    is_in_project: bool
    is_in_conda: bool
    conda_env_name: Optional[str]
    environment_path: Optional[str]
    parent_directory: str
    
    # ìƒíƒœ ë° ê²€ì¦
    is_valid: bool
    is_complete: bool
    is_corrupted: bool
    validation_errors: List[str]
    
    # ê´€ê³„ì„±
    companion_files: List[str]
    related_models: List[str]
    duplicate_of: Optional[str]
    
    # ê³ ê¸‰ ë©”íƒ€ë°ì´í„°
    metadata: ModelMetadata
    
    # ì‚¬ìš©ëŸ‰ ì •ë³´
    last_accessed: Optional[datetime] = None
    access_count: int = 0
    importance_score: float = 0.0
    
@dataclass
class ScanConfig:
    """ìŠ¤ìº” ì„¤ì •"""
    include_patterns: List[str] = field(default_factory=lambda: [
        '*.pth', '*.pt', '*.bin', '*.safetensors', '*.ckpt', '*.checkpoint',
        '*.h5', '*.pb', '*.onnx', '*.tflite', '*.pkl', '*.joblib',
        '*.model', '*.weights', '*.npz', '*.npy'
    ])
    exclude_patterns: List[str] = field(default_factory=lambda: [
        'node_modules', '__pycache__', '.git', '.cache/pip',
        'trash', 'recycle', 'temp', 'tmp', '.DS_Store'
    ])
    min_size_mb: float = 0.1
    max_size_gb: float = 100.0
    max_depth: int = 10
    follow_symlinks: bool = False
    conda_priority: bool = True
    deep_scan: bool = False
    verify_integrity: bool = True
    extract_metadata: bool = True

@dataclass
class ScanStatistics:
    """ì™„ì „í•œ ìŠ¤ìº” í†µê³„"""
    # ê¸°ë³¸ í†µê³„
    total_files_scanned: int = 0
    models_found: int = 0
    total_size_bytes: int = 0
    total_size_gb: float = 0.0
    scan_duration: float = 0.0
    
    # ìœ„ì¹˜ í†µê³„
    locations_scanned: int = 0
    conda_models: int = 0
    project_models: int = 0
    system_models: int = 0
    
    # í’ˆì§ˆ í†µê³„
    valid_models: int = 0
    corrupted_models: int = 0
    duplicate_groups: int = 0
    unique_models: int = 0
    
    # í”„ë ˆì„ì›Œí¬ ë¶„í¬
    framework_distribution: Dict[str, int] = field(default_factory=dict)
    type_distribution: Dict[str, int] = field(default_factory=dict)
    step_distribution: Dict[str, int] = field(default_factory=dict)
    
    # ì„±ëŠ¥ í†µê³„
    errors_count: int = 0
    warnings_count: int = 0
    processing_speed_files_per_sec: float = 0.0

# ==============================================
# ğŸ” ì™„ì „ ê³ ë„í™”ëœ AI ëª¨ë¸ ìŠ¤ìºë„ˆ
# ==============================================

class AdvancedModelScanner:
    """ì™„ì „ ê³ ë„í™”ëœ AI ëª¨ë¸ ë° ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìºë„ˆ"""
    
    def __init__(self, config: ScanConfig = None):
        self.config = config or ScanConfig()
        self.project_root = Path.cwd()
        self.scan_start_time = time.time()
        
        # ìŠ¤ìº” ê²°ê³¼ ì €ì¥
        self.found_models: List[ModelInfo] = []
        self.scan_locations: Dict[str, List[str]] = {}
        self.duplicates: Dict[str, List[ModelInfo]] = {}
        self.errors: List[str] = []
        self.warnings: List[str] = []
        
        # ì„±ëŠ¥ ìµœì í™”
        self.cpu_count = os.cpu_count() or 4
        self.max_workers = min(self.cpu_count, 16)  # M3 Max ìµœì í™”
        
        # conda í™˜ê²½ ì •ë³´
        self.conda_environments = self._detect_conda_environments()
        self.current_conda_env = os.environ.get('CONDA_DEFAULT_ENV')
        
        # ëª¨ë¸ ë¶„ë¥˜ íŒ¨í„´ (ê³ ë„í™”)
        self._init_classification_patterns()
        
        # ê²€ì¦ ìºì‹œ
        self.validation_cache = {}
        self.metadata_cache = {}
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()
        
        logger.info(f"ğŸš€ AdvancedModelScanner ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ’» ì‹œìŠ¤í…œ: {platform.system()} {platform.machine()}")
        logger.info(f"ğŸ Python: {platform.python_version()}")
        logger.info(f"ğŸ”§ ì›Œì»¤: {self.max_workers}ê°œ")
        logger.info(f"ğŸ Conda í™˜ê²½: {len(self.conda_environments)}ê°œ ë°œê²¬")
        
    def _init_classification_patterns(self):
        """AI ëª¨ë¸ ë¶„ë¥˜ íŒ¨í„´ ì´ˆê¸°í™” (ê³ ë„í™”)"""
        
        # í”„ë ˆì„ì›Œí¬ íŒ¨í„´ (í™•ì¥)
        self.framework_patterns = {
            'pytorch': {
                'extensions': ['.pth', '.pt', '.bin'],
                'magic_bytes': [b'PK', b'\x80', b'PYTORCH'],
                'indicators': ['torch', 'pytorch', 'state_dict']
            },
            'safetensors': {
                'extensions': ['.safetensors'],
                'magic_bytes': [b'{"'],
                'indicators': ['safetensors', 'huggingface']
            },
            'tensorflow': {
                'extensions': ['.pb', '.h5', '.tflite'],
                'magic_bytes': [b'\x08', b'\x89HDF'],
                'indicators': ['tensorflow', 'keras', 'saved_model']
            },
            'onnx': {
                'extensions': ['.onnx'],
                'magic_bytes': [b'\x08\x01'],
                'indicators': ['onnx', 'opset']
            },
            'diffusers': {
                'extensions': ['.bin', '.safetensors'],
                'magic_bytes': [],
                'indicators': ['diffusion', 'unet', 'vae', 'scheduler']
            },
            'transformers': {
                'extensions': ['.bin', '.safetensors'],
                'magic_bytes': [],
                'indicators': ['transformer', 'bert', 'gpt', 'clip']
            }
        }
        
        # MyCloset AI 8ë‹¨ê³„ íŒ¨í„´ (ë” ì •êµí•¨)
        self.step_patterns = {
            'step_01_human_parsing': {
                'patterns': [
                    r'human.*pars.*', r'graphonomy', r'schp', r'atr.*', r'lip.*',
                    r'parsing.*human', r'self.*correction.*human',
                    r'human.*segmentation', r'body.*parsing'
                ],
                'models': ['Graphonomy', 'Self-Correction-Human-Parsing', 'ATR', 'LIP'],
                'keywords': ['human', 'parsing', 'segmentation', 'body', 'person']
            },
            'step_02_pose_estimation': {
                'patterns': [
                    r'pose.*estimation', r'openpose', r'mediapipe.*pose', r'dwpose',
                    r'body.*pose', r'keypoint.*detection', r'skeleton.*detection',
                    r'pose.*net', r'human.*pose'
                ],
                'models': ['OpenPose', 'MediaPipe', 'DWPose', 'PoseNet'],
                'keywords': ['pose', 'keypoint', 'skeleton', 'joint', 'landmark']
            },
            'step_03_cloth_segmentation': {
                'patterns': [
                    r'cloth.*seg.*', r'u2net', r'sam.*', r'segment.*anything',
                    r'mask.*rcnn', r'deeplabv3', r'segmentation.*cloth',
                    r'garment.*seg.*', r'clothing.*mask'
                ],
                'models': ['U2Net', 'SAM', 'MaskRCNN', 'DeepLabV3'],
                'keywords': ['cloth', 'garment', 'clothing', 'mask', 'segment']
            },
            'step_04_geometric_matching': {
                'patterns': [
                    r'geometric.*match.*', r'gmm.*', r'tps.*', r'spatial.*transform',
                    r'warping.*grid', r'flow.*estimation', r'optical.*flow',
                    r'matching.*network'
                ],
                'models': ['GMM', 'TPS', 'FlowNet', 'PWCNet'],
                'keywords': ['geometric', 'matching', 'flow', 'transform', 'warp']
            },
            'step_05_cloth_warping': {
                'patterns': [
                    r'cloth.*warp.*', r'tom.*', r'viton.*warp', r'deformation',
                    r'elastic.*transform', r'thin.*plate.*spline', r'warping.*net',
                    r'garment.*warp.*'
                ],
                'models': ['TOM', 'VITON', 'TPS-Warp'],
                'keywords': ['warp', 'deformation', 'elastic', 'spline', 'transform']
            },
            'step_06_virtual_fitting': {
                'patterns': [
                    r'virtual.*fit.*', r'ootdiffusion', r'stable.*diffusion',
                    r'diffusion.*unet', r'try.*on', r'outfit.*diffusion',
                    r'viton.*hd', r'hr.*viton', r'virtual.*tryon'
                ],
                'models': ['OOTDiffusion', 'VITON-HD', 'HR-VITON', 'StableDiffusion'],
                'keywords': ['virtual', 'fitting', 'tryon', 'diffusion', 'generation']
            },
            'step_07_post_processing': {
                'patterns': [
                    r'post.*process.*', r'enhancement', r'super.*resolution',
                    r'sr.*net', r'esrgan', r'real.*esrgan', r'upscal.*',
                    r'denoise.*', r'refine.*', r'enhance.*'
                ],
                'models': ['ESRGAN', 'Real-ESRGAN', 'SRResNet', 'EDSR'],
                'keywords': ['enhancement', 'super', 'resolution', 'upscale', 'denoise']
            },
            'step_08_quality_assessment': {
                'patterns': [
                    r'quality.*assess.*', r'clip.*', r'aesthetic.*', r'scoring',
                    r'evaluation', r'metric.*', r'lpips', r'ssim', r'fid.*',
                    r'perceptual.*loss'
                ],
                'models': ['CLIP', 'LPIPS', 'FID', 'SSIM'],
                'keywords': ['quality', 'assessment', 'metric', 'evaluation', 'score']
            }
        }
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ íŒ¨í„´
        self.architecture_patterns = {
            'transformer': [r'transformer', r'bert', r'gpt', r'clip', r'vit'],
            'cnn': [r'resnet', r'vgg', r'inception', r'mobilenet', r'efficientnet'],
            'unet': [r'unet', r'u.*net', r'segmentation'],
            'gan': [r'gan', r'generator', r'discriminator'],
            'diffusion': [r'diffusion', r'ddpm', r'ddim', r'score'],
            'autoencoder': [r'vae', r'autoencoder', r'encoder.*decoder'],
            'detection': [r'yolo', r'rcnn', r'ssd', r'detection'],
            'pose': [r'pose', r'keypoint', r'landmark']
        }

    def _detect_conda_environments(self) -> Dict[str, Path]:
        """conda í™˜ê²½ ìë™ íƒì§€"""
        environments = {}
        
        try:
            # conda infoë¡œ í™˜ê²½ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            result = subprocess.run(
                ['conda', 'env', 'list', '--json'],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                env_data = json.loads(result.stdout)
                for env_path in env_data.get('envs', []):
                    env_name = Path(env_path).name
                    environments[env_name] = Path(env_path)
                    
        except (subprocess.SubprocessError, json.JSONDecodeError, FileNotFoundError):
            # conda ëª…ë ¹ì–´ ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ íƒì§€
            conda_bases = [
                Path.home() / "miniconda3" / "envs",
                Path.home() / "anaconda3" / "envs", 
                Path("/opt/anaconda3/envs"),
                Path("/opt/miniconda3/envs")
            ]
            
            for base in conda_bases:
                if base.exists():
                    for env_dir in base.iterdir():
                        if env_dir.is_dir() and (env_dir / "bin" / "python").exists():
                            environments[env_dir.name] = env_dir
        
        logger.info(f"ğŸ ë°œê²¬ëœ conda í™˜ê²½: {list(environments.keys())}")
        return environments

    def _init_database(self):
        """ìŠ¤ìº” ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        db_path = self.project_root / "model_scanner.db"
        self.db_connection = sqlite3.connect(str(db_path), check_same_thread=False)
        
        # í…Œì´ë¸” ìƒì„±
        cursor = self.db_connection.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS model_scans (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scan_date TEXT,
                model_path TEXT UNIQUE,
                model_name TEXT,
                size_gb REAL,
                framework TEXT,
                step_candidate TEXT,
                confidence REAL,
                checksum TEXT,
                is_valid BOOLEAN,
                metadata TEXT,
                last_seen TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scan_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scan_date TEXT,
                total_models INTEGER,
                total_size_gb REAL,
                scan_duration REAL,
                statistics TEXT
            )
        ''')
        
        self.db_connection.commit()

    def scan_comprehensive_system(self, organize: bool = False) -> List[ModelInfo]:
        """ì™„ì „í•œ ì‹œìŠ¤í…œ ìŠ¤ìº” ì‹¤í–‰"""
        logger.info("ğŸš€ ì™„ì „ ê³ ë„í™”ëœ AI ëª¨ë¸ ìŠ¤ìº” ì‹œì‘")
        logger.info("=" * 80)
        
        scan_start = time.time()
        
        # 1. ìŠ¤ìº” ê²½ë¡œ ìµœì í™” ìƒì„±
        scan_paths = self._generate_optimized_scan_paths()
        
        # 2. ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
        logger.info(f"ğŸ” {len(scan_paths)}ê°œ ìœ„ì¹˜ì—ì„œ ë³‘ë ¬ ìŠ¤ìº” ì‹œì‘...")
        all_files = self._parallel_file_discovery(scan_paths)
        
        if not all_files:
            logger.warning("âŒ AI ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 3. ëª¨ë¸ íŒŒì¼ ë¶„ì„
        logger.info(f"ğŸ§  {len(all_files):,}ê°œ íŒŒì¼ AI ë¶„ì„ ì¤‘...")
        self._analyze_models_advanced(all_files)
        
        # 4. ê³ ê¸‰ í›„ì²˜ë¦¬
        self._post_process_results()
        
        # 5. ìë™ ì •ë¦¬ (ì˜µì…˜)
        if organize:
            self._auto_organize_models()
        
        # 6. ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
        scan_duration = time.time() - scan_start
        self._save_scan_results(scan_duration)
        self._print_comprehensive_report(scan_duration)
        
        return self.found_models

    def _generate_optimized_scan_paths(self) -> List[Path]:
        """ìµœì í™”ëœ ìŠ¤ìº” ê²½ë¡œ ìƒì„± (conda ìš°ì„ )"""
        paths = []
        
        # 1. conda í™˜ê²½ ìš°ì„  (ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„)
        if self.config.conda_priority:
            for env_name, env_path in self.conda_environments.items():
                conda_paths = [
                    env_path / "lib" / "python3.11" / "site-packages",
                    env_path / "lib" / "python3.10" / "site-packages", 
                    env_path / "lib" / "python3.9" / "site-packages",
                    env_path / "share",
                    env_path / "models",
                    env_path / "checkpoints"
                ]
                
                for path in conda_paths:
                    if self._is_accessible_path(path):
                        paths.append(path)
        
        # 2. í”„ë¡œì íŠ¸ ê²½ë¡œ (ë‘ ë²ˆì§¸ ìš°ì„ ìˆœìœ„)
        project_paths = [
            self.project_root / "backend" / "ai_models",
            self.project_root / "ai_models",
            self.project_root / "models",
            self.project_root / "checkpoints",
            self.project_root / "weights"
        ]
        
        # 3. ì‚¬ìš©ì ê²½ë¡œ
        home = Path.home()
        user_paths = [
            home / "Downloads",
            home / "Documents" / "AI_Models",
            home / "Desktop",
            home / ".cache" / "huggingface",
            home / ".cache" / "torch", 
            home / ".cache" / "diffusers",
            home / ".cache" / "transformers",
            home / ".local" / "lib",
            home / ".local" / "share"
        ]
        
        # 4. ì‹œìŠ¤í…œ ê²½ë¡œ
        system_paths = []
        system = platform.system().lower()
        
        if system == "darwin":  # macOS
            system_paths = [
                Path("/opt/homebrew/lib"),
                Path("/usr/local/lib"),
                Path("/opt"),
                Path("/Applications") if self.config.deep_scan else None
            ]
        elif system == "linux":
            system_paths = [
                Path("/opt"),
                Path("/usr/local/lib"),
                Path("/usr/share"),
                Path("/var/lib") if self.config.deep_scan else None
            ]
        else:  # Windows
            system_paths = [
                Path("C:/Program Files"),
                home / "AppData"
            ]
        
        # ê²½ë¡œ í†µí•© ë° í•„í„°ë§
        all_paths = project_paths + user_paths + [p for p in system_paths if p]
        
        if self.config.conda_priority:
            all_paths = paths + all_paths  # conda ê²½ë¡œë¥¼ ì•ì—
        else:
            all_paths = all_paths + paths
        
        # ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œë§Œ ë°˜í™˜
        final_paths = []
        for path in all_paths:
            if self._is_accessible_path(path):
                final_paths.append(path)
        
        # ì¤‘ë³µ ì œê±° (ë¶€ëª¨-ìì‹ ê´€ê³„ í™•ì¸)
        return self._remove_duplicate_paths(final_paths)

    def _is_accessible_path(self, path: Path) -> bool:
        """ê²½ë¡œ ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸ (ê¶Œí•œ ì•ˆì „ì„±)"""
        try:
            if not path.exists():
                return False
            
            # ì½ê¸° ê¶Œí•œ í™•ì¸
            if not os.access(path, os.R_OK):
                return False
            
            # ë³´í˜¸ëœ ì‹œìŠ¤í…œ ê²½ë¡œ ì œì™¸
            path_str = str(path).lower()
            protected_patterns = [
                '/system/', '/private/var/db', '/dev/', '/proc/',
                'keychain', 'security', 'loginwindow'
            ]
            
            return not any(pattern in path_str for pattern in protected_patterns)
            
        except (PermissionError, OSError):
            return False

    def _remove_duplicate_paths(self, paths: List[Path]) -> List[Path]:
        """ì¤‘ë³µ ê²½ë¡œ ì œê±° (ë¶€ëª¨-ìì‹ ê´€ê³„ ê³ ë ¤)"""
        unique_paths = []
        
        # ê²½ë¡œ ê¸¸ì´ìˆœ ì •ë ¬ (ì§§ì€ ê²ƒë¶€í„°)
        sorted_paths = sorted(set(paths), key=lambda p: len(str(p)))
        
        for path in sorted_paths:
            is_child = False
            for existing in unique_paths:
                try:
                    if path != existing and path.is_relative_to(existing):
                        is_child = True
                        break
                except (ValueError, OSError):
                    continue
            
            if not is_child:
                unique_paths.append(path)
        
        return unique_paths

    def _parallel_file_discovery(self, scan_paths: List[Path]) -> List[Path]:
        """ë³‘ë ¬ íŒŒì¼ ë°œê²¬"""
        all_files = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ê° ê²½ë¡œë³„ ìŠ¤ìº” ì‘ì—… ì œì¶œ
            future_to_path = {
                executor.submit(self._scan_path_advanced, path): path
                for path in scan_paths
            }
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if HAS_TQDM:
                progress = tqdm(
                    as_completed(future_to_path), 
                    total=len(future_to_path),
                    desc="ê²½ë¡œ ìŠ¤ìº”"
                )
            else:
                progress = as_completed(future_to_path)
            
            for future in progress:
                path = future_to_path[future]
                try:
                    files = future.result(timeout=120)  # 2ë¶„ ì œí•œ
                    if files:
                        all_files.extend(files)
                        self.scan_locations[str(path)] = [str(f) for f in files]
                        
                        total_size_mb = sum(
                            f.stat().st_size for f in files if f.exists()
                        ) / (1024 * 1024)
                        
                        logger.info(f"âœ… {path}: {len(files)}ê°œ íŒŒì¼ ({total_size_mb:.1f}MB)")
                        
                except Exception as e:
                    error_msg = f"ìŠ¤ìº” ì‹¤íŒ¨ {path}: {e}"
                    self.errors.append(error_msg)
                    logger.warning(f"âš ï¸ {error_msg}")
        
        return all_files

    def _scan_path_advanced(self, path: Path) -> List[Path]:
        """ê³ ê¸‰ ê²½ë¡œ ìŠ¤ìº” (ìµœì í™”)"""
        found_files = []
        
        try:
            if not self._is_accessible_path(path):
                return found_files
            
            # ì‹œìŠ¤í…œë³„ ìµœì í™”ëœ ìŠ¤ìº”
            if platform.system() != "Windows" and shutil.which('find'):
                found_files = self._unix_find_optimized(path)
            else:
                found_files = self._python_scan_optimized(path)
                
        except Exception as e:
            logger.warning(f"ê²½ë¡œ ìŠ¤ìº” ì˜¤ë¥˜ {path}: {e}")
        
        return found_files

    def _unix_find_optimized(self, path: Path) -> List[Path]:
        """Unix find ëª…ë ¹ì–´ ìµœì í™”"""
        found_files = []
        
        try:
            # íŒ¨í„´ ê¸°ë°˜ find ëª…ë ¹ì–´ êµ¬ì„±
            patterns = []
            for pattern in self.config.include_patterns:
                patterns.extend(['-name', f"'{pattern}'"])
            
            if patterns:
                patterns = patterns[:-1] + ['-o'] + patterns[-1:]  # OR ì¡°ê±´
            
            cmd = [
                'find', str(path),
                '-type', 'f',
                '(', *patterns, ')',
                '-size', f'+{int(self.config.min_size_mb)}M',
                '-not', '-path', '*/.*',  # ìˆ¨ê¹€ í´ë” ì œì™¸
                '-not', '-path', '*/__pycache__/*',
                '-not', '-path', '*/node_modules/*',
                '-maxdepth', str(self.config.max_depth)
            ]
            
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=60, check=False
            )
            
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if line and not self._should_exclude_file(line):
                        file_path = Path(line)
                        if file_path.exists():
                            found_files.append(file_path)
                            
        except subprocess.SubprocessError:
            # find ì‹¤íŒ¨ ì‹œ Python ë°©ì‹ìœ¼ë¡œ í´ë°±
            found_files = self._python_scan_optimized(path)
            
        return found_files

    def _python_scan_optimized(self, path: Path) -> List[Path]:
        """Python ê¸°ë°˜ ìµœì í™” ìŠ¤ìº”"""
        found_files = []
        
        try:
            for pattern in self.config.include_patterns:
                glob_pattern = f"**/{pattern}"
                
                for file_path in path.rglob(pattern):
                    if (file_path.is_file() and 
                        not self._should_exclude_file(str(file_path)) and
                        self._check_file_size(file_path)):
                        found_files.append(file_path)
                        
        except Exception as e:
            logger.warning(f"Python ìŠ¤ìº” ì‹¤íŒ¨ {path}: {e}")
        
        return found_files

    def _should_exclude_file(self, file_path: str) -> bool:
        """íŒŒì¼ ì œì™¸ ì—¬ë¶€ íŒë‹¨"""
        path_lower = file_path.lower()
        
        for pattern in self.config.exclude_patterns:
            if pattern in path_lower:
                return True
        
        return False

    def _check_file_size(self, file_path: Path) -> bool:
        """íŒŒì¼ í¬ê¸° ê²€ì‚¬"""
        try:
            size_bytes = file_path.stat().st_size
            size_mb = size_bytes / (1024 * 1024)
            size_gb = size_mb / 1024
            
            return (self.config.min_size_mb <= size_mb <= 
                   self.config.max_size_gb * 1024)
        except OSError:
            return False

    def _analyze_models_advanced(self, model_files: List[Path]):
        """ê³ ê¸‰ ëª¨ë¸ ë¶„ì„ (ë³‘ë ¬ + AI ê¸°ë°˜)"""
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ë¶„ì„ ì‘ì—… ì œì¶œ
            futures = [
                executor.submit(self._analyze_single_model_advanced, file_path)
                for file_path in model_files
            ]
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if HAS_TQDM:
                progress = tqdm(
                    as_completed(futures),
                    total=len(futures), 
                    desc="AI ëª¨ë¸ ë¶„ì„"
                )
            else:
                progress = as_completed(futures)
            
            for future in progress:
                try:
                    model_info = future.result()
                    if model_info and model_info.is_valid:
                        self.found_models.append(model_info)
                        
                except Exception as e:
                    self.errors.append(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")

    def _analyze_single_model_advanced(self, file_path: Path) -> Optional[ModelInfo]:
        """ë‹¨ì¼ ëª¨ë¸ ê³ ê¸‰ ë¶„ì„"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            stat_info = file_path.stat()
            size_bytes = stat_info.st_size
            size_mb = size_bytes / (1024 * 1024)
            size_gb = size_mb / 1024
            
            # ì²´í¬ì„¬ ê³„ì‚° (ìµœì í™”)
            checksums = self._calculate_checksums_optimized(file_path, size_mb)
            
            # í”„ë ˆì„ì›Œí¬ ë¶„ë¥˜ (ê³ ë„í™”)
            framework = self._classify_framework_advanced(file_path)
            
            # ëª¨ë¸ íƒ€ì… ë° ë‹¨ê³„ ë¶„ë¥˜
            model_type = self._classify_model_type_advanced(file_path)
            step_candidate, confidence = self._classify_step_advanced(file_path)
            architecture = self._classify_architecture(file_path)
            
            # í™˜ê²½ ì •ë³´
            conda_info = self._check_conda_environment(file_path)
            is_in_project = self._is_in_project(file_path)
            
            # ê²€ì¦ ë° ë©”íƒ€ë°ì´í„°
            validation_result = self._validate_model_advanced(file_path, framework)
            metadata = self._extract_metadata_advanced(file_path, framework)
            
            # ê´€ë ¨ íŒŒì¼ íƒì§€
            companion_files = self._find_companion_files(file_path)
            
            # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
            importance_score = self._calculate_importance_score(
                file_path, size_gb, framework, step_candidate, confidence
            )
            
            return ModelInfo(
                # ê¸°ë³¸ ì •ë³´
                name=file_path.name,
                path=str(file_path),
                absolute_path=str(file_path.absolute()),
                size_bytes=size_bytes,
                size_mb=size_mb,
                size_gb=size_gb,
                
                # íŒŒì¼ ì •ë³´  
                extension=file_path.suffix.lower(),
                mime_type=mimetypes.guess_type(str(file_path))[0] or 'unknown',
                created_time=datetime.fromtimestamp(stat_info.st_ctime),
                modified_time=datetime.fromtimestamp(stat_info.st_mtime),
                access_time=datetime.fromtimestamp(stat_info.st_atime),
                checksum_md5=checksums['md5'],
                checksum_sha256=checksums['sha256'],
                
                # AI ëª¨ë¸ ë¶„ë¥˜
                framework=framework,
                model_type=model_type,
                step_candidate=step_candidate,
                confidence=confidence,
                architecture=architecture,
                
                # ìœ„ì¹˜ ë° í™˜ê²½
                is_in_project=is_in_project,
                is_in_conda=conda_info['is_conda'],
                conda_env_name=conda_info['env_name'],
                environment_path=conda_info['env_path'],
                parent_directory=file_path.parent.name,
                
                # ìƒíƒœ ë° ê²€ì¦
                is_valid=validation_result['is_valid'],
                is_complete=validation_result['is_complete'],
                is_corrupted=validation_result['is_corrupted'],
                validation_errors=validation_result['errors'],
                
                # ê´€ê³„ì„±
                companion_files=companion_files,
                related_models=[],  # í›„ì²˜ë¦¬ì—ì„œ ì„¤ì •
                duplicate_of=None,  # í›„ì²˜ë¦¬ì—ì„œ ì„¤ì •
                
                # ê³ ê¸‰ ë©”íƒ€ë°ì´í„°
                metadata=metadata,
                importance_score=importance_score
            )
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return None

    def _calculate_checksums_optimized(self, file_path: Path, size_mb: float) -> Dict[str, str]:
        """ìµœì í™”ëœ ì²´í¬ì„¬ ê³„ì‚°"""
        checksums = {'md5': 'unknown', 'sha256': 'unknown'}
        
        try:
            md5_hasher = hashlib.md5()
            sha256_hasher = hashlib.sha256()
            
            with open(file_path, 'rb') as f:
                if size_mb > 100:  # 100MB ì´ìƒì€ ìƒ˜í”Œë§
                    # ì‹œì‘ ë¶€ë¶„
                    chunk = f.read(1024 * 1024)  # 1MB
                    if chunk:
                        md5_hasher.update(chunk)
                        sha256_hasher.update(chunk)
                    
                    # ì¤‘ê°„ ë¶€ë¶„
                    try:
                        f.seek(int(size_mb * 1024 * 512))  # ì¤‘ê°„
                        chunk = f.read(1024 * 1024)
                        if chunk:
                            md5_hasher.update(chunk)
                            sha256_hasher.update(chunk)
                    except:
                        pass
                    
                    # ë ë¶€ë¶„
                    try:
                        f.seek(-1024 * 1024, 2)  # ëì—ì„œ 1MB
                        chunk = f.read(1024 * 1024)
                        if chunk:
                            md5_hasher.update(chunk)
                            sha256_hasher.update(chunk)
                    except:
                        pass
                else:
                    # ì‘ì€ íŒŒì¼ì€ ì „ì²´ í•´ì‹œ
                    while True:
                        chunk = f.read(8192)
                        if not chunk:
                            break
                        md5_hasher.update(chunk)
                        sha256_hasher.update(chunk)
            
            checksums['md5'] = md5_hasher.hexdigest()
            checksums['sha256'] = sha256_hasher.hexdigest()[:32]  # ì²˜ìŒ 32ìë§Œ
            
        except Exception as e:
            logger.warning(f"ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
        
        return checksums

    def _classify_framework_advanced(self, file_path: Path) -> str:
        """ê³ ê¸‰ í”„ë ˆì„ì›Œí¬ ë¶„ë¥˜"""
        extension = file_path.suffix.lower()
        path_str = str(file_path).lower()
        
        # í™•ì¥ì ê¸°ë°˜ 1ì°¨ ë¶„ë¥˜
        for framework, info in self.framework_patterns.items():
            if extension in info['extensions']:
                # ì¶”ê°€ ê²€ì¦
                if info['indicators']:
                    for indicator in info['indicators']:
                        if indicator in path_str:
                            return framework
                return framework
        
        # ë§¤ì§ ë°”ì´íŠ¸ ê²€ì¦ (ì†Œê·œëª¨ íŒŒì¼ë§Œ)
        try:
            if file_path.stat().st_size < 100 * 1024 * 1024:  # 100MB ë¯¸ë§Œ
                with open(file_path, 'rb') as f:
                    header = f.read(1024)
                    
                for framework, info in self.framework_patterns.items():
                    for magic in info['magic_bytes']:
                        if magic in header:
                            return framework
        except:
            pass
        
        return 'unknown'

    def _classify_model_type_advanced(self, file_path: Path) -> str:
        """ê³ ê¸‰ ëª¨ë¸ íƒ€ì… ë¶„ë¥˜"""
        path_str = str(file_path).lower()
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ê¸°ë°˜ ë¶„ë¥˜
        path_parts = [part.lower() for part in file_path.parts]
        
        type_indicators = {
            'diffusion_model': ['diffusion', 'stable', 'ootd', 'unet'],
            'clip_model': ['clip', 'vit', 'vision', 'transformer'],
            'pose_model': ['pose', 'openpose', 'dwpose', 'keypoint'],
            'segmentation_model': ['segment', 'u2net', 'mask', 'sam'],
            'parsing_model': ['parsing', 'human', 'atr', 'schp', 'graphonomy'],
            'warping_model': ['warp', 'tom', 'tps', 'flow', 'matching'],
            'checkpoint': ['checkpoint', 'ckpt', 'epoch', 'step'],
            'config_file': ['config', 'tokenizer', 'vocab']
        }
        
        for model_type, indicators in type_indicators.items():
            for indicator in indicators:
                if any(indicator in part for part in path_parts):
                    return model_type
                if indicator in file_path.name.lower():
                    return model_type
        
        return 'unknown'

    def _classify_step_advanced(self, file_path: Path) -> Tuple[str, float]:
        """MyCloset AI 8ë‹¨ê³„ ê³ ê¸‰ ë¶„ë¥˜"""
        path_str = str(file_path).lower()
        name_str = file_path.name.lower()
        parent_str = file_path.parent.name.lower()
        
        best_step = "unknown"
        best_confidence = 0.0
        
        for step_name, step_info in self.step_patterns.items():
            confidence = 0.0
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in step_info['patterns']:
                try:
                    if re.search(pattern, path_str):
                        confidence = max(confidence, 0.9)
                    elif re.search(pattern, name_str):
                        confidence = max(confidence, 0.8)
                    elif re.search(pattern, parent_str):
                        confidence = max(confidence, 0.7)
                except re.error:
                    # ì •ê·œì‹ ì˜¤ë¥˜ ì‹œ ë¬¸ìì—´ ê²€ìƒ‰
                    clean_pattern = pattern.replace(r'\.*', '').replace('.*', '')
                    if clean_pattern in path_str:
                        confidence = max(confidence, 0.6)
            
            # ëª¨ë¸ëª… ë§¤ì¹­
            for model in step_info['models']:
                if model.lower() in path_str:
                    confidence = max(confidence, 0.85)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_matches = sum(1 for kw in step_info['keywords'] if kw in path_str)
            if keyword_matches > 0:
                confidence = max(confidence, 0.5 + (keyword_matches * 0.1))
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_step = step_name
        
        return best_step, best_confidence

    def _classify_architecture(self, file_path: Path) -> str:
        """ì•„í‚¤í…ì²˜ ë¶„ë¥˜"""
        path_str = str(file_path).lower()
        
        for arch_type, patterns in self.architecture_patterns.items():
            for pattern in patterns:
                if re.search(pattern, path_str):
                    return arch_type
        
        return 'unknown'

    def _check_conda_environment(self, file_path: Path) -> Dict[str, Any]:
        """conda í™˜ê²½ í™•ì¸"""
        result = {
            'is_conda': False,
            'env_name': None,
            'env_path': None
        }
        
        for env_name, env_path in self.conda_environments.items():
            try:
                if file_path.is_relative_to(env_path):
                    result['is_conda'] = True
                    result['env_name'] = env_name
                    result['env_path'] = str(env_path)
                    break
            except ValueError:
                continue
        
        return result

    def _is_in_project(self, file_path: Path) -> bool:
        """í”„ë¡œì íŠ¸ ë‚´ë¶€ ì—¬ë¶€ í™•ì¸"""
        try:
            return file_path.is_relative_to(self.project_root)
        except ValueError:
            return False

    def _validate_model_advanced(self, file_path: Path, framework: str) -> Dict[str, Any]:
        """ê³ ê¸‰ ëª¨ë¸ ê²€ì¦"""
        result = {
            'is_valid': False,
            'is_complete': False,
            'is_corrupted': False,
            'errors': []
        }
        
        try:
            # ê¸°ë³¸ íŒŒì¼ ê²€ì¦
            if not file_path.exists():
                result['errors'].append("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                return result
            
            size = file_path.stat().st_size
            if size == 0:
                result['errors'].append("ë¹ˆ íŒŒì¼")
                return result
            
            # í”„ë ˆì„ì›Œí¬ë³„ ê²€ì¦
            if framework == 'pytorch':
                result.update(self._validate_pytorch_model(file_path))
            elif framework == 'safetensors':
                result.update(self._validate_safetensors_model(file_path))
            elif framework == 'tensorflow':
                result.update(self._validate_tensorflow_model(file_path))
            elif framework == 'onnx':
                result.update(self._validate_onnx_model(file_path))
            else:
                result['is_valid'] = True
                result['is_complete'] = True
            
        except Exception as e:
            result['errors'].append(f"ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        return result

    def _validate_pytorch_model(self, file_path: Path) -> Dict[str, Any]:
        """PyTorch ëª¨ë¸ ê²€ì¦"""
        result = {'is_valid': False, 'is_complete': False, 'is_corrupted': False, 'errors': []}
        
        try:
            with open(file_path, 'rb') as f:
                header = f.read(1024)
            
            # PyTorch ë§¤ì§ ë°”ì´íŠ¸ í™•ì¸
            if b'PK' in header or b'\x80' in header:
                result['is_valid'] = True
                result['is_complete'] = True
            else:
                result['errors'].append("PyTorch í˜•ì‹ì´ ì•„ë‹˜")
                
        except Exception as e:
            result['errors'].append(f"PyTorch ê²€ì¦ ì‹¤íŒ¨: {e}")
            result['is_corrupted'] = True
        
        return result

    def _validate_safetensors_model(self, file_path: Path) -> Dict[str, Any]:
        """Safetensors ëª¨ë¸ ê²€ì¦"""
        result = {'is_valid': False, 'is_complete': False, 'is_corrupted': False, 'errors': []}
        
        try:
            with open(file_path, 'rb') as f:
                header = f.read(1024).decode('utf-8', errors='ignore')
            
            if '{' in header and '"' in header:
                result['is_valid'] = True
                result['is_complete'] = True
            else:
                result['errors'].append("Safetensors í˜•ì‹ì´ ì•„ë‹˜")
                
        except Exception as e:
            result['errors'].append(f"Safetensors ê²€ì¦ ì‹¤íŒ¨: {e}")
            result['is_corrupted'] = True
        
        return result

    def _validate_tensorflow_model(self, file_path: Path) -> Dict[str, Any]:
        """TensorFlow ëª¨ë¸ ê²€ì¦"""
        result = {'is_valid': False, 'is_complete': False, 'is_corrupted': False, 'errors': []}
        
        try:
            extension = file_path.suffix.lower()
            
            with open(file_path, 'rb') as f:
                header = f.read(100)
            
            if extension == '.h5' and header.startswith(b'\x89HDF'):
                result['is_valid'] = True
                result['is_complete'] = True
            elif extension == '.pb' and len(header) > 10:
                result['is_valid'] = True
                result['is_complete'] = True
            else:
                result['errors'].append("TensorFlow í˜•ì‹ í™•ì¸ ì‹¤íŒ¨")
                
        except Exception as e:
            result['errors'].append(f"TensorFlow ê²€ì¦ ì‹¤íŒ¨: {e}")
            result['is_corrupted'] = True
        
        return result

    def _validate_onnx_model(self, file_path: Path) -> Dict[str, Any]:
        """ONNX ëª¨ë¸ ê²€ì¦"""
        result = {'is_valid': False, 'is_complete': False, 'is_corrupted': False, 'errors': []}
        
        try:
            with open(file_path, 'rb') as f:
                header = f.read(100)
            
            if b'onnx' in header.lower() or len(header) > 50:
                result['is_valid'] = True
                result['is_complete'] = True
            else:
                result['errors'].append("ONNX í˜•ì‹ì´ ì•„ë‹˜")
                
        except Exception as e:
            result['errors'].append(f"ONNX ê²€ì¦ ì‹¤íŒ¨: {e}")
            result['is_corrupted'] = True
        
        return result

    def _extract_metadata_advanced(self, file_path: Path, framework: str) -> ModelMetadata:
        """ê³ ê¸‰ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = ModelMetadata()
        
        try:
            # ë™ë°˜ íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            parent_dir = file_path.parent
            
            # config.json í™•ì¸
            config_path = parent_dir / "config.json"
            if config_path.exists():
                metadata.has_config = True
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config_data = json.load(f)
                    
                    metadata.architecture = config_data.get('model_type', 'unknown')
                    metadata.framework_version = config_data.get('transformers_version', 'unknown')
                    
                    if 'base_model' in config_data:
                        metadata.base_model = config_data['base_model']
                        metadata.is_fine_tuned = True
                    
                except:
                    pass
            
            # tokenizer í™•ì¸
            tokenizer_files = ['tokenizer.json', 'tokenizer_config.json', 'vocab.txt']
            metadata.has_tokenizer = any((parent_dir / tf).exists() for tf in tokenizer_files)
            
            # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
            name_lower = file_path.name.lower()
            
            # ì •ë°€ë„ ì¶”ì¶œ
            if 'fp16' in name_lower:
                metadata.precision = 'fp16'
            elif 'fp32' in name_lower:
                metadata.precision = 'fp32'
            elif 'int8' in name_lower:
                metadata.precision = 'int8'
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = []
            if 'fine' in name_lower and 'tuned' in name_lower:
                tags.append('fine-tuned')
            if 'checkpoint' in name_lower:
                tags.append('checkpoint')
            if 'epoch' in name_lower:
                tags.append('training')
            
            metadata.tags = tags
            
        except Exception as e:
            logger.warning(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
        
        return metadata

    def _find_companion_files(self, file_path: Path) -> List[str]:
        """ë™ë°˜ íŒŒì¼ ì°¾ê¸°"""
        companions = []
        parent_dir = file_path.parent
        
        companion_patterns = [
            'config.json', 'config.yaml', 'model_config.json',
            'tokenizer.json', 'tokenizer_config.json', 'vocab.txt',
            'pytorch_model.bin', 'model.safetensors',
            'scheduler_config.json', 'unet_config.json'
        ]
        
        for pattern in companion_patterns:
            companion_path = parent_dir / pattern
            if companion_path.exists() and companion_path != file_path:
                companions.append(str(companion_path))
        
        return companions

    def _calculate_importance_score(
        self, file_path: Path, size_gb: float, framework: str, 
        step_candidate: str, confidence: float
    ) -> float:
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ê¸°ë³¸ ì ìˆ˜ (í¬ê¸° ê¸°ë°˜)
        score += min(size_gb * 10, 50)  # ìµœëŒ€ 50ì 
        
        # ì‹ ë¢°ë„ ì ìˆ˜
        score += confidence * 30  # ìµœëŒ€ 30ì 
        
        # í”„ë ˆì„ì›Œí¬ ë³´ë„ˆìŠ¤
        framework_bonus = {
            'pytorch': 10, 'safetensors': 8, 'diffusers': 15,
            'transformers': 12, 'onnx': 5
        }
        score += framework_bonus.get(framework, 0)
        
        # Step ë³´ë„ˆìŠ¤ (MyCloset AI íŠ¹í™”)
        if step_candidate != 'unknown':
            score += 20
        
        # í”„ë¡œì íŠ¸ ë‚´ë¶€ ë³´ë„ˆìŠ¤
        if self._is_in_project(file_path):
            score += 15
        
        # conda í™˜ê²½ ë³´ë„ˆìŠ¤
        if any(file_path.is_relative_to(env_path) 
               for env_path in self.conda_environments.values()):
            score += 10
        
        return min(score, 100.0)  # ìµœëŒ€ 100ì 

    def _post_process_results(self):
        """ê³ ê¸‰ í›„ì²˜ë¦¬"""
        logger.info("ğŸ”„ ê²°ê³¼ í›„ì²˜ë¦¬ ì¤‘...")
        
        # 1. ì¤‘ë³µ íƒì§€
        self._detect_duplicates_advanced()
        
        # 2. ê´€ë ¨ ëª¨ë¸ ì—°ê²°
        self._link_related_models()
        
        # 3. ì¤‘ìš”ë„ ì¬ê³„ì‚°
        self._recalculate_importance()
        
        # 4. ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        self._update_database()

    def _detect_duplicates_advanced(self):
        """ê³ ê¸‰ ì¤‘ë³µ íƒì§€"""
        # MD5 ê¸°ë°˜ ê·¸ë£¹í™”
        md5_groups = defaultdict(list)
        for model in self.found_models:
            if model.checksum_md5 != 'unknown':
                md5_groups[model.checksum_md5].append(model)
        
        # ì¤‘ë³µ ê·¸ë£¹ ì²˜ë¦¬
        for checksum, models in md5_groups.items():
            if len(models) > 1:
                # ê°€ì¥ ì¤‘ìš”í•œ ëª¨ë¸ì„ ì›ë³¸ìœ¼ë¡œ ì„¤ì •
                primary = max(models, key=lambda m: m.importance_score)
                
                for model in models:
                    if model != primary:
                        model.duplicate_of = primary.path
                
                self.duplicates[checksum] = models

    def _link_related_models(self):
        """ê´€ë ¨ ëª¨ë¸ ì—°ê²°"""
        # ê°™ì€ ë””ë ‰í† ë¦¬ì˜ ëª¨ë¸ë“¤ ì—°ê²°
        dir_groups = defaultdict(list)
        for model in self.found_models:
            dir_groups[model.parent_directory].append(model)
        
        for models in dir_groups.values():
            if len(models) > 1:
                for model in models:
                    model.related_models = [
                        m.path for m in models if m != model
                    ]

    def _recalculate_importance(self):
        """ì¤‘ìš”ë„ ì¬ê³„ì‚° (ê´€ê³„ì„± ê³ ë ¤)"""
        for model in self.found_models:
            bonus = 0
            
            # ë™ë°˜ íŒŒì¼ ë³´ë„ˆìŠ¤
            if model.companion_files:
                bonus += len(model.companion_files) * 2
            
            # ê´€ë ¨ ëª¨ë¸ ë³´ë„ˆìŠ¤
            if model.related_models:
                bonus += len(model.related_models)
            
            # ìµœì‹ ì„± ë³´ë„ˆìŠ¤
            days_old = (datetime.now() - model.modified_time).days
            if days_old < 30:
                bonus += 5
            elif days_old < 90:
                bonus += 3
            
            model.importance_score = min(model.importance_score + bonus, 100.0)

    def _update_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        cursor = self.db_connection.cursor()
        
        for model in self.found_models:
            cursor.execute('''
                INSERT OR REPLACE INTO model_scans 
                (scan_date, model_path, model_name, size_gb, framework, 
                 step_candidate, confidence, checksum, is_valid, metadata, last_seen)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                model.path,
                model.name,
                model.size_gb,
                model.framework,
                model.step_candidate,
                model.confidence,
                model.checksum_md5,
                model.is_valid,
                json.dumps(asdict(model.metadata)),
                datetime.now().isoformat()
            ))
        
        self.db_connection.commit()

    def _auto_organize_models(self):
        """ìë™ ëª¨ë¸ ì •ë¦¬"""
        logger.info("ğŸ“ ìë™ ëª¨ë¸ ì •ë¦¬ ì‹œì‘...")
        
        # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
        organized_dir = self.project_root / "backend" / "ai_models" / "organized"
        organized_dir.mkdir(parents=True, exist_ok=True)
        
        # Stepë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        for step_name in self.step_patterns.keys():
            step_dir = organized_dir / step_name.replace('step_', '').replace('_', '-')
            step_dir.mkdir(exist_ok=True)
        
        # ëª¨ë¸ ì´ë™ (ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ)
        moved_count = 0
        for model in self.found_models:
            if (model.confidence > 0.7 and 
                not model.is_in_project and 
                model.step_candidate != 'unknown'):
                
                try:
                    step_name = model.step_candidate.replace('step_', '').replace('_', '-')
                    target_dir = organized_dir / step_name
                    target_path = target_dir / model.name
                    
                    if not target_path.exists():
                        shutil.copy2(model.path, target_path)
                        logger.info(f"âœ… ì´ë™: {model.name} â†’ {step_name}")
                        moved_count += 1
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ì´ë™ ì‹¤íŒ¨ {model.name}: {e}")
        
        logger.info(f"ğŸ“¦ {moved_count}ê°œ ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")

    def _save_scan_results(self, scan_duration: float):
        """ìŠ¤ìº” ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í†µê³„ ê³„ì‚°
        stats = self._calculate_statistics(scan_duration)
        
        # JSON ê²°ê³¼ ì €ì¥
        result_data = {
            "scan_info": {
                "timestamp": datetime.now().isoformat(),
                "duration": scan_duration,
                "system": platform.system(),
                "machine": platform.machine(),
                "python_version": platform.python_version(),
                "conda_environments": list(self.conda_environments.keys()),
                "current_conda_env": self.current_conda_env
            },
            "statistics": asdict(stats),
            "models": [asdict(model) for model in self.found_models],
            "duplicates": {
                checksum: [asdict(model) for model in models]
                for checksum, models in self.duplicates.items()
            },
            "scan_locations": self.scan_locations,
            "errors": self.errors,
            "warnings": self.warnings
        }
        
        # íŒŒì¼ ì €ì¥
        output_file = self.project_root / f"model_scan_complete_{timestamp}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ’¾ ìŠ¤ìº” ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ìŠ¤ìº” ì´ë ¥ ì €ì¥
        cursor = self.db_connection.cursor()
        cursor.execute('''
            INSERT INTO scan_history 
            (scan_date, total_models, total_size_gb, scan_duration, statistics)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            datetime.now().isoformat(),
            len(self.found_models),
            stats.total_size_gb,
            scan_duration,
            json.dumps(asdict(stats))
        ))
        self.db_connection.commit()

    def _calculate_statistics(self, scan_duration: float) -> ScanStatistics:
        """ìƒì„¸ í†µê³„ ê³„ì‚°"""
        stats = ScanStatistics()
        
        # ê¸°ë³¸ í†µê³„
        stats.total_files_scanned = sum(len(files) for files in self.scan_locations.values())
        stats.models_found = len(self.found_models)
        stats.total_size_bytes = sum(model.size_bytes for model in self.found_models)
        stats.total_size_gb = stats.total_size_bytes / (1024**3)
        stats.scan_duration = scan_duration
        
        # ìœ„ì¹˜ í†µê³„
        stats.locations_scanned = len(self.scan_locations)
        stats.conda_models = sum(1 for model in self.found_models if model.is_in_conda)
        stats.project_models = sum(1 for model in self.found_models if model.is_in_project)
        stats.system_models = stats.models_found - stats.conda_models - stats.project_models
        
        # í’ˆì§ˆ í†µê³„
        stats.valid_models = sum(1 for model in self.found_models if model.is_valid)
        stats.corrupted_models = sum(1 for model in self.found_models if model.is_corrupted)
        stats.duplicate_groups = len(self.duplicates)
        stats.unique_models = stats.models_found - sum(
            len(models) - 1 for models in self.duplicates.values()
        )
        
        # ë¶„í¬ í†µê³„
        for model in self.found_models:
            # í”„ë ˆì„ì›Œí¬ ë¶„í¬
            fw = model.framework
            stats.framework_distribution[fw] = stats.framework_distribution.get(fw, 0) + 1
            
            # íƒ€ì… ë¶„í¬
            mt = model.model_type
            stats.type_distribution[mt] = stats.type_distribution.get(mt, 0) + 1
            
            # Step ë¶„í¬ (ì‹ ë¢°ë„ 0.5+ ë§Œ)
            if model.confidence >= 0.5:
                step = model.step_candidate
                stats.step_distribution[step] = stats.step_distribution.get(step, 0) + 1
        
        # ì„±ëŠ¥ í†µê³„
        stats.errors_count = len(self.errors)
        stats.warnings_count = len(self.warnings)
        if scan_duration > 0:
            stats.processing_speed_files_per_sec = stats.total_files_scanned / scan_duration
        
        return stats

    def _print_comprehensive_report(self, scan_duration: float):
        """ì™„ì „í•œ ìŠ¤ìº” ë³´ê³ ì„œ ì¶œë ¥"""
        stats = self._calculate_statistics(scan_duration)
        
        print("\n" + "=" * 100)
        print("ğŸ¯ MyCloset AI - ì™„ì „ ê³ ë„í™”ëœ AI ëª¨ë¸ ìŠ¤ìº” ê²°ê³¼")
        print("=" * 100)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"ğŸ• ìŠ¤ìº” ì‹œê°„: {scan_duration:.1f}ì´ˆ ({stats.processing_speed_files_per_sec:.1f} íŒŒì¼/ì´ˆ)")
        print(f"ğŸ’» ì‹œìŠ¤í…œ: {platform.system()} {platform.machine()}")
        print(f"ğŸ Python: {platform.python_version()}")
        print(f"ğŸ”§ ì›Œì»¤: {self.max_workers}ê°œ ë³‘ë ¬ ì²˜ë¦¬")
        print(f"ğŸ conda í™˜ê²½: {self.current_conda_env or 'None'}")
        
        # ìŠ¤ìº” í†µê³„
        print(f"\nğŸ“Š ìŠ¤ìº” í†µê³„:")
        print(f"   ğŸ“ ìŠ¤ìº” ìœ„ì¹˜: {stats.locations_scanned}ê³³")
        print(f"   ğŸ“„ ê²€ì‚¬ íŒŒì¼: {stats.total_files_scanned:,}ê°œ")
        print(f"   ğŸ¤– ë°œê²¬ ëª¨ë¸: {stats.models_found:,}ê°œ")
        print(f"   ğŸ’¾ ì´ ìš©ëŸ‰: {stats.total_size_gb:.2f}GB")
        print(f"   âœ… ìœ íš¨ ëª¨ë¸: {stats.valid_models}ê°œ")
        print(f"   âŒ ì†ìƒ ëª¨ë¸: {stats.corrupted_models}ê°œ")
        
        if not self.found_models:
            print("\nâŒ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self._print_suggestions()
            return
        
        # ìœ„ì¹˜ë³„ ë¶„í¬
        print(f"\nğŸ“ ìœ„ì¹˜ë³„ ë¶„í¬:")
        print(f"   ğŸ conda í™˜ê²½: {stats.conda_models}ê°œ")
        print(f"   ğŸ  í”„ë¡œì íŠ¸ ë‚´ë¶€: {stats.project_models}ê°œ") 
        print(f"   ğŸŒ ì‹œìŠ¤í…œ ì „ì²´: {stats.system_models}ê°œ")
        
        # conda í™˜ê²½ë³„ ìƒì„¸
        if stats.conda_models > 0:
            conda_dist = defaultdict(int)
            for model in self.found_models:
                if model.is_in_conda and model.conda_env_name:
                    conda_dist[model.conda_env_name] += 1
            
            print(f"   conda í™˜ê²½ë³„:")
            for env_name, count in sorted(conda_dist.items()):
                env_size = sum(m.size_gb for m in self.found_models 
                             if m.conda_env_name == env_name)
                print(f"     - {env_name}: {count}ê°œ ({env_size:.1f}GB)")
        
        # í”„ë ˆì„ì›Œí¬ ë¶„í¬
        print(f"\nğŸ”§ í”„ë ˆì„ì›Œí¬ë³„ ë¶„í¬:")
        for fw, count in sorted(stats.framework_distribution.items(), 
                               key=lambda x: x[1], reverse=True):
            fw_size = sum(m.size_gb for m in self.found_models if m.framework == fw)
            percentage = (count / stats.models_found) * 100
            print(f"   - {fw}: {count}ê°œ ({fw_size:.2f}GB, {percentage:.1f}%)")
        
        # MyCloset AI Step ë¶„í¬
        if stats.step_distribution:
            print(f"\nğŸ¯ MyCloset AI Stepë³„ ë¶„í¬ (ì‹ ë¢°ë„ 50%+):")
            step_names = {
                'step_01_human_parsing': '1ï¸âƒ£ Human Parsing',
                'step_02_pose_estimation': '2ï¸âƒ£ Pose Estimation',
                'step_03_cloth_segmentation': '3ï¸âƒ£ Cloth Segmentation', 
                'step_04_geometric_matching': '4ï¸âƒ£ Geometric Matching',
                'step_05_cloth_warping': '5ï¸âƒ£ Cloth Warping',
                'step_06_virtual_fitting': '6ï¸âƒ£ Virtual Fitting',
                'step_07_post_processing': '7ï¸âƒ£ Post Processing',
                'step_08_quality_assessment': '8ï¸âƒ£ Quality Assessment'
            }
            
            total_classified = sum(stats.step_distribution.values())
            for step, count in sorted(stats.step_distribution.items()):
                if count > 0:
                    display_name = step_names.get(step, step)
                    step_size = sum(m.size_gb for m in self.found_models 
                                  if m.step_candidate == step and m.confidence >= 0.5)
                    percentage = (count / total_classified) * 100
                    print(f"   {display_name}: {count}ê°œ ({step_size:.1f}GB, {percentage:.1f}%)")
        
        # ì¤‘ë³µ íŒŒì¼ ì •ë³´
        if self.duplicates:
            duplicate_count = len(self.duplicates)
            total_duplicates = sum(len(models) for models in self.duplicates.values())
            waste_size = sum(
                sum(m.size_gb for m in models[1:])  # ì²« ë²ˆì§¸ ì œì™¸í•œ ë‚˜ë¨¸ì§€
                for models in self.duplicates.values()
            )
            print(f"\nğŸ”„ ì¤‘ë³µ íŒŒì¼ ë¶„ì„:")
            print(f"   ì¤‘ë³µ ê·¸ë£¹: {duplicate_count}ê°œ")
            print(f"   ì¤‘ë³µ íŒŒì¼: {total_duplicates - duplicate_count}ê°œ")
            print(f"   ì ˆì•½ ê°€ëŠ¥: {waste_size:.2f}GB")
            
            if duplicate_count <= 5:  # 5ê°œ ì´í•˜ë©´ ìƒì„¸ í‘œì‹œ
                for i, (checksum, models) in enumerate(self.duplicates.items(), 1):
                    print(f"   ê·¸ë£¹ {i}: {len(models)}ê°œ íŒŒì¼")
                    for j, model in enumerate(models):
                        marker = "ğŸ†" if j == 0 else "ğŸ“„"
                        location = "conda" if model.is_in_conda else "project" if model.is_in_project else "system"
                        print(f"     {marker} {model.name} ({location}, {model.size_gb:.1f}GB)")
        
        # ìƒìœ„ ì¤‘ìš” ëª¨ë¸ë“¤
        print(f"\nğŸ† ì¤‘ìš”ë„ ìƒìœ„ ëª¨ë¸ë“¤:")
        top_models = sorted(self.found_models, key=lambda x: x.importance_score, reverse=True)[:10]
        
        for i, model in enumerate(top_models, 1):
            location_icon = "ğŸ" if model.is_in_conda else "ğŸ " if model.is_in_project else "ğŸŒ"
            step_info = ""
            if model.confidence >= 0.5 and model.step_candidate != 'unknown':
                step_num = model.step_candidate.split('_')[1] if '_' in model.step_candidate else '?'
                step_info = f" | ğŸ¯ Step {step_num}"
            
            confidence_icon = "ğŸŸ¢" if model.confidence >= 0.8 else "ğŸŸ¡" if model.confidence >= 0.5 else "ğŸ”´"
            
            print(f"  {i:2d}. {model.name}")
            print(f"      ğŸ“ {model.path}")
            print(f"      ğŸ“Š {model.size_gb:.2f}GB | {model.framework} | {model.model_type}")
            print(f"      {location_icon} {model.conda_env_name or model.parent_directory} | "
                  f"{confidence_icon} {model.confidence:.2f} | â­ {model.importance_score:.1f}{step_info}")
        
        # ë¬¸ì œ ë° ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        recommendations = []
        
        if stats.conda_models > 0 and stats.project_models == 0:
            recommendations.append(f"ğŸ”„ conda í™˜ê²½ì˜ {stats.conda_models}ê°œ ëª¨ë¸ì„ í”„ë¡œì íŠ¸ë¡œ ì—°ê²° ê³ ë ¤")
        
        if stats.system_models > stats.project_models:
            recommendations.append(f"ğŸ“¦ ì‹œìŠ¤í…œì˜ {stats.system_models}ê°œ ëª¨ë¸ì„ í”„ë¡œì íŠ¸ë¡œ í†µí•© ê³ ë ¤")
        
        if stats.duplicate_groups > 0:
            waste_gb = sum(sum(m.size_gb for m in models[1:]) for models in self.duplicates.values())
            recommendations.append(f"ğŸ—‘ï¸ ì¤‘ë³µ íŒŒì¼ {stats.duplicate_groups}ê·¸ë£¹ ì •ë¦¬ë¡œ {waste_gb:.1f}GB ì ˆì•½ ê°€ëŠ¥")
        
        if stats.corrupted_models > 0:
            recommendations.append(f"ğŸ”§ ì†ìƒëœ {stats.corrupted_models}ê°œ ëª¨ë¸ ë³µêµ¬ ë˜ëŠ” ì œê±°")
        
        large_models = [m for m in self.found_models if m.size_gb > 2.0]
        if large_models:
            recommendations.append(f"ğŸ“¦ 2GB+ ëŒ€ìš©ëŸ‰ ëª¨ë¸ {len(large_models)}ê°œ ìµœì í™” ê²€í† ")
        
        unclassified = [m for m in self.found_models if m.step_candidate == 'unknown']
        if unclassified:
            recommendations.append(f"ğŸ¯ ë¯¸ë¶„ë¥˜ ëª¨ë¸ {len(unclassified)}ê°œ ìˆ˜ë™ ê²€í†  í•„ìš”")
        
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        if not recommendations:
            print("   âœ… í˜„ì¬ ëª¨ë¸ êµ¬ì„±ì´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        
        # ì˜¤ë¥˜ ë° ê²½ê³ 
        if self.errors or self.warnings:
            print(f"\nâš ï¸ ë°œê²¬ëœ ë¬¸ì œë“¤:")
            if self.errors:
                print(f"   âŒ ì˜¤ë¥˜ {len(self.errors)}ê°œ:")
                for error in self.errors[:3]:
                    print(f"     - {error}")
                if len(self.errors) > 3:
                    print(f"     ... ì™¸ {len(self.errors) - 3}ê°œ")
            
            if self.warnings:
                print(f"   âš ï¸ ê²½ê³  {len(self.warnings)}ê°œ:")
                for warning in self.warnings[:3]:
                    print(f"     - {warning}")
                if len(self.warnings) > 3:
                    print(f"     ... ì™¸ {len(self.warnings) - 3}ê°œ")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. python advanced_scanner.py --organize     # ìë™ ëª¨ë¸ ì •ë¦¬")
        print("2. python advanced_scanner.py --repair       # ì†ìƒëœ ëª¨ë¸ ë³µêµ¬")
        print("3. ìŠ¤ìº” ê²°ê³¼ JSON íŒŒì¼ í™•ì¸ ë° í™œìš©")
        print("4. ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ë° ê³µê°„ ìµœì í™”")

    def _print_suggestions(self):
        """ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ì œì•ˆì‚¬í•­"""
        print("\nğŸ’¡ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        print("\nğŸ” ê²€ìƒ‰ í™•ì¥:")
        print("   python advanced_scanner.py --deep                 # ì „ì²´ ì‹œìŠ¤í…œ ë”¥ ìŠ¤ìº”")
        print("   python advanced_scanner.py --conda-first          # conda í™˜ê²½ ìš°ì„  ìŠ¤ìº”")
        print("   python advanced_scanner.py --deep --organize      # ë”¥ ìŠ¤ìº” + ìë™ ì •ë¦¬")
        
        print("\nğŸ“ ì¼ë°˜ì ì¸ AI ëª¨ë¸ ìœ„ì¹˜:")
        for env_name, env_path in self.conda_environments.items():
            print(f"   ğŸ {env_name}: {env_path}/lib/python*/site-packages")
        
        print("   ğŸ  í”„ë¡œì íŠ¸: ./backend/ai_models/")
        print("   ğŸ“¥ ë‹¤ìš´ë¡œë“œ: ~/Downloads/")
        print("   ğŸ’¾ ìºì‹œ: ~/.cache/huggingface/, ~/.cache/torch/")
        
        print("\nâš™ï¸ ì„¤ì • í™•ì¸:")
        print(f"   ìµœì†Œ í¬ê¸°: {self.config.min_size_mb}MB")
        print(f"   ìµœëŒ€ í¬ê¸°: {self.config.max_size_gb}GB")
        print(f"   ìŠ¤ìº” ê¹Šì´: {self.config.max_depth}ë‹¨ê³„")

    def repair_corrupted_models(self) -> int:
        """ì†ìƒëœ ëª¨ë¸ ë³µêµ¬ ì‹œë„"""
        logger.info("ğŸ”§ ì†ìƒëœ ëª¨ë¸ ë³µêµ¬ ì‹œì‘...")
        
        corrupted_models = [m for m in self.found_models if m.is_corrupted]
        if not corrupted_models:
            logger.info("âœ… ì†ìƒëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        repaired_count = 0
        
        for model in corrupted_models:
            try:
                # ë°±ì—… ìƒì„±
                backup_path = Path(model.path + '.backup')
                if not backup_path.exists():
                    shutil.copy2(model.path, backup_path)
                
                # ë³µêµ¬ ì‹œë„ (í”„ë ˆì„ì›Œí¬ë³„)
                if model.framework == 'pytorch':
                    if self._repair_pytorch_model(Path(model.path)):
                        repaired_count += 1
                        logger.info(f"âœ… ë³µêµ¬ ì„±ê³µ: {model.name}")
                elif model.framework == 'safetensors':
                    if self._repair_safetensors_model(Path(model.path)):
                        repaired_count += 1
                        logger.info(f"âœ… ë³µêµ¬ ì„±ê³µ: {model.name}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ë³µêµ¬ ì‹¤íŒ¨ {model.name}: {e}")
        
        logger.info(f"ğŸ¯ ì´ {repaired_count}/{len(corrupted_models)}ê°œ ëª¨ë¸ ë³µêµ¬ ì™„ë£Œ")
        return repaired_count

    def _repair_pytorch_model(self, file_path: Path) -> bool:
        """PyTorch ëª¨ë¸ ë³µêµ¬"""
        try:
            # ê°„ë‹¨í•œ í—¤ë” ë³µêµ¬ ì‹œë„
            with open(file_path, 'rb') as f:
                data = f.read()
            
            # ë§¤ì§ ë°”ì´íŠ¸ í™•ì¸ ë° ë³µêµ¬
            if not data.startswith(b'PK') and b'PK' in data:
                # PK í—¤ë”ë¥¼ ì°¾ì•„ì„œ ì•ìœ¼ë¡œ ì´ë™
                pk_index = data.find(b'PK')
                if pk_index > 0:
                    repaired_data = data[pk_index:]
                    
                    with open(file_path, 'wb') as f:
                        f.write(repaired_data)
                    
                    return True
            
        except Exception:
            pass
        
        return False

    def _repair_safetensors_model(self, file_path: Path) -> bool:
        """Safetensors ëª¨ë¸ ë³µêµ¬"""
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            # JSON í—¤ë” ì°¾ê¸°
            if data.startswith(b'{'):
                return True  # ì´ë¯¸ ì˜¬ë°”ë¦„
            
            json_start = data.find(b'{')
            if json_start > 0:
                repaired_data = data[json_start:]
                
                with open(file_path, 'wb') as f:
                    f.write(repaired_data)
                
                return True
            
        except Exception:
            pass
        
        return False

    def generate_conda_config(self, output_file: str = None) -> str:
        """conda í™˜ê²½ ìš°ì„  ì„¤ì • íŒŒì¼ ìƒì„±"""
        if output_file is None:
            output_file = "conda_model_config.py"
        
        config_content = f'''#!/usr/bin/env python3
"""
ğŸ MyCloset AI - Conda í™˜ê²½ ìš°ì„  ëª¨ë¸ ì„¤ì •
==========================================

ìë™ ìƒì„±ë¨: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ìŠ¤ìº”ëœ ëª¨ë¸: {len(self.found_models)}ê°œ
conda í™˜ê²½: {len(self.conda_environments)}ê°œ

ì‚¬ìš©ë²•:
    from conda_model_config import get_model_path, get_conda_models
    
    # íŠ¹ì • ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    clip_path = get_model_path("clip")
    
    # conda í™˜ê²½ì˜ ëª¨ë“  ëª¨ë¸
    conda_models = get_conda_models()
"""

import os
from pathlib import Path
from typing import Dict, List, Optional

# ==============================================
# ğŸ Conda í™˜ê²½ ì •ë³´
# ==============================================

CONDA_ENVIRONMENTS = {{
{self._format_conda_envs_for_config()}
}}

CURRENT_CONDA_ENV = "{self.current_conda_env or 'None'}"

# ==============================================
# ğŸ¤– ë°œê²¬ëœ ëª¨ë¸ ê²½ë¡œë“¤
# ==============================================

MODEL_PATHS = {{
{self._format_model_paths_for_config()}
}}

# MyCloset AI 8ë‹¨ê³„ë³„ ëª¨ë¸ ë§¤í•‘
STEP_MODELS = {{
{self._format_step_models_for_config()}
}}

# í”„ë ˆì„ì›Œí¬ë³„ ëª¨ë¸ ê·¸ë£¹
FRAMEWORK_MODELS = {{
{self._format_framework_models_for_config()}
}}

# ==============================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def get_model_path(model_name: str, prefer_conda: bool = True) -> Optional[str]:
    """ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (conda ìš°ì„ )"""
    candidates = []
    
    # ëª¨ë¸ëª…ìœ¼ë¡œ ì§ì ‘ ê²€ìƒ‰
    for key, path in MODEL_PATHS.items():
        if model_name.lower() in key.lower():
            candidates.append((key, path))
    
    if not candidates:
        return None
    
    # conda í™˜ê²½ ìš°ì„  ì •ë ¬
    if prefer_conda:
        conda_models = [c for c in candidates if any(env in c[1] for env in CONDA_ENVIRONMENTS.values())]
        if conda_models:
            return conda_models[0][1]
    
    return candidates[0][1]

def get_conda_models() -> Dict[str, List[str]]:
    """conda í™˜ê²½ë³„ ëª¨ë¸ ëª©ë¡"""
    result = {{}}
    
    for env_name, env_path in CONDA_ENVIRONMENTS.items():
        env_models = []
        for model_name, model_path in MODEL_PATHS.items():
            if env_path in model_path:
                env_models.append(model_path)
        result[env_name] = env_models
    
    return result

def get_step_model(step_number: int, prefer_conda: bool = True) -> Optional[str]:
    """Stepë³„ ìµœì  ëª¨ë¸ ê²½ë¡œ"""
    step_key = f"step_{{step_number:02d}}"
    
    for key, models in STEP_MODELS.items():
        if step_key in key and models:
            if prefer_conda:
                # conda í™˜ê²½ì˜ ëª¨ë¸ ìš°ì„ 
                conda_models = [m for m in models if any(env in m for env in CONDA_ENVIRONMENTS.values())]
                if conda_models:
                    return conda_models[0]
            return models[0]
    
    return None

def get_framework_models(framework: str) -> List[str]:
    """í”„ë ˆì„ì›Œí¬ë³„ ëª¨ë¸ ëª©ë¡"""
    return FRAMEWORK_MODELS.get(framework, [])

def validate_model_availability() -> Dict[str, bool]:
    """ëª¨ë¸ ê°€ìš©ì„± ê²€ì¦"""
    result = {{}}
    
    for model_name, model_path in MODEL_PATHS.items():
        result[model_name] = Path(model_path).exists()
    
    return result

def get_model_info(model_path: str) -> Dict[str, any]:
    """ëª¨ë¸ ìƒì„¸ ì •ë³´"""
    path_obj = Path(model_path)
    if not path_obj.exists():
        return {{"error": "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"}}
    
    stat_info = path_obj.stat()
    return {{
        "name": path_obj.name,
        "size_mb": round(stat_info.st_size / (1024 * 1024), 2),
        "modified": stat_info.st_mtime,
        "is_conda": any(env in model_path for env in CONDA_ENVIRONMENTS.values()),
        "framework": _detect_framework(path_obj)
    }}

def _detect_framework(path: Path) -> str:
    """í”„ë ˆì„ì›Œí¬ ê°ì§€"""
    ext = path.suffix.lower()
    if ext in ['.pth', '.pt']:
        return 'pytorch'
    elif ext == '.safetensors':
        return 'safetensors'
    elif ext in ['.pb', '.h5']:
        return 'tensorflow'
    elif ext == '.onnx':
        return 'onnx'
    return 'unknown'

# ==============================================
# ğŸš€ Quick Start ì˜ˆì œ
# ==============================================

if __name__ == "__main__":
    print("ğŸ MyCloset AI Conda ëª¨ë¸ ì„¤ì •")
    print("=" * 50)
    
    print(f"conda í™˜ê²½: {{len(CONDA_ENVIRONMENTS)}}ê°œ")
    print(f"ë°œê²¬ëœ ëª¨ë¸: {{len(MODEL_PATHS)}}ê°œ")
    
    # conda í™˜ê²½ë³„ ëª¨ë¸ ìˆ˜
    conda_models = get_conda_models()
    for env_name, models in conda_models.items():
        print(f"  {{env_name}}: {{len(models)}}ê°œ ëª¨ë¸")
    
    # Stepë³„ ëª¨ë¸ í™•ì¸
    print("\\nStepë³„ ëª¨ë¸:")
    for i in range(1, 9):
        model_path = get_step_model(i)
        if model_path:
            print(f"  Step {{i:02d}}: {{Path(model_path).name}}")
        else:
            print(f"  Step {{i:02d}}: ì—†ìŒ")
    
    # ê°€ìš©ì„± ê²€ì¦
    availability = validate_model_availability()
    available_count = sum(availability.values())
    print(f"\\nê°€ìš© ëª¨ë¸: {{available_count}}/{{len(MODEL_PATHS)}}ê°œ")
'''
        
        config_path = self.project_root / output_file
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"ğŸ“ conda ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        return str(config_path)

    def _format_conda_envs_for_config(self) -> str:
        """conda í™˜ê²½ ì„¤ì • í˜•ì‹"""
        lines = []
        for env_name, env_path in self.conda_environments.items():
            lines.append(f'    "{env_name}": "{env_path}",')
        return '\n'.join(lines)

    def _format_model_paths_for_config(self) -> str:
        """ëª¨ë¸ ê²½ë¡œ ì„¤ì • í˜•ì‹"""
        lines = []
        for model in sorted(self.found_models, key=lambda x: x.importance_score, reverse=True):
            safe_name = model.name.replace('.', '_').replace('-', '_')
            lines.append(f'    "{safe_name}": "{model.path}",')
        return '\n'.join(lines)

    def _format_step_models_for_config(self) -> str:
        """Stepë³„ ëª¨ë¸ ì„¤ì • í˜•ì‹"""
        lines = []
        step_models = defaultdict(list)
        
        for model in self.found_models:
            if model.confidence >= 0.5 and model.step_candidate != 'unknown':
                step_models[model.step_candidate].append(model.path)
        
        for step_name in sorted(step_models.keys()):
            models = step_models[step_name]
            models_str = ', '.join(f'"{path}"' for path in models)
            lines.append(f'    "{step_name}": [{models_str}],')
        
        return '\n'.join(lines)

    def _format_framework_models_for_config(self) -> str:
        """í”„ë ˆì„ì›Œí¬ë³„ ëª¨ë¸ ì„¤ì • í˜•ì‹"""
        lines = []
        framework_models = defaultdict(list)
        
        for model in self.found_models:
            framework_models[model.framework].append(model.path)
        
        for framework in sorted(framework_models.keys()):
            models = framework_models[framework]
            models_str = ', '.join(f'"{path}"' for path in models)
            lines.append(f'    "{framework}": [{models_str}],')
        
        return '\n'.join(lines)

    def cleanup_and_close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë° ì¢…ë£Œ"""
        if hasattr(self, 'db_connection'):
            self.db_connection.close()

# ==============================================
# ğŸš€ CLI ì¸í„°í˜ì´ìŠ¤ ë° ë©”ì¸ í•¨ìˆ˜
# ==============================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ì™„ì „ ê³ ë„í™”ëœ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ:
  python advanced_scanner.py                           # í‘œì¤€ ìŠ¤ìº”
  python advanced_scanner.py --deep                    # ë”¥ ìŠ¤ìº”
  python advanced_scanner.py --conda-first             # conda ìš°ì„  ìŠ¤ìº”
  python advanced_scanner.py --organize                # ìŠ¤ìº” + ìë™ ì •ë¦¬
  python advanced_scanner.py --repair                  # ì†ìƒëœ ëª¨ë¸ ë³µêµ¬
  python advanced_scanner.py --deep --organize --repair # ì™„ì „ ìë™í™”

ğŸ conda í™˜ê²½ ìµœì í™”:
  python advanced_scanner.py --conda-first --generate-config
        """
    )
    
    # ìŠ¤ìº” ì˜µì…˜
    parser.add_argument('--deep', action='store_true', 
                       help='ì „ì²´ ì‹œìŠ¤í…œ ë”¥ ìŠ¤ìº” (ë” ë§ì€ ìœ„ì¹˜ ê²€ìƒ‰)')
    parser.add_argument('--conda-first', action='store_true',
                       help='conda í™˜ê²½ ìš°ì„  ìŠ¤ìº”')
    parser.add_argument('--organize', action='store_true',
                       help='ìŠ¤ìº” í›„ ìë™ ëª¨ë¸ ì •ë¦¬')
    parser.add_argument('--repair', action='store_true',
                       help='ì†ìƒëœ ëª¨ë¸ ë³µêµ¬ ì‹œë„')
    
    # ì„¤ì • ì˜µì…˜
    parser.add_argument('--min-size', type=float, default=0.1,
                       help='ìµœì†Œ íŒŒì¼ í¬ê¸° (MB, ê¸°ë³¸: 0.1)')
    parser.add_argument('--max-size', type=float, default=100.0,
                       help='ìµœëŒ€ íŒŒì¼ í¬ê¸° (GB, ê¸°ë³¸: 100.0)')
    parser.add_argument('--max-depth', type=int, default=10,
                       help='ìµœëŒ€ ìŠ¤ìº” ê¹Šì´ (ê¸°ë³¸: 10)')
    parser.add_argument('--workers', type=int, default=None,
                       help='ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: CPU ì½”ì–´ ìˆ˜)')
    
    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument('--generate-config', action='store_true',
                       help='conda ì„¤ì • íŒŒì¼ ìƒì„±')
    parser.add_argument('--quiet', action='store_true',
                       help='ì¡°ìš©í•œ ëª¨ë“œ (ìµœì†Œ ì¶œë ¥)')
    parser.add_argument('--output', type=str,
                       help='ê²°ê³¼ ì €ì¥ íŒŒì¼ëª…')
    
    args = parser.parse_args()
    
    try:
        # ì„¤ì • êµ¬ì„±
        config = ScanConfig(
            min_size_mb=args.min_size,
            max_size_gb=args.max_size,
            max_depth=args.max_depth,
            conda_priority=args.conda_first,
            deep_scan=args.deep,
            verify_integrity=True,
            extract_metadata=True
        )
        
        # ìŠ¤ìºë„ˆ ì´ˆê¸°í™”
        scanner = AdvancedModelScanner(config)
        
        if args.workers:
            scanner.max_workers = min(args.workers, scanner.cpu_count)
        
        # ìŠ¤ìº” ì‹¤í–‰
        models = scanner.scan_comprehensive_system(organize=args.organize)
        
        # ë³µêµ¬ ì‘ì—… (ì˜µì…˜)
        if args.repair and models:
            repaired = scanner.repair_corrupted_models()
            if repaired > 0:
                logger.info(f"ğŸ”§ {repaired}ê°œ ëª¨ë¸ ë³µêµ¬ ì™„ë£Œ")
        
        # conda ì„¤ì • ìƒì„± (ì˜µì…˜)
        if args.generate_config and models:
            config_file = scanner.generate_conda_config()
            print(f"ğŸ“ conda ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")
        
        # ì™„ë£Œ ë©”ì‹œì§€
        if not args.quiet:
            print(f"\nâœ… ìŠ¤ìº” ì™„ë£Œ!")
            print(f"ğŸ¤– ë°œê²¬ëœ ëª¨ë¸: {len(models)}ê°œ")
            print(f"ğŸ conda ëª¨ë¸: {sum(1 for m in models if m.is_in_conda)}ê°œ")
            print(f"ğŸ  í”„ë¡œì íŠ¸ ëª¨ë¸: {sum(1 for m in models if m.is_in_project)}ê°œ")
            
            if models:
                total_size = sum(m.size_gb for m in models)
                avg_importance = sum(m.importance_score for m in models) / len(models)
                print(f"ğŸ’¾ ì´ ìš©ëŸ‰: {total_size:.2f}GB")
                print(f"â­ í‰ê·  ì¤‘ìš”ë„: {avg_importance:.1f}/100")
        
        # ì •ë¦¬
        scanner.cleanup_and_close()
        
        return 0 if models else 1
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if not args.quiet:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())