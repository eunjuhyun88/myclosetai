#!/usr/bin/env python3
"""
ğŸ”¥ ìˆ˜ì •ëœ AI ëª¨ë¸ ì •ë¦¬ ì‹œìŠ¤í…œ v2.0
âœ… re ëª¨ë“ˆ import ì˜¤ë¥˜ í•´ê²°
âœ… ì‹¤ì œ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ê°•í™”
âœ… auto_model_detector.py ê°œì„  ì™„ë£Œ
âœ… conda í™˜ê²½ ìµœì í™”
"""

import os
import sys
import re  # ğŸ”¥ re ëª¨ë“ˆ import ì¶”ê°€
import shutil
import hashlib
import json
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional, Any
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FixedModelCleanupSystem:
    """ìˆ˜ì •ëœ AI ëª¨ë¸ ì •ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, base_path: str = "backend/ai_models"):
        self.base_path = Path(base_path)
        self.analysis_result = {}
        
        # ğŸ”¥ step_05_cloth_warping ì¶”ê°€
        self.standard_structure = {
            "step_01_human_parsing": ["schp", "graphonomy", "densepose", "parsing"],
            "step_02_pose_estimation": ["openpose", "dwpose", "coco", "pose"],
            "step_03_cloth_segmentation": ["u2net", "cloth", "segmentation", "mask"],
            "step_04_geometric_matching": ["tps", "geometric", "matching", "transform"],
            "step_05_cloth_warping": ["warping", "cloth_warping", "deformation", "tps"],  # ğŸ”¥ ì¶”ê°€
            "step_06_virtual_fitting": ["viton", "diffusion", "fitting", "ootd"],
            "step_07_post_processing": ["postprocess", "refinement", "enhancement"],
            "step_08_quality_assessment": ["quality", "assessment", "metric"]
        }
    
    def run_advanced_cleanup(self, aggressive_mode: bool = False) -> Dict[str, Any]:
        """ê³ ê¸‰ ì •ë¦¬ ëª¨ë“œ ì‹¤í–‰"""
        try:
            logger.info("ğŸš€ ìˆ˜ì •ëœ AI ëª¨ë¸ ì •ë¦¬ ì‹œìŠ¤í…œ v2.0 ì‹œì‘")
            
            # 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ ì¬ë¶„ì„
            logger.info("ğŸ“Š 1ë‹¨ê³„: í–¥ìƒëœ ìƒíƒœ ë¶„ì„")
            self.analysis_result = self.enhanced_analysis()
            
            # 2ë‹¨ê³„: ê³ ê¸‰ ì¤‘ë³µ íƒì§€
            logger.info("ğŸ” 2ë‹¨ê³„: ê³ ê¸‰ ì¤‘ë³µ íƒì§€")
            advanced_duplicates = self.find_advanced_duplicates()
            
            # 3ë‹¨ê³„: ì¤‘ë³µ ì •ë¦¬ (aggressive_mode ì ìš©)
            logger.info("ğŸ—‚ï¸ 3ë‹¨ê³„: ì¤‘ë³µ íŒŒì¼ ì •ë¦¬")
            duplicate_results = self.cleanup_advanced_duplicates(
                advanced_duplicates, aggressive=aggressive_mode
            )
            
            # 4ë‹¨ê³„: step_05_cloth_warping ë””ë ‰í† ë¦¬ ì¶”ê°€
            logger.info("ğŸ“ 4ë‹¨ê³„: ëˆ„ë½ëœ ë””ë ‰í† ë¦¬ ìƒì„±")
            missing_dirs = self.create_missing_directories()
            
            # 5ë‹¨ê³„: auto_model_detector.py ìˆ˜ì •
            logger.info("ğŸ”§ 5ë‹¨ê³„: auto_model_detector.py ìˆ˜ì •")
            detector_results = self.fix_auto_detector()
            
            # 6ë‹¨ê³„: íŒŒì¼ ì´ë™ ìµœì í™”
            logger.info("ğŸ“‹ 6ë‹¨ê³„: íŒŒì¼ ì´ë™ ìµœì í™”")
            movement_results = self.optimize_file_placement()
            
            # ìµœì¢… ë¦¬í¬íŠ¸
            final_report = self.generate_enhanced_report(
                duplicate_results, missing_dirs, detector_results, movement_results
            )
            
            logger.info("ğŸ‰ ìˆ˜ì •ëœ AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ!")
            return final_report
            
        except Exception as e:
            logger.error(f"âŒ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def enhanced_analysis(self) -> Dict[str, Any]:
        """í–¥ìƒëœ ìƒíƒœ ë¶„ì„"""
        try:
            analysis = {
                "total_files": 0,
                "total_size_gb": 0.0,
                "by_extension": defaultdict(int),
                "by_step": defaultdict(int),
                "file_details": [],
                "large_files": [],
                "potential_duplicates": []
            }
            
            # ë” ì •êµí•œ íŒŒì¼ ìŠ¤ìº”
            for file_path in self.base_path.rglob("*"):
                if file_path.is_file():
                    try:
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        analysis["total_size_gb"] += size_mb / 1024
                        analysis["total_files"] += 1
                        
                        ext = file_path.suffix.lower()
                        analysis["by_extension"][ext] += 1
                        
                        step = self.classify_by_step_enhanced(file_path)
                        analysis["by_step"][step] += 1
                        
                        file_info = {
                            "path": str(file_path.relative_to(self.base_path)),
                            "size_mb": round(size_mb, 2),
                            "extension": ext,
                            "step": step,
                            "modified": file_path.stat().st_mtime,
                            "name": file_path.name,
                            "stem": file_path.stem
                        }
                        analysis["file_details"].append(file_info)
                        
                        # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì¶”ì  (100MB ì´ìƒ)
                        if size_mb > 100:
                            analysis["large_files"].append(file_info)
                            
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"í–¥ìƒëœ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def find_advanced_duplicates(self) -> List[Dict[str, Any]]:
        """ê³ ê¸‰ ì¤‘ë³µ íƒì§€"""
        duplicates = []
        
        try:
            # 1. íŒŒì¼ëª… ê¸°ë°˜ ì¤‘ë³µ íƒì§€ (ë” ì •êµí•¨)
            name_groups = defaultdict(list)
            for file_info in self.analysis_result.get("file_details", []):
                # ë²„ì „ ë²ˆí˜¸ ì œê±°í•œ ê¸°ë³¸ ì´ë¦„
                clean_name = re.sub(r'_\d+$|_v\d+$|_final$|_best$', '', file_info["stem"])
                name_groups[clean_name].append(file_info)
            
            for base_name, files in name_groups.items():
                if len(files) > 1:
                    # í¬ê¸°ë¡œ ì •ë ¬ (í° ê²ƒì´ ë³´í†µ ë” ì¢‹ìŒ)
                    files.sort(key=lambda x: x["size_mb"], reverse=True)
                    
                    duplicates.append({
                        "type": "name_similarity",
                        "base_name": base_name,
                        "files": files,
                        "recommended_keep": files[0],  # ê°€ì¥ í° íŒŒì¼
                        "recommended_remove": files[1:],
                        "savings_mb": sum(f["size_mb"] for f in files[1:])
                    })
            
            # 2. ë™ì¼ í™•ì¥ì + í¬ê¸° ê¸°ë°˜ ì¤‘ë³µ
            size_ext_groups = defaultdict(list)
            for file_info in self.analysis_result.get("file_details", []):
                key = f"{file_info['extension']}_{file_info['size_mb']}"
                size_ext_groups[key].append(file_info)
            
            for key, files in size_ext_groups.items():
                if len(files) > 1:
                    duplicates.append({
                        "type": "size_extension_match",
                        "key": key,
                        "files": files,
                        "recommended_keep": files[0],
                        "recommended_remove": files[1:],
                        "savings_mb": sum(f["size_mb"] for f in files[1:])
                    })
            
            # 3. ë‹¨ê³„ë³„ ê³¼ë„í•œ ì§‘ì¤‘ íƒì§€
            step_counts = defaultdict(int)
            for file_info in self.analysis_result.get("file_details", []):
                step_counts[file_info["step"]] += 1
            
            # step_01ì— ë„ˆë¬´ ë§ì€ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
            if step_counts.get("step_01_human_parsing", 0) > 200:
                step_01_files = [f for f in self.analysis_result["file_details"] 
                               if f["step"] == "step_01_human_parsing"]
                
                # í¬ê¸°ê°€ ì‘ì€ íŒŒì¼ë“¤ ì¤‘ë³µ ì˜ì‹¬
                small_files = [f for f in step_01_files if f["size_mb"] < 10]
                if len(small_files) > 50:
                    duplicates.append({
                        "type": "step_concentration",
                        "step": "step_01_human_parsing",
                        "files": small_files[:30],  # ìƒìœ„ 30ê°œë§Œ
                        "recommended_remove": small_files[:20],  # 20ê°œ ì œê±° ê¶Œì¥
                        "savings_mb": sum(f["size_mb"] for f in small_files[:20])
                    })
            
            return duplicates
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ì¤‘ë³µ íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def cleanup_advanced_duplicates(
        self, 
        duplicates: List[Dict[str, Any]], 
        aggressive: bool = False
    ) -> Dict[str, Any]:
        """ê³ ê¸‰ ì¤‘ë³µ ì •ë¦¬"""
        results = {
            "removed_files": [],
            "saved_space_gb": 0.0,
            "errors": []
        }
        
        try:
            for duplicate_group in duplicates:
                # aggressive ëª¨ë“œê°€ ì•„ë‹ˆë©´ ì•ˆì „í•œ ê²ƒë§Œ ì œê±°
                if not aggressive and duplicate_group["type"] == "step_concentration":
                    continue
                
                files_to_remove = duplicate_group.get("recommended_remove", [])
                
                for file_info in files_to_remove:
                    try:
                        file_path = self.base_path / file_info["path"]
                        
                        if file_path.exists():
                            # ë°±ì—… ìƒì„±
                            backup_path = self.base_path / "cleanup_backup" / file_info["path"]
                            backup_path.parent.mkdir(parents=True, exist_ok=True)
                            shutil.copy2(file_path, backup_path)
                            
                            # ì›ë³¸ ì‚­ì œ
                            file_path.unlink()
                            
                            results["removed_files"].append(file_info["path"])
                            results["saved_space_gb"] += file_info["size_mb"] / 1024
                            
                            logger.info(f"âœ… ì¤‘ë³µ íŒŒì¼ ì œê±°: {file_path.name}")
                            
                    except Exception as e:
                        error_msg = f"íŒŒì¼ ì œê±° ì‹¤íŒ¨ {file_info['path']}: {e}"
                        logger.error(error_msg)
                        results["errors"].append(error_msg)
            
            return results
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"errors": [str(e)]}
    
    def create_missing_directories(self) -> Dict[str, Any]:
        """ëˆ„ë½ëœ ë””ë ‰í† ë¦¬ ìƒì„±"""
        results = {
            "created_directories": [],
            "errors": []
        }
        
        try:
            organized_path = self.base_path / "organized"
            
            for step_name in self.standard_structure.keys():
                step_dir = organized_path / step_name
                
                if not step_dir.exists():
                    step_dir.mkdir(parents=True, exist_ok=True)
                    results["created_directories"].append(step_name)
                    logger.info(f"âœ… ë””ë ‰í† ë¦¬ ìƒì„±: {step_name}")
            
            return results
            
        except Exception as e:
            logger.error(f"ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"errors": [str(e)]}
    
    def fix_auto_detector(self) -> Dict[str, Any]:
        """auto_model_detector.py ìˆ˜ì •"""
        results = {
            "fixed": False,
            "issues_found": [],
            "fixes_applied": []
        }
        
        try:
            detector_path = Path("backend/app/ai_pipeline/utils/auto_model_detector.py")
            
            if not detector_path.exists():
                results["issues_found"].append("auto_model_detector.py íŒŒì¼ ì—†ìŒ")
                return results
            
            # íŒŒì¼ ì½ê¸°
            with open(detector_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë¬¸ì œì  ì²´í¬ ë° ìˆ˜ì •
            fixes_needed = []
            
            # 1. re ëª¨ë“ˆ import ì²´í¬
            if 'import re' not in content:
                fixes_needed.append("re ëª¨ë“ˆ import ì¶”ê°€")
                content = "import re\n" + content
            
            # 2. auto_detector ì¸ìŠ¤í„´ìŠ¤ export ì²´í¬
            if 'auto_detector = ' not in content:
                fixes_needed.append("auto_detector ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€")
                content += "\n\n# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤\nauto_detector = ImprovedAutoModelDetector()\n"
            
            # 3. __all__ ì²´í¬
            if '__all__' not in content:
                fixes_needed.append("__all__ ì¶”ê°€")
                content += "\n__all__ = ['auto_detector', 'ImprovedAutoModelDetector']\n"
            
            # ìˆ˜ì •ì‚¬í•­ì´ ìˆìœ¼ë©´ íŒŒì¼ ì €ì¥
            if fixes_needed:
                # ë°±ì—… ìƒì„±
                backup_path = detector_path.with_suffix('.py.backup')
                shutil.copy2(detector_path, backup_path)
                
                # ìˆ˜ì •ëœ ë‚´ìš© ì €ì¥
                with open(detector_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                results["fixed"] = True
                results["fixes_applied"] = fixes_needed
                logger.info(f"âœ… auto_model_detector.py ìˆ˜ì • ì™„ë£Œ: {fixes_needed}")
            else:
                results["fixed"] = True
                results["fixes_applied"] = ["ì´ë¯¸ ì •ìƒ ìƒíƒœ"]
            
            return results
            
        except Exception as e:
            logger.error(f"auto_detector ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return {"errors": [str(e)]}
    
    def optimize_file_placement(self) -> Dict[str, Any]:
        """íŒŒì¼ ë°°ì¹˜ ìµœì í™”"""
        results = {
            "moved_files": [],
            "errors": []
        }
        
        try:
            organized_path = self.base_path / "organized"
            
            # step_01ì— ì§‘ì¤‘ëœ íŒŒì¼ë“¤ ì¬ë¶„ë°°
            step_01_files = [f for f in self.analysis_result.get("file_details", [])
                           if f["step"] == "step_01_human_parsing"]
            
            # íŒŒì¼ëª…ìœ¼ë¡œ ë‹¤ë¥¸ ë‹¨ê³„ë¡œ ì´ë™í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ ì°¾ê¸°
            for file_info in step_01_files:
                file_path = self.base_path / file_info["path"]
                
                if not file_path.exists():
                    continue
                
                # ë” ì ì ˆí•œ ë‹¨ê³„ ì°¾ê¸°
                better_step = self.find_better_step_placement(file_info["name"])
                
                if better_step and better_step != "step_01_human_parsing":
                    try:
                        target_dir = organized_path / better_step
                        target_dir.mkdir(parents=True, exist_ok=True)
                        
                        target_path = target_dir / file_path.name
                        
                        # ì¤‘ë³µ ë°©ì§€
                        if not target_path.exists():
                            shutil.move(str(file_path), str(target_path))
                            results["moved_files"].append({
                                "from": file_info["path"],
                                "to": str(target_path.relative_to(self.base_path)),
                                "reason": f"ì¬ë¶„ë¥˜: {better_step}"
                            })
                            logger.info(f"ğŸ“‹ íŒŒì¼ ì´ë™: {file_path.name} â†’ {better_step}")
                    
                    except Exception as e:
                        results["errors"].append(f"íŒŒì¼ ì´ë™ ì‹¤íŒ¨ {file_info['path']}: {e}")
            
            return results
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë°°ì¹˜ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"errors": [str(e)]}
    
    def find_better_step_placement(self, filename: str) -> Optional[str]:
        """íŒŒì¼ì— ë” ì í•©í•œ ë‹¨ê³„ ì°¾ê¸°"""
        filename_lower = filename.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
        step_keywords = {
            "step_02_pose_estimation": ["pose", "openpose", "dwpose", "coco", "mpii"],
            "step_03_cloth_segmentation": ["u2net", "segment", "mask", "cloth"],
            "step_04_geometric_matching": ["tps", "geometric", "matching", "transform"],
            "step_05_cloth_warping": ["warp", "deform", "flow"],
            "step_06_virtual_fitting": ["viton", "diffusion", "fitting", "ootd", "stable"],
            "step_07_post_processing": ["enhance", "refine", "post", "super"],
            "step_08_quality_assessment": ["quality", "metric", "eval", "assess"]
        }
        
        for step, keywords in step_keywords.items():
            for keyword in keywords:
                if keyword in filename_lower:
                    return step
        
        return None
    
    def classify_by_step_enhanced(self, file_path: Path) -> str:
        """í–¥ìƒëœ ë‹¨ê³„ë³„ ë¶„ë¥˜"""
        try:
            path_str = str(file_path).lower()
            file_name = file_path.name.lower()
            
            # ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ë¶„ë¥˜ (1ì°¨)
            for step, keywords in self.standard_structure.items():
                if step in path_str:
                    return step
            
            # í‚¤ì›Œë“œë¡œ ë¶„ë¥˜ (2ì°¨)
            for step, keywords in self.standard_structure.items():
                for keyword in keywords:
                    if keyword in file_name or keyword in path_str:
                        return step
            
            # íŠ¹ë³„ íŒ¨í„´ë“¤ (3ì°¨)
            if any(pattern in file_name for pattern in ['schp', 'graphonomy', 'parsing']):
                return "step_01_human_parsing"
            elif any(pattern in file_name for pattern in ['openpose', 'dwpose', 'pose']):
                return "step_02_pose_estimation"
            elif any(pattern in file_name for pattern in ['viton', 'diffusion', 'ootd']):
                return "step_06_virtual_fitting"
            elif any(pattern in file_name for pattern in ['u2net', 'segment', 'cloth']):
                return "step_03_cloth_segmentation"
            elif any(pattern in file_name for pattern in ['warp', 'flow', 'deform']):
                return "step_05_cloth_warping"
            
            return "unknown"
            
        except Exception as e:
            logger.warning(f"íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨ {file_path}: {e}")
            return "unknown"
    
    def generate_enhanced_report(self, *results) -> Dict[str, Any]:
        """í–¥ìƒëœ ìµœì¢… ë¦¬í¬íŠ¸"""
        try:
            duplicate_results, missing_dirs, detector_results, movement_results = results
            
            report = {
                "success": True,
                "version": "2.0",
                "cleanup_summary": {
                    "files_removed": len(duplicate_results.get("removed_files", [])),
                    "space_saved_gb": round(duplicate_results.get("saved_space_gb", 0), 2),
                    "directories_created": len(missing_dirs.get("created_directories", [])),
                    "files_moved": len(movement_results.get("moved_files", [])),
                    "detector_fixed": detector_results.get("fixed", False)
                },
                "before_after": {
                    "before": {
                        "total_files": self.analysis_result.get("total_files", 0),
                        "total_size_gb": round(self.analysis_result.get("total_size_gb", 0), 2)
                    },
                    "estimated_after": {
                        "total_files": self.analysis_result.get("total_files", 0) - len(duplicate_results.get("removed_files", [])),
                        "total_size_gb": round(self.analysis_result.get("total_size_gb", 0) - duplicate_results.get("saved_space_gb", 0), 2)
                    }
                },
                "detailed_results": {
                    "duplicates": duplicate_results,
                    "directories": missing_dirs,
                    "detector": detector_results,
                    "movements": movement_results
                },
                "next_steps": [
                    "ğŸ”§ ìˆ˜ì •ëœ auto_model_detector.py í…ŒìŠ¤íŠ¸",
                    "ğŸ“Š stepë³„ íŒŒì¼ ë¶„í¬ ì¬í™•ì¸",
                    "ğŸ’¾ ë°±ì—…ëœ íŒŒì¼ë“¤ ê²€í† ",
                    "ğŸ” ì¶”ê°€ ìµœì í™” ê¸°íšŒ íƒìƒ‰"
                ]
            }
            
            return report
            
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ìˆ˜ì •ëœ AI ëª¨ë¸ ì •ë¦¬ ì‹œìŠ¤í…œ v2.0')
    parser.add_argument('--aggressive', action='store_true', help='ì ê·¹ì  ì •ë¦¬ ëª¨ë“œ')
    parser.add_argument('--path', default='backend/ai_models', help='AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ìˆ˜ì •ëœ ì •ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰
    cleanup_system = FixedModelCleanupSystem(args.path)
    
    try:
        result = cleanup_system.run_advanced_cleanup(aggressive_mode=args.aggressive)
        
        print("\n" + "="*80)
        print("ğŸ‰ ìˆ˜ì •ëœ AI ëª¨ë¸ ì •ë¦¬ v2.0 ì™„ë£Œ!")
        print("="*80)
        
        if result.get("success"):
            summary = result["cleanup_summary"]
            print(f"ğŸ—‚ï¸ ì œê±°ëœ íŒŒì¼: {summary['files_removed']}ê°œ")
            print(f"ğŸ’¾ ì ˆì•½ëœ ìš©ëŸ‰: {summary['space_saved_gb']:.2f}GB")
            print(f"ğŸ“ ìƒì„±ëœ ë””ë ‰í† ë¦¬: {summary['directories_created']}ê°œ")
            print(f"ğŸ“‹ ì´ë™ëœ íŒŒì¼: {summary['files_moved']}ê°œ")
            print(f"ğŸ”§ auto_detector ìˆ˜ì •: {'âœ…' if summary['detector_fixed'] else 'âŒ'}")
            
            before_after = result["before_after"]
            print(f"\nğŸ“ˆ Before â†’ After:")
            print(f"   íŒŒì¼: {before_after['before']['total_files']}ê°œ â†’ {before_after['estimated_after']['total_files']}ê°œ")
            print(f"   ìš©ëŸ‰: {before_after['before']['total_size_gb']:.1f}GB â†’ {before_after['estimated_after']['total_size_gb']:.1f}GB")
            
            print(f"\nğŸ”§ ë‹¤ìŒ ë‹¨ê³„:")
            for step in result["next_steps"]:
                print(f"   â€¢ {step}")
        else:
            print(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()