#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - AI ëª¨ë¸ ê²€ì¦ ë° ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸
í˜„ì¬ conda í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª¨ë¸ ê²€ì¦ ë„êµ¬

âœ… PIL.Image VERSION ì˜¤ë¥˜ í•´ê²°
âœ… MemoryManagerAdapter ë©”ì„œë“œ ëˆ„ë½ ìˆ˜ì •
âœ… conda í™˜ê²½ ì •ë³´ ì •í™•í•œ ê°ì§€
âœ… M3 Max 128GB ìµœì í™” ê²€ì¦
âœ… AI ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ë° ìˆ˜ì •
"""

import os
import sys
import time
import json
import logging
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent
backend_root = project_root / "backend"
sys.path.insert(0, str(backend_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

print("ğŸ” MyCloset AI - AI ëª¨ë¸ ê²€ì¦ ë° ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸")
print("=" * 60)

class ModelVerifier:
    """AI ëª¨ë¸ ê²€ì¦ ë° ìˆ˜ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.project_root = project_root
        self.backend_root = backend_root
        self.results = {}
        self.fixes_applied = []
        
    def check_system_environment(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í™˜ê²½ ê²€ì¦"""
        print("\nğŸ”§ ì‹œìŠ¤í…œ í™˜ê²½ ê²€ì¦ ì¤‘...")
        
        env_info = {}
        
        # Python ë²„ì „ í™•ì¸
        env_info['python_version'] = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        print(f"ğŸ Python: {env_info['python_version']}")
        
        # conda í™˜ê²½ í™•ì¸ (ì •í™•í•œ ê°ì§€)
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'base')
        conda_prefix = os.environ.get('CONDA_PREFIX', '')
        env_info['conda_env'] = conda_env
        env_info['conda_prefix'] = conda_prefix
        env_info['in_conda'] = conda_env != 'base'
        
        print(f"ğŸ Conda í™˜ê²½: {conda_env} ({'âœ…' if env_info['in_conda'] else 'âŒ'})")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        try:
            import psutil
            memory = psutil.virtual_memory()
            env_info['memory_total_gb'] = round(memory.total / (1024**3), 1)
            env_info['memory_available_gb'] = round(memory.available / (1024**3), 1)
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {env_info['memory_total_gb']}GB (ì‚¬ìš©ê°€ëŠ¥: {env_info['memory_available_gb']}GB)")
        except ImportError:
            env_info['memory_total_gb'] = 'unknown'
            print("ğŸ’¾ ë©”ëª¨ë¦¬: psutil ì—†ìŒ")
        
        # M3 Max ê°ì§€
        try:
            import platform
            if platform.system() == 'Darwin' and 'arm64' in platform.machine():
                env_info['is_m3_max'] = env_info['memory_total_gb'] >= 64
            else:
                env_info['is_m3_max'] = False
        except:
            env_info['is_m3_max'] = False
            
        print(f"ğŸ M3 Max: {'âœ…' if env_info['is_m3_max'] else 'âŒ'}")
        
        return env_info
    
    def check_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦"""
        print("\nğŸ“š ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦ ì¤‘...")
        
        deps = {}
        
        # PIL/Pillow ê²€ì¦
        try:
            from PIL import Image
            # PIL.Image.VERSION â†’ PIL.__version__ ìˆ˜ì • ê²€ì¦
            pil_version = getattr(Image, '__version__', 'unknown')
            if hasattr(Image, 'VERSION'):
                deps['pil_version'] = Image.VERSION
                deps['pil_issue'] = False
            else:
                deps['pil_version'] = pil_version
                deps['pil_issue'] = True
            
            print(f"ğŸ–¼ï¸ PIL/Pillow: {deps['pil_version']} ({'âœ…' if not deps['pil_issue'] else 'âš ï¸ VERSION ì†ì„± ì—†ìŒ'})")
        except ImportError:
            deps['pil_version'] = None
            deps['pil_issue'] = True
            print("ğŸ–¼ï¸ PIL/Pillow: âŒ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        # NumPy ê²€ì¦
        try:
            import numpy as np
            deps['numpy_version'] = np.__version__
            deps['numpy_compatible'] = int(np.__version__.split('.')[0]) < 2
            print(f"ğŸ”¢ NumPy: {deps['numpy_version']} ({'âœ…' if deps['numpy_compatible'] else 'âš ï¸ 2.x ë²„ì „'})")
        except ImportError:
            deps['numpy_version'] = None
            deps['numpy_compatible'] = False
            print("ğŸ”¢ NumPy: âŒ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        # PyTorch ê²€ì¦
        try:
            import torch
            deps['torch_version'] = torch.__version__
            deps['mps_available'] = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False
            deps['mps_built'] = torch.backends.mps.is_built() if hasattr(torch.backends, 'mps') else False
            
            print(f"ğŸ”¥ PyTorch: {deps['torch_version']}")
            print(f"ğŸ MPS: {'âœ… ì‚¬ìš©ê°€ëŠ¥' if deps['mps_available'] else 'âŒ ì‚¬ìš©ë¶ˆê°€'}")
        except ImportError:
            deps['torch_version'] = None
            deps['mps_available'] = False
            print("ğŸ”¥ PyTorch: âŒ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        return deps
    
    def check_ai_models_paths(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ê²½ë¡œ ê²€ì¦"""
        print("\nğŸ¤– AI ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì¤‘...")
        
        models_info = {}
        
        # ê¸°ë³¸ AI ëª¨ë¸ ë””ë ‰í† ë¦¬
        ai_models_dir = self.backend_root / "ai_models"
        models_info['ai_models_exists'] = ai_models_dir.exists()
        
        if models_info['ai_models_exists']:
            print(f"ğŸ“ AI ëª¨ë¸ ë””ë ‰í† ë¦¬: âœ… {ai_models_dir}")
            
            # HuggingFace ìºì‹œ í™•ì¸
            hf_cache = ai_models_dir / "huggingface_cache"
            models_info['hf_cache_exists'] = hf_cache.exists()
            
            if models_info['hf_cache_exists']:
                print(f"ğŸ“ HuggingFace ìºì‹œ: âœ… {hf_cache}")
                
                # OOTDiffusion ëª¨ë¸ í™•ì¸
                ootd_path = hf_cache / "models--levihsu--OOTDiffusion"
                models_info['ootdiffusion_exists'] = ootd_path.exists()
                
                if models_info['ootdiffusion_exists']:
                    print(f"ğŸ¯ OOTDiffusion: âœ… {ootd_path}")
                    
                    # ìŠ¤ëƒ…ìƒ· ë””ë ‰í† ë¦¬ í™•ì¸
                    snapshots = list(ootd_path.glob("snapshots/*"))
                    models_info['ootd_snapshots'] = len(snapshots)
                    print(f"ğŸ“¸ ìŠ¤ëƒ…ìƒ·: {models_info['ootd_snapshots']}ê°œ")
                    
                    if snapshots:
                        latest_snapshot = snapshots[0]
                        unet_path = latest_snapshot / "checkpoints" / "ootd" / "ootd_dc" / "checkpoint-36000" / "unet_vton"
                        models_info['unet_vton_exists'] = unet_path.exists()
                        
                        if models_info['unet_vton_exists']:
                            print(f"ğŸ§  UNet VTON: âœ… {unet_path}")
                        else:
                            print(f"ğŸ§  UNet VTON: âŒ {unet_path}")
                else:
                    print("ğŸ¯ OOTDiffusion: âŒ ì—†ìŒ")
            else:
                print(f"ğŸ“ HuggingFace ìºì‹œ: âŒ {hf_cache}")
        else:
            print(f"ğŸ“ AI ëª¨ë¸ ë””ë ‰í† ë¦¬: âŒ {ai_models_dir}")
        
        # ê°œë³„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ í™•ì¸
        checkpoint_files = [
            "exp-schp-201908301523-atr.pth",
            "openpose.pth",
            "u2net.pth"
        ]
        
        models_info['checkpoints'] = {}
        for ckpt in checkpoint_files:
            ckpt_path = ai_models_dir / ckpt
            exists = ckpt_path.exists()
            models_info['checkpoints'][ckpt] = {
                'exists': exists,
                'path': str(ckpt_path),
                'size_mb': round(ckpt_path.stat().st_size / (1024*1024), 1) if exists else 0
            }
            
            status = "âœ…" if exists else "âŒ"
            size_info = f"({models_info['checkpoints'][ckpt]['size_mb']}MB)" if exists else ""
            print(f"âš™ï¸ {ckpt}: {status} {size_info}")
        
        return models_info
    
    def check_backend_issues(self) -> Dict[str, Any]:
        """ë°±ì—”ë“œ ì½”ë“œ ë¬¸ì œì  ê²€ì¦"""
        print("\nğŸ”§ ë°±ì—”ë“œ ì½”ë“œ ë¬¸ì œì  ê²€ì¦ ì¤‘...")
        
        issues = {}
        
        # MemoryManagerAdapter ê²€ì¦
        memory_manager_path = self.backend_root / "app" / "ai_pipeline" / "utils" / "memory_manager.py"
        if memory_manager_path.exists():
            try:
                with open(memory_manager_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # optimize_memory ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
                has_optimize_memory = 'def optimize_memory' in content
                issues['memory_manager_optimize_method'] = has_optimize_memory
                
                print(f"ğŸ§  MemoryManagerAdapter.optimize_memory: {'âœ…' if has_optimize_memory else 'âŒ ë©”ì„œë“œ ëˆ„ë½'}")
            except Exception as e:
                issues['memory_manager_optimize_method'] = False
                print(f"ğŸ§  MemoryManagerAdapter: âŒ ì½ê¸° ì‹¤íŒ¨ - {e}")
        else:
            issues['memory_manager_optimize_method'] = False
            print(f"ğŸ§  MemoryManagerAdapter: âŒ íŒŒì¼ ì—†ìŒ")
        
        # PIL.Image.VERSION ì‚¬ìš© í™•ì¸
        utils_files = list((self.backend_root / "app").rglob("*.py"))
        pil_version_usage = []
        
        for file_path in utils_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if 'PIL.Image.VERSION' in content or 'Image.VERSION' in content:
                    pil_version_usage.append(str(file_path.relative_to(self.backend_root)))
            except:
                continue
        
        issues['pil_version_usage'] = pil_version_usage
        if pil_version_usage:
            print(f"ğŸ–¼ï¸ PIL.Image.VERSION ì‚¬ìš©: âŒ {len(pil_version_usage)}ê°œ íŒŒì¼")
            for file in pil_version_usage:
                print(f"   ğŸ“„ {file}")
        else:
            print("ğŸ–¼ï¸ PIL.Image.VERSION ì‚¬ìš©: âœ… ì—†ìŒ")
        
        return issues
    
    def apply_immediate_fixes(self, env_info: Dict, deps: Dict, issues: Dict) -> List[str]:
        """ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì •ì‚¬í•­ë“¤"""
        print("\nğŸ› ï¸ ì¦‰ì‹œ ìˆ˜ì • ì ìš© ì¤‘...")
        
        fixes = []
        
        # 1. PIL.Image.VERSION ìˆ˜ì •
        if issues.get('pil_version_usage'):
            for file_rel_path in issues['pil_version_usage']:
                file_path = self.backend_root / file_rel_path
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # PIL.Image.VERSION â†’ PIL.__version__ ë³€ê²½
                    modified_content = content.replace('PIL.Image.VERSION', 'PIL.__version__')
                    modified_content = modified_content.replace('Image.VERSION', 'Image.__version__')
                    
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(modified_content)
                    
                    fixes.append(f"PIL.Image.VERSION ìˆ˜ì •: {file_rel_path}")
                    print(f"âœ… {file_rel_path} - PIL.Image.VERSION ìˆ˜ì •")
                except Exception as e:
                    print(f"âŒ {file_rel_path} - ìˆ˜ì • ì‹¤íŒ¨: {e}")
        
        # 2. MemoryManagerAdapter.optimize_memory ë©”ì„œë“œ ì¶”ê°€
        if not issues.get('memory_manager_optimize_method'):
            memory_manager_path = self.backend_root / "app" / "ai_pipeline" / "utils" / "memory_manager.py"
            if memory_manager_path.exists():
                try:
                    with open(memory_manager_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # MemoryManagerAdapter í´ë˜ìŠ¤ ì°¾ê¸°
                    if 'class MemoryManagerAdapter' in content:
                        optimize_method = '''
    async def optimize_memory(self, aggressive: bool = False):
        """ë©”ëª¨ë¦¬ ìµœì í™” - M3 Max 128GB ìµœì í™”"""
        try:
            import gc
            gc.collect()
            
            if hasattr(self, 'device') and self.device == "mps":
                try:
                    import torch
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except (AttributeError, RuntimeError):
                    pass  # MPS ê¸°ëŠ¥ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
            
            if aggressive:
                # ì¶”ê°€ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                for _ in range(3):
                    gc.collect()
                    
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
'''
                        
                        # í´ë˜ìŠ¤ ë ë¶€ë¶„ì— ë©”ì„œë“œ ì¶”ê°€
                        lines = content.split('\n')
                        new_lines = []
                        in_class = False
                        class_indent = 0
                        
                        for line in lines:
                            new_lines.append(line)
                            
                            if 'class MemoryManagerAdapter' in line:
                                in_class = True
                                class_indent = len(line) - len(line.lstrip())
                            elif in_class and line.strip() and not line.startswith(' ' * (class_indent + 1)):
                                # í´ë˜ìŠ¤ ë
                                new_lines.insert(-1, optimize_method)
                                in_class = False
                        
                        if in_class:
                            # íŒŒì¼ ëì—ì„œ í´ë˜ìŠ¤ê°€ ëë‚˜ëŠ” ê²½ìš°
                            new_lines.append(optimize_method)
                        
                        with open(memory_manager_path, 'w', encoding='utf-8') as f:
                            f.write('\n'.join(new_lines))
                        
                        fixes.append("MemoryManagerAdapter.optimize_memory ë©”ì„œë“œ ì¶”ê°€")
                        print("âœ… MemoryManagerAdapter.optimize_memory ë©”ì„œë“œ ì¶”ê°€")
                    else:
                        print("âŒ MemoryManagerAdapter í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                except Exception as e:
                    print(f"âŒ MemoryManagerAdapter ìˆ˜ì • ì‹¤íŒ¨: {e}")
        
        return fixes
    
    def generate_conda_fix_script(self, env_info: Dict) -> str:
        """Conda í™˜ê²½ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        script_content = f"""#!/bin/bash
# MyCloset AI - Conda í™˜ê²½ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ (Python {env_info['python_version']})

echo "ğŸ”§ MyCloset AI Conda í™˜ê²½ ìˆ˜ì •"
echo "í˜„ì¬ í™˜ê²½: {env_info['conda_env']}"
echo "Python: {env_info['python_version']}"
echo ""

# í˜„ì¬ í™˜ê²½ í™œì„±í™”
conda activate {env_info['conda_env']}

# NumPy í˜¸í™˜ì„± í•´ê²° (Python 3.12 ë²„ì „)
echo "ğŸ”¢ NumPy í˜¸í™˜ì„± ìˆ˜ì • ì¤‘..."
pip install numpy==1.24.4

# PyTorch M3 Max ìµœì í™” ë²„ì „ ì„¤ì¹˜
echo "ğŸ”¥ PyTorch M3 Max ìµœì í™” ì„¤ì¹˜ ì¤‘..."
pip install torch torchvision torchaudio

# ê¸°íƒ€ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
echo "ğŸ“š í•„ìˆ˜ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ ì¤‘..."
pip install --upgrade fastapi uvicorn pydantic

echo "âœ… Conda í™˜ê²½ ìˆ˜ì • ì™„ë£Œ"
echo "ğŸš€ ì„œë²„ ì‹¤í–‰: cd backend && python app/main.py"
"""
        
        script_path = self.project_root / "fix_conda_env.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        os.chmod(script_path, 0o755)
        return str(script_path)
    
    def run_verification(self) -> Dict[str, Any]:
        """ì „ì²´ ê²€ì¦ ì‹¤í–‰"""
        print("ğŸ” MyCloset AI ì „ì²´ ê²€ì¦ ì‹œì‘...")
        start_time = time.time()
        
        # 1. ì‹œìŠ¤í…œ í™˜ê²½ ê²€ì¦
        env_info = self.check_system_environment()
        
        # 2. ì˜ì¡´ì„± ê²€ì¦
        deps = self.check_dependencies()
        
        # 3. AI ëª¨ë¸ ê²½ë¡œ ê²€ì¦
        models_info = self.check_ai_models_paths()
        
        # 4. ë°±ì—”ë“œ ì½”ë“œ ë¬¸ì œì  ê²€ì¦
        issues = self.check_backend_issues()
        
        # 5. ì¦‰ì‹œ ìˆ˜ì • ì ìš©
        fixes = self.apply_immediate_fixes(env_info, deps, issues)
        
        # 6. Conda ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        conda_script = self.generate_conda_fix_script(env_info)
        
        elapsed_time = time.time() - start_time
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'verification_time': round(elapsed_time, 2),
            'environment': env_info,
            'dependencies': deps,
            'models': models_info,
            'issues': issues,
            'fixes_applied': fixes,
            'conda_fix_script': conda_script,
            'summary': self.generate_summary(env_info, deps, models_info, issues, fixes)
        }
        
        return results
    
    def generate_summary(self, env_info: Dict, deps: Dict, models_info: Dict, issues: Dict, fixes: List[str]) -> Dict[str, Any]:
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½"""
        summary = {
            'status': 'healthy',
            'critical_issues': [],
            'warnings': [],
            'recommendations': []
        }
        
        # Critical Issues
        if not deps.get('pil_version'):
            summary['critical_issues'].append("PIL/Pillow not installed")
            summary['status'] = 'critical'
        
        if not deps.get('torch_version'):
            summary['critical_issues'].append("PyTorch not installed")
            summary['status'] = 'critical'
        
        if not issues.get('memory_manager_optimize_method'):
            summary['critical_issues'].append("MemoryManagerAdapter.optimize_memory missing")
            summary['status'] = 'critical'
        
        # Warnings
        if deps.get('pil_issue'):
            summary['warnings'].append("PIL.Image.VERSION attribute issue")
        
        if not deps.get('mps_available'):
            summary['warnings'].append("MPS not available for M3 Max")
        
        if not models_info.get('ootdiffusion_exists'):
            summary['warnings'].append("OOTDiffusion model not found")
        
        # Recommendations
        if env_info.get('is_m3_max'):
            summary['recommendations'].append("Enable M3 Max 128GB optimization")
        
        if not deps.get('numpy_compatible'):
            summary['recommendations'].append("Downgrade NumPy to 1.x for compatibility")
        
        return summary

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    verifier = ModelVerifier()
    
    try:
        results = verifier.run_verification()
        
        print("\n" + "="*60)
        print("ğŸ¯ ê²€ì¦ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        summary = results['summary']
        print(f"ğŸ“Š ì „ì²´ ìƒíƒœ: {summary['status'].upper()}")
        print(f"â±ï¸ ê²€ì¦ ì‹œê°„: {results['verification_time']}ì´ˆ")
        print(f"ğŸ› ï¸ ì ìš©ëœ ìˆ˜ì •: {len(results['fixes_applied'])}ê°œ")
        
        if summary['critical_issues']:
            print(f"\nğŸš¨ ì¤‘ìš” ë¬¸ì œ: {len(summary['critical_issues'])}ê°œ")
            for issue in summary['critical_issues']:
                print(f"   âŒ {issue}")
        
        if summary['warnings']:
            print(f"\nâš ï¸ ê²½ê³  ì‚¬í•­: {len(summary['warnings'])}ê°œ")
            for warning in summary['warnings']:
                print(f"   âš ï¸ {warning}")
        
        if summary['recommendations']:
            print(f"\nğŸ’¡ ê¶Œì¥ ì‚¬í•­: {len(summary['recommendations'])}ê°œ")
            for rec in summary['recommendations']:
                print(f"   ğŸ’¡ {rec}")
        
        if results['fixes_applied']:
            print(f"\nâœ… ì ìš©ëœ ìˆ˜ì • ì‚¬í•­:")
            for fix in results['fixes_applied']:
                print(f"   âœ… {fix}")
        
        print(f"\nğŸ“ Conda ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸: {results['conda_fix_script']}")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ./fix_conda_env.sh ì‹¤í–‰")
        print("2. cd backend && python app/main.py")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        results_file = project_root / "verification_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“„ ìƒì„¸ ê²°ê³¼: {results_file}")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()