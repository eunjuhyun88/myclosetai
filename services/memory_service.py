#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Memory Service for Cloth Warping
==================================================

ğŸ¯ ì˜ë¥˜ ì›Œí•‘ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„œë¹„ìŠ¤
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
âœ… ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬
âœ… ë©”ëª¨ë¦¬ ìµœì í™”
âœ… M3 Max ìµœì í™”
"""

import torch
import logging
import gc
import psutil
import time
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class MemoryConfig:
    """ë©”ëª¨ë¦¬ ì„¤ì •"""
    max_memory_gb: float = 8.0
    cleanup_threshold: float = 0.8
    enable_auto_cleanup: bool = True
    cleanup_interval_seconds: int = 60
    enable_memory_profiling: bool = True
    max_cache_size_mb: int = 1024

class MemoryService:
    """ì˜ë¥˜ ì›Œí•‘ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: MemoryConfig = None):
        self.config = config or MemoryConfig()
        self.logger = logging.getLogger(__name__)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.memory_usage_history = []
        self.last_cleanup_time = time.time()
        
        # ìºì‹œëœ í…ì„œë“¤
        self.cached_tensors = {}
        self.tensor_sizes = {}
        
        # ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
        if self.config.enable_memory_profiling:
            self.memory_profiles = []
        
        self.logger.info(f"ğŸ¯ Memory Service ì´ˆê¸°í™” (ìµœëŒ€ ë©”ëª¨ë¦¬: {self.config.max_memory_gb}GB)")
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        self._check_initial_memory_state()
        
        self.logger.info("âœ… Memory Service ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _check_initial_memory_state(self):
        """ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
        try:
            system_memory = psutil.virtual_memory()
            available_memory_gb = system_memory.available / (1024**3)
            
            self.logger.info(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {system_memory.total / (1024**3):.2f}GB")
            self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {available_memory_gb:.2f}GB")
            
            if available_memory_gb < self.config.max_memory_gb:
                self.logger.warning(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ê°€ ì„¤ì •ëœ ìµœëŒ€ê°’ë³´ë‹¤ ì ìŠµë‹ˆë‹¤: {available_memory_gb:.2f}GB < {self.config.max_memory_gb}GB")
                
        except Exception as e:
            self.logger.warning(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        memory_info = {}
        
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
            system_memory = psutil.virtual_memory()
            memory_info['system_total_gb'] = system_memory.total / (1024**3)
            memory_info['system_available_gb'] = system_memory.available / (1024**3)
            memory_info['system_used_gb'] = system_memory.used / (1024**3)
            memory_info['system_percent'] = system_memory.percent
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë³´
            if torch.cuda.is_available():
                memory_info['gpu_total_mb'] = torch.cuda.get_device_properties(0).total_memory / (1024**2)
                memory_info['gpu_allocated_mb'] = torch.cuda.memory_allocated(0) / (1024**2)
                memory_info['gpu_cached_mb'] = torch.cuda.memory_reserved(0) / (1024**2)
                memory_info['gpu_free_mb'] = memory_info['gpu_total_mb'] - memory_info['gpu_allocated_mb']
            
            # MPS ë©”ëª¨ë¦¬ ì •ë³´ (Apple Silicon)
            if torch.backends.mps.is_available():
                memory_info['mps_available'] = True
                # MPSëŠ” ì§ì ‘ì ì¸ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                memory_info['mps_memory_estimate_mb'] = self._estimate_mps_memory_usage()
            else:
                memory_info['mps_available'] = False
            
            # ìºì‹œëœ í…ì„œ ì •ë³´
            memory_info['cached_tensors_count'] = len(self.cached_tensors)
            memory_info['cached_tensors_size_mb'] = sum(self.tensor_sizes.values()) / (1024**2)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory_info['memory_usage_ratio'] = memory_info['system_used_gb'] / memory_info['system_total_gb']
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            memory_info = {
                'error': str(e),
                'system_total_gb': 0.0,
                'system_available_gb': 0.0,
                'system_used_gb': 0.0,
                'system_percent': 0.0
            }
        
        return memory_info
    
    def _estimate_mps_memory_usage(self) -> float:
        """MPS ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        try:
            # ìºì‹œëœ í…ì„œë“¤ì˜ í¬ê¸°ë¡œ ì¶”ì •
            total_tensor_memory = sum(self.tensor_sizes.values())
            
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ë¹„êµí•˜ì—¬ ì¶”ì •
            system_memory = psutil.virtual_memory()
            estimated_mps_memory = min(total_tensor_memory / (1024**2), system_memory.used / (1024**2) * 0.3)
            
            return estimated_mps_memory
            
        except Exception:
            return 0.0
    
    def monitor_memory_usage(self, interval_seconds: int = 5) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        monitoring_data = {
            'timestamp': time.time(),
            'memory_usage': self.get_memory_usage(),
            'recommendations': []
        }
        
        try:
            memory_usage = monitoring_data['memory_usage']
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
            if memory_usage.get('memory_usage_ratio', 0) > self.config.cleanup_threshold:
                monitoring_data['recommendations'].append("ë©”ëª¨ë¦¬ ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
                
                if self.config.enable_auto_cleanup:
                    self.cleanup_memory()
                    monitoring_data['cleanup_performed'] = True
            
            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            if 'gpu_allocated_mb' in memory_usage:
                gpu_usage_ratio = memory_usage['gpu_allocated_mb'] / memory_usage['gpu_total_mb']
                if gpu_usage_ratio > 0.9:
                    monitoring_data['recommendations'].append("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # ìºì‹œ í¬ê¸° í™•ì¸
            if memory_usage.get('cached_tensors_size_mb', 0) > self.config.max_cache_size_mb:
                monitoring_data['recommendations'].append("í…ì„œ ìºì‹œ ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
                self.cleanup_tensor_cache()
                monitoring_data['cache_cleanup_performed'] = True
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.memory_usage_history.append(monitoring_data)
            
            # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
            if len(self.memory_usage_history) > 1000:
                self.memory_usage_history = self.memory_usage_history[-500:]
            
            # ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
            if self.config.enable_memory_profiling:
                self._add_memory_profile(monitoring_data)
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            monitoring_data['error'] = str(e)
        
        return monitoring_data
    
    def _add_memory_profile(self, monitoring_data: Dict[str, Any]):
        """ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ ì¶”ê°€"""
        try:
            profile = {
                'timestamp': monitoring_data['timestamp'],
                'memory_usage': monitoring_data['memory_usage'],
                'peak_memory_mb': max([h['memory_usage'].get('system_used_gb', 0) * 1024 
                                     for h in self.memory_usage_history[-100:]]),
                'average_memory_mb': np.mean([h['memory_usage'].get('system_used_gb', 0) * 1024 
                                            for h in self.memory_usage_history[-100:]])
            }
            
            self.memory_profiles.append(profile)
            
            # í”„ë¡œíŒŒì¼ í¬ê¸° ì œí•œ
            if len(self.memory_profiles) > 100:
                self.memory_profiles = self.memory_profiles[-50:]
                
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    def cleanup_memory(self, force: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        cleanup_results = {
            'timestamp': time.time(),
            'cleanup_type': 'automatic' if not force else 'forced',
            'results': {}
        }
        
        try:
            # ìë™ ì •ë¦¬ ì¡°ê±´ í™•ì¸
            if not force and not self.config.enable_auto_cleanup:
                cleanup_results['results']['auto_cleanup_disabled'] = True
                return cleanup_results
            
            # ë§ˆì§€ë§‰ ì •ë¦¬ í›„ ì‹œê°„ í™•ì¸
            time_since_last_cleanup = time.time() - self.last_cleanup_time
            if not force and time_since_last_cleanup < self.config.cleanup_interval_seconds:
                cleanup_results['results']['cleanup_interval_not_reached'] = True
                return cleanup_results
            
            # 1ë‹¨ê³„: Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            cleanup_results['results']['garbage_collection'] = True
            
            # 2ë‹¨ê³„: PyTorch ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                cleanup_results['results']['cuda_cache_cleared'] = True
            
            # 3ë‹¨ê³„: í…ì„œ ìºì‹œ ì •ë¦¬
            cache_cleanup_result = self.cleanup_tensor_cache()
            cleanup_results['results']['tensor_cache_cleanup'] = cache_cleanup_result
            
            # 4ë‹¨ê³„: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬í™•ì¸
            memory_after = self.get_memory_usage()
            cleanup_results['results']['memory_after_cleanup'] = memory_after
            
            # ì •ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.last_cleanup_time = time.time()
            
            self.logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            cleanup_results['results']['error'] = str(e)
        
        return cleanup_results
    
    def cleanup_tensor_cache(self) -> Dict[str, Any]:
        """í…ì„œ ìºì‹œ ì •ë¦¬"""
        cache_cleanup_result = {
            'tensors_removed': 0,
            'memory_freed_mb': 0.0,
            'removed_tensors': []
        }
        
        try:
            # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” í…ì„œë“¤ ì œê±°
            tensors_to_remove = []
            
            for tensor_name, tensor in self.cached_tensors.items():
                # í…ì„œê°€ ì—¬ì „íˆ ìœ íš¨í•œì§€ í™•ì¸
                if not self._is_tensor_valid(tensor):
                    tensors_to_remove.append(tensor_name)
                    cache_cleanup_result['tensors_removed'] += 1
                    cache_cleanup_result['memory_freed_mb'] += self.tensor_sizes.get(tensor_name, 0) / (1024**2)
                    cache_cleanup_result['removed_tensors'].append(tensor_name)
            
            # í…ì„œ ì œê±°
            for tensor_name in tensors_to_remove:
                del self.cached_tensors[tensor_name]
                if tensor_name in self.tensor_sizes:
                    del self.tensor_sizes[tensor_name]
            
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.logger.info(f"âœ… í…ì„œ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {cache_cleanup_result['tensors_removed']}ê°œ í…ì„œ ì œê±°")
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            cache_cleanup_result['error'] = str(e)
        
        return cache_cleanup_result
    
    def _is_tensor_valid(self, tensor) -> bool:
        """í…ì„œ ìœ íš¨ì„± í™•ì¸"""
        try:
            # í…ì„œê°€ ì—¬ì „íˆ ìœ íš¨í•œì§€ í™•ì¸
            if hasattr(tensor, 'device'):
                # ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
                return True
            return False
        except Exception:
            return False
    
    def cache_tensor(self, tensor_name: str, tensor, size_mb: float = None):
        """í…ì„œ ìºì‹±"""
        try:
            if size_mb is None:
                # í…ì„œ í¬ê¸° ìë™ ê³„ì‚°
                if hasattr(tensor, 'element_size'):
                    size_mb = tensor.element_size() * tensor.nelement() / (1024**2)
                else:
                    size_mb = 0.0
            
            self.cached_tensors[tensor_name] = tensor
            self.tensor_sizes[tensor_name] = size_mb * (1024**2)  # MBë¥¼ bytesë¡œ ë³€í™˜
            
            self.logger.debug(f"í…ì„œ ìºì‹±: {tensor_name} ({size_mb:.2f}MB)")
            
        except Exception as e:
            self.logger.warning(f"í…ì„œ ìºì‹± ì‹¤íŒ¨: {tensor_name} - {e}")
    
    def get_cached_tensor(self, tensor_name: str):
        """ìºì‹œëœ í…ì„œ ì¡°íšŒ"""
        return self.cached_tensors.get(tensor_name)
    
    def remove_cached_tensor(self, tensor_name: str) -> bool:
        """ìºì‹œëœ í…ì„œ ì œê±°"""
        try:
            if tensor_name in self.cached_tensors:
                del self.cached_tensors[tensor_name]
                if tensor_name in self.tensor_sizes:
                    del self.tensor_sizes[tensor_name]
                
                self.logger.debug(f"ìºì‹œëœ í…ì„œ ì œê±°: {tensor_name}")
                return True
            
            return False
            
        except Exception as e:
            self.logger.warning(f"ìºì‹œëœ í…ì„œ ì œê±° ì‹¤íŒ¨: {tensor_name} - {e}")
            return False
    
    def get_memory_statistics(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ"""
        stats = {}
        
        try:
            # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ
            current_memory = self.get_memory_usage()
            stats['current_memory'] = current_memory
            
            # íˆìŠ¤í† ë¦¬ í†µê³„
            if self.memory_usage_history:
                memory_values = [h['memory_usage'].get('system_used_gb', 0) for h in self.memory_usage_history]
                stats['history_stats'] = {
                    'peak_memory_gb': max(memory_values),
                    'average_memory_gb': np.mean(memory_values),
                    'min_memory_gb': min(memory_values),
                    'total_samples': len(memory_values)
                }
            
            # í”„ë¡œíŒŒì¼ í†µê³„
            if self.memory_profiles:
                stats['profile_stats'] = {
                    'total_profiles': len(self.memory_profiles),
                    'latest_profile': self.memory_profiles[-1] if self.memory_profiles else None
                }
            
            # ìºì‹œ í†µê³„
            stats['cache_stats'] = {
                'cached_tensors_count': len(self.cached_tensors),
                'total_cache_size_mb': sum(self.tensor_sizes.values()) / (1024**2),
                'largest_tensor_mb': max(self.tensor_sizes.values()) / (1024**2) if self.tensor_sizes else 0
            }
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            stats = {'error': str(e)}
        
        return stats
    
    def optimize_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        optimization_results = {
            'timestamp': time.time(),
            'optimizations_applied': [],
            'memory_saved_mb': 0.0
        }
        
        try:
            # 1ë‹¨ê³„: ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_result = self.cleanup_memory(force=True)
            optimization_results['cleanup_result'] = cleanup_result
            
            # 2ë‹¨ê³„: ìºì‹œ í¬ê¸° ìµœì í™”
            if len(self.cached_tensors) > 100:
                # ê°€ì¥ í° í…ì„œë“¤ë¶€í„° ì œê±°
                sorted_tensors = sorted(self.tensor_sizes.items(), key=lambda x: x[1], reverse=True)
                tensors_to_remove = sorted_tensors[50:]  # ìƒìœ„ 50ê°œë§Œ ìœ ì§€
                
                for tensor_name, size in tensors_to_remove:
                    if self.remove_cached_tensor(tensor_name):
                        optimization_results['memory_saved_mb'] += size / (1024**2)
                        optimization_results['optimizations_applied'].append(f"í° í…ì„œ ì œê±°: {tensor_name}")
            
            # 3ë‹¨ê³„: ë©”ëª¨ë¦¬ ì••ì¶•
            if hasattr(torch, 'compiled'):
                # PyTorch 2.0+ ì»´íŒŒì¼ ìµœì í™”
                optimization_results['optimizations_applied'].append("PyTorch ì»´íŒŒì¼ ìµœì í™”")
            
            self.logger.info("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            optimization_results['error'] = str(e)
        
        return optimization_results

# ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def create_memory_service(config: MemoryConfig = None) -> MemoryService:
    """Memory Service ìƒì„±"""
    return MemoryService(config)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # ì„¤ì • ìƒì„±
    config = MemoryConfig(
        max_memory_gb=8.0,
        cleanup_threshold=0.8,
        enable_auto_cleanup=True
    )
    
    # ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ìƒì„±
    service = create_memory_service(config)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
    memory_usage = service.get_memory_usage()
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}")
    
    # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    monitoring_data = service.monitor_memory_usage()
    print(f"ëª¨ë‹ˆí„°ë§ ê²°ê³¼: {monitoring_data}")
    
    # ë©”ëª¨ë¦¬ í†µê³„
    stats = service.get_memory_statistics()
    print(f"ë©”ëª¨ë¦¬ í†µê³„: {stats}")
