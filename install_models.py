#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyCloset AI - AI ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (ìˆ˜ì •ëœ ë²„ì „)
==========================================================

ğŸ¤– ê¸°ëŠ¥:
- ì‹¤ì œ Hugging Face ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©
- ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦
- ë©”ëª¨ë¦¬ ìµœì í™” ë‹¤ìš´ë¡œë“œ
- M3 Max íŠ¹í™” ì„¤ì •
- ì§„í–‰ë¥  í‘œì‹œ

ğŸ’¡ ì‚¬ìš©ë²•:
python install_models.py --all                # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python install_models.py --essential          # í•„ìˆ˜ ëª¨ë¸ë§Œ
python install_models.py --model human_parsing # íŠ¹ì • ëª¨ë¸ë§Œ
"""

import os
import sys
import json
import hashlib
import argparse
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import urllib.request
from urllib.parse import urlparse

# ì§„í–‰ë¥  í‘œì‹œ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    
    class tqdm:
        def __init__(self, *args, **kwargs):
            self.total = kwargs.get('total', 100)
            self.desc = kwargs.get('desc', 'Progress')
            self.current = 0
        
        def update(self, n=1):
            self.current += n
            percent = (self.current / self.total) * 100
            print(f"\r{self.desc}: {percent:.1f}%", end='', flush=True)
        
        def close(self):
            print()
        
        def __enter__(self):
            return self
        
        def __exit__(self, *args):
            self.close()

# Hugging Face Hub
try:
    from huggingface_hub import hf_hub_download, snapshot_download, list_repo_files
    HF_HUB_AVAILABLE = True
except ImportError:
    HF_HUB_AVAILABLE = False
    print("âš ï¸ huggingface_hubê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ: pip install huggingface_hub")

# ============================================================================
# ğŸ“‹ í”„ë¡œì íŠ¸ ì„¤ì •
# ============================================================================

PROJECT_ROOT = Path(__file__).parent.absolute()
BACKEND_ROOT = PROJECT_ROOT / "backend"
AI_MODELS_ROOT = BACKEND_ROOT / "ai_models"

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
AI_MODELS_ROOT.mkdir(parents=True, exist_ok=True)

# ============================================================================
# ğŸ¤– AI ëª¨ë¸ ì •ì˜ (ì‹¤ì œ Hugging Face ê²½ë¡œ ê¸°ë°˜)
# ============================================================================

@dataclass
class ModelInfo:
    """AI ëª¨ë¸ ì •ë³´"""
    name: str
    description: str
    repo_id: str
    files: List[str]
    size_mb: float
    is_essential: bool = False
    local_path: Optional[str] = None
    checksum: Optional[str] = None
    download_url: Optional[str] = None

# MyCloset AIì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë“¤ (ìˆ˜ì •ëœ ì‹¤ì œ ê²½ë¡œ)
AI_MODELS = {
    # ========================================================================
    # ğŸ”¥ Step 1: Human Parsing (í•„ìˆ˜) - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ ì‚¬ìš©
    # ========================================================================
    "human_parsing_schp": ModelInfo(
        name="Human Parsing (SCHP)",
        description="Self-Correction Human Parsing ëª¨ë¸",
        repo_id="mattmdjaga/segformer_b2_clothes",
        files=["pytorch_model.bin", "config.json"],
        size_mb=255.1,
        is_essential=True,
        local_path="step_01_human_parsing"
    ),
    
    # ========================================================================
    # ğŸ”¥ Step 2: Pose Estimation (í•„ìˆ˜) - ControlNet ì‚¬ìš©
    # ========================================================================
    "pose_estimation_controlnet": ModelInfo(
        name="Pose Estimation (ControlNet)",
        description="ControlNet OpenPose ê¸°ë°˜ ìì„¸ ì¶”ì • ëª¨ë¸",
        repo_id="lllyasviel/control_v11p_sd15_openpose",
        files=["diffusion_pytorch_model.bin", "config.json"],
        size_mb=1400.0,
        is_essential=True,
        local_path="step_02_pose_estimation"
    ),
    
    # ========================================================================
    # ğŸ”¥ Step 3: Cloth Segmentation (í•„ìˆ˜) - U2Net ëŒ€ì²´
    # ========================================================================
    "cloth_segmentation_rembg": ModelInfo(
        name="Cloth Segmentation (REMBG)",
        description="REMBG ê¸°ë°˜ ì˜ë¥˜ ë¶„í•  ëª¨ë¸",
        repo_id="skytnt/anime-seg",
        files=["isnetis.onnx"],
        size_mb=168.1,
        is_essential=True,
        local_path="step_03_cloth_segmentation"
    ),
    
    # ========================================================================
    # ğŸ”¥ Step 6: Virtual Fitting (í•µì‹¬!) - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” OOTDiffusion
    # ========================================================================
    "virtual_fitting_ootd": ModelInfo(
        name="Virtual Fitting (OOTDiffusion)",
        description="OOTDiffusion ê°€ìƒ í”¼íŒ… ëª¨ë¸",
        repo_id="levihsu/OOTDiffusion",
        files=[
            "ootd_hd/pytorch_model.bin",
            "ootd_hd/config.json"
        ],
        size_mb=577.2,
        is_essential=True,
        local_path="step_06_virtual_fitting"
    ),
    
    # ========================================================================
    # ğŸ”¥ ë³´ì¡° ëª¨ë¸ë“¤ (ì„ íƒì ) - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë“¤
    # ========================================================================
    "stable_diffusion_base": ModelInfo(
        name="Stable Diffusion 1.5",
        description="Stable Diffusion v1.5 ê¸°ë³¸ ëª¨ë¸",
        repo_id="runwayml/stable-diffusion-v1-5",
        files=[
            "text_encoder/pytorch_model.bin",
            "unet/diffusion_pytorch_model.bin",
            "vae/diffusion_pytorch_model.bin"
        ],
        size_mb=4000.0,
        is_essential=False,
        local_path="stable_diffusion_v15"
    ),
    
    "clip_vit_large": ModelInfo(
        name="CLIP ViT Large",
        description="OpenAI CLIP ViT-Large ëª¨ë¸",
        repo_id="openai/clip-vit-large-patch14",
        files=["pytorch_model.bin", "config.json"],
        size_mb=890.0,
        is_essential=False,
        local_path="clip_vit_large"
    ),
    
    "depth_estimation": ModelInfo(
        name="Depth Estimation",
        description="MiDaS ê¹Šì´ ì¶”ì • ëª¨ë¸",
        repo_id="Intel/dpt-large",
        files=["pytorch_model.bin", "config.json"],
        size_mb=1300.0,
        is_essential=False,
        local_path="depth_estimation"
    ),
}

# ============================================================================
# ğŸ”§ ë‹¤ìš´ë¡œë“œ ìœ í‹¸ë¦¬í‹° (ê°œì„ ëœ ë²„ì „)
# ============================================================================

class ModelDownloader:
    """AI ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, base_path: Path = AI_MODELS_ROOT):
        self.base_path = base_path
        self.base_path.mkdir(parents=True, exist_ok=True)
        self.download_stats = {
            'total_models': 0,
            'downloaded': 0,
            'skipped': 0,
            'failed': 0,
            'total_size_mb': 0.0
        }
    
    def check_model_exists(self, model_info: ModelInfo) -> bool:
        """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        if model_info.local_path:
            local_dir = self.base_path / model_info.local_path
            if local_dir.exists():
                # ëª¨ë“  í•„ìˆ˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                for file_name in model_info.files:
                    file_path = local_dir / Path(file_name).name
                    if not file_path.exists():
                        return False
                return True
        return False
    
    def list_repo_files_safe(self, repo_id: str) -> List[str]:
        """ì €ì¥ì†Œ íŒŒì¼ ëª©ë¡ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if HF_HUB_AVAILABLE:
                return list_repo_files(repo_id)
            else:
                return []
        except Exception as e:
            print(f"âš ï¸ {repo_id} íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def find_alternative_files(self, repo_id: str, target_files: List[str]) -> List[str]:
        """ëŒ€ì²´ íŒŒì¼ ì°¾ê¸°"""
        try:
            all_files = self.list_repo_files_safe(repo_id)
            found_files = []
            
            for target_file in target_files:
                # ì •í™•í•œ íŒŒì¼ëª… ë¨¼ì € í™•ì¸
                if target_file in all_files:
                    found_files.append(target_file)
                    continue
                
                # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë¹„ìŠ·í•œ íŒŒì¼ ì°¾ê¸°
                target_name = Path(target_file).name
                target_ext = Path(target_file).suffix
                
                alternatives = [
                    f for f in all_files 
                    if f.endswith(target_ext) and (
                        target_name.replace('_', '-') in f or
                        target_name.replace('-', '_') in f or
                        Path(f).name == target_name
                    )
                ]
                
                if alternatives:
                    found_files.append(alternatives[0])
                    print(f"   ğŸ”„ ëŒ€ì²´ íŒŒì¼ ì‚¬ìš©: {target_file} â†’ {alternatives[0]}")
                else:
                    print(f"   âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {target_file}")
            
            return found_files
            
        except Exception as e:
            print(f"   âš ï¸ ëŒ€ì²´ íŒŒì¼ íƒìƒ‰ ì‹¤íŒ¨: {e}")
            return target_files
    
    def download_from_huggingface(self, model_info: ModelInfo) -> bool:
        """Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê°œì„ ëœ ë²„ì „)"""
        if not HF_HUB_AVAILABLE:
            print(f"âŒ Hugging Face Hubê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            return False
        
        try:
            local_dir = self.base_path / model_info.local_path
            local_dir.mkdir(parents=True, exist_ok=True)
            
            print(f"ğŸ“¥ {model_info.name} ë‹¤ìš´ë¡œë“œ ì¤‘...")
            print(f"   ì €ì¥ì†Œ: {model_info.repo_id}")
            print(f"   ì˜ˆìƒ í¬ê¸°: {model_info.size_mb:.1f}MB")
            
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ í™•ì¸ ë° ëŒ€ì²´ íŒŒì¼ ì°¾ê¸°
            available_files = self.find_alternative_files(model_info.repo_id, model_info.files)
            
            if not available_files:
                print(f"   âŒ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            downloaded_count = 0
            for file_name in available_files:
                try:
                    print(f"   ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {file_name}")
                    
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    downloaded_path = hf_hub_download(
                        repo_id=model_info.repo_id,
                        filename=file_name,
                        cache_dir=str(local_dir.parent / "cache"),
                        force_download=False
                    )
                    
                    # íŒŒì¼ì„ ì§€ì •ëœ ìœ„ì¹˜ë¡œ ë³µì‚¬
                    target_path = local_dir / Path(file_name).name
                    if Path(downloaded_path) != target_path:
                        shutil.copy2(downloaded_path, target_path)
                    
                    file_size = target_path.stat().st_size / (1024*1024)
                    print(f"   âœ… {file_name} ({file_size:.1f}MB)")
                    downloaded_count += 1
                    
                except Exception as e:
                    print(f"   âŒ {file_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            if downloaded_count > 0:
                print(f"âœ… {model_info.name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({downloaded_count}/{len(available_files)} íŒŒì¼)")
                return True
            else:
                print(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ëª¨ë“  íŒŒì¼ ì‹¤íŒ¨)")
                return False
            
        except Exception as e:
            print(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def download_large_model_in_chunks(self, model_info: ModelInfo) -> bool:
        """ëŒ€ìš©ëŸ‰ ëª¨ë¸ ì²­í¬ ë‹¨ìœ„ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ëŒ€ìš©ëŸ‰ ëª¨ë¸ì˜ ê²½ìš° snapshot_download ì‚¬ìš©
            if model_info.size_mb > 1000:
                print(f"ğŸ“¥ ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {model_info.name}")
                
                local_dir = self.base_path / model_info.local_path
                
                snapshot_download(
                    repo_id=model_info.repo_id,
                    local_dir=str(local_dir),
                    cache_dir=str(self.base_path / "cache"),
                    resume_download=True
                )
                
                print(f"âœ… {model_info.name} ëŒ€ìš©ëŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                return self.download_from_huggingface(model_info)
                
        except Exception as e:
            print(f"âŒ ëŒ€ìš©ëŸ‰ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self.download_from_huggingface(model_info)
    
    def verify_checksum(self, model_info: ModelInfo) -> bool:
        """ì²´í¬ì„¬ ê²€ì¦"""
        if not model_info.checksum or not model_info.local_path:
            return True  # ì²´í¬ì„¬ì´ ì—†ìœ¼ë©´ ê²€ì¦ ìƒëµ
        
        try:
            local_dir = self.base_path / model_info.local_path
            
            # ëª¨ë“  íŒŒì¼ì˜ í•´ì‹œ ê³„ì‚°
            hasher = hashlib.sha256()
            for file_name in model_info.files:
                file_path = local_dir / Path(file_name).name
                if file_path.exists():
                    with open(file_path, 'rb') as f:
                        for chunk in iter(lambda: f.read(4096), b""):
                            hasher.update(chunk)
            
            calculated_hash = hasher.hexdigest()
            if calculated_hash != model_info.checksum:
                print(f"âš ï¸ {model_info.name} ì²´í¬ì„¬ ë¶ˆì¼ì¹˜")
                return False
            
            print(f"âœ… {model_info.name} ì²´í¬ì„¬ ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ {model_info.name} ì²´í¬ì„¬ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def download_model(self, model_key: str, model_info: ModelInfo) -> bool:
        """ê°œë³„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        self.download_stats['total_models'] += 1
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if self.check_model_exists(model_info):
            print(f"â­ï¸  {model_info.name} ì´ë¯¸ ì¡´ì¬í•¨ (ê±´ë„ˆë›°ê¸°)")
            self.download_stats['skipped'] += 1
            return True
        
        # ë‹¤ìš´ë¡œë“œ ì‹œë„
        success = False
        
        # 1. ëŒ€ìš©ëŸ‰ ëª¨ë¸ ì²­í¬ ë‹¤ìš´ë¡œë“œ ì‹œë„
        if model_info.size_mb > 1000:
            success = self.download_large_model_in_chunks(model_info)
        else:
            # 2. ì¼ë°˜ Hugging Face ë‹¤ìš´ë¡œë“œ ì‹œë„
            success = self.download_from_huggingface(model_info)
        
        if success:
            # ì²´í¬ì„¬ ê²€ì¦
            if self.verify_checksum(model_info):
                self.download_stats['downloaded'] += 1
                self.download_stats['total_size_mb'] += model_info.size_mb
                return True
            else:
                success = False
        
        if not success:
            self.download_stats['failed'] += 1
            print(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        
        return success
    
    def download_models(self, model_keys: List[str], 
                       parallel: bool = False, max_workers: int = 2) -> bool:
        """ì—¬ëŸ¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        print(f"ğŸ¤– AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘ ({len(model_keys)}ê°œ ëª¨ë¸)")
        print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {self.base_path}")
        print("=" * 60)
        
        if parallel and len(model_keys) > 1:
            # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ (ëŒ€ìš©ëŸ‰ ëª¨ë¸ì€ ìˆœì°¨ ì²˜ë¦¬)
            sequential_models = []
            parallel_models = []
            
            for key in model_keys:
                if key in AI_MODELS:
                    if AI_MODELS[key].size_mb > 1000:
                        sequential_models.append(key)
                    else:
                        parallel_models.append(key)
            
            # ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¨¼ì € ìˆœì°¨ ì²˜ë¦¬
            for key in sequential_models:
                if key in AI_MODELS:
                    self.download_model(key, AI_MODELS[key])
                    print()
            
            # ì†Œìš©ëŸ‰ ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬
            if parallel_models:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = {}
                    for key in parallel_models:
                        if key in AI_MODELS:
                            future = executor.submit(
                                self.download_model, key, AI_MODELS[key]
                            )
                            futures[future] = key
                    
                    for future in as_completed(futures):
                        key = futures[future]
                        try:
                            future.result()
                        except Exception as e:
                            print(f"âŒ {key} ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            # ìˆœì°¨ ë‹¤ìš´ë¡œë“œ
            for key in model_keys:
                if key in AI_MODELS:
                    self.download_model(key, AI_MODELS[key])
                    print()  # ë¹ˆ ì¤„
        
        # ë‹¤ìš´ë¡œë“œ í†µê³„ ì¶œë ¥
        self.print_download_stats()
        
        return self.download_stats['failed'] == 0
    
    def print_download_stats(self):
        """ë‹¤ìš´ë¡œë“œ í†µê³„ ì¶œë ¥"""
        stats = self.download_stats
        print("=" * 60)
        print("ğŸ“Š ë‹¤ìš´ë¡œë“œ í†µê³„:")
        print(f"   ì „ì²´ ëª¨ë¸: {stats['total_models']}ê°œ")
        print(f"   ë‹¤ìš´ë¡œë“œ: {stats['downloaded']}ê°œ")
        print(f"   ê±´ë„ˆë›°ê¸°: {stats['skipped']}ê°œ")
        print(f"   ì‹¤íŒ¨: {stats['failed']}ê°œ")
        print(f"   ì´ í¬ê¸°: {stats['total_size_mb']:.1f}MB")
        
        if stats['failed'] == 0:
            print("âœ… ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            print(f"âš ï¸ {stats['failed']}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

# ============================================================================
# ğŸ› ï¸ ì¶”ê°€ ìœ í‹¸ë¦¬í‹°
# ============================================================================

def create_model_config():
    """ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
    config = {
        "models": {},
        "paths": {
            "base_path": str(AI_MODELS_ROOT),
            "cache_path": str(AI_MODELS_ROOT / "cache")
        },
        "settings": {
            "device": "mps",  # M3 Max ê¸°ë³¸ê°’
            "precision": "fp16",
            "memory_optimization": True
        }
    }
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ ì •ë³´ ì¶”ê°€
    for key, model_info in AI_MODELS.items():
        if model_info.local_path:
            local_dir = AI_MODELS_ROOT / model_info.local_path
            if local_dir.exists():
                config["models"][key] = {
                    "name": model_info.name,
                    "path": str(local_dir),
                    "size_mb": model_info.size_mb,
                    "files": model_info.files,
                    "is_essential": model_info.is_essential
                }
    
    # ì„¤ì • íŒŒì¼ ì €ì¥
    config_file = AI_MODELS_ROOT / "model_config.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")

def check_disk_space(required_mb: float) -> bool:
    """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸"""
    try:
        import shutil
        total, used, free = shutil.disk_usage(AI_MODELS_ROOT)
        free_mb = free / (1024 * 1024)
        
        print(f"ğŸ’¾ ë””ìŠ¤í¬ ê³µê°„: {free_mb:.1f}MB ì‚¬ìš© ê°€ëŠ¥")
        print(f"ğŸ“¦ í•„ìš” ê³µê°„: {required_mb:.1f}MB")
        
        if free_mb < required_mb:
            print(f"âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ({required_mb - free_mb:.1f}MB ì¶”ê°€ í•„ìš”)")
            return False
        
        return True
        
    except Exception as e:
        print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # í™•ì¸ ì‹¤íŒ¨ì‹œ ê³„ì† ì§„í–‰

def print_model_list():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
    print("ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸:")
    print("=" * 80)
    
    essential_models = []
    optional_models = []
    
    for key, model_info in AI_MODELS.items():
        if model_info.is_essential:
            essential_models.append((key, model_info))
        else:
            optional_models.append((key, model_info))
    
    print("\nğŸ”¥ í•„ìˆ˜ ëª¨ë¸:")
    for key, model in essential_models:
        downloader = ModelDownloader()
        status = "âœ…" if downloader.check_model_exists(model) else "â¬œ"
        print(f"   {status} {key}: {model.name} ({model.size_mb:.1f}MB)")
        print(f"      {model.description}")
    
    print("\nğŸ“¦ ì„ íƒì  ëª¨ë¸:")
    for key, model in optional_models:
        downloader = ModelDownloader()
        status = "âœ…" if downloader.check_model_exists(model) else "â¬œ"
        print(f"   {status} {key}: {model.name} ({model.size_mb:.1f}MB)")
        print(f"      {model.description}")
    
    total_size = sum(model.size_mb for model in AI_MODELS.values())
    essential_size = sum(model.size_mb for model in AI_MODELS.values() if model.is_essential)
    
    print(f"\nğŸ“Š í¬ê¸° ìš”ì•½:")
    print(f"   í•„ìˆ˜ ëª¨ë¸: {essential_size:.1f}MB")
    print(f"   ì „ì²´ ëª¨ë¸: {total_size:.1f}MB")

# ============================================================================
# ğŸš€ ë©”ì¸ í•¨ìˆ˜
# ============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë„êµ¬ (ìˆ˜ì •ëœ ë²„ì „)")
    
    # ë‹¤ìš´ë¡œë“œ ëª¨ë“œ
    parser.add_argument('--all', action='store_true', 
                       help='ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--essential', action='store_true', 
                       help='í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--model', type=str, 
                       help='íŠ¹ì • ëª¨ë¸ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--models', nargs='+', 
                       help='ì—¬ëŸ¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ')
    
    # ì˜µì…˜
    parser.add_argument('--list', action='store_true', 
                       help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥')
    parser.add_argument('--check', action='store_true', 
                       help='ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸')
    parser.add_argument('--parallel', action='store_true', 
                       help='ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ (ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ)')
    parser.add_argument('--max-workers', type=int, default=2, 
                       help='ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì›Œì»¤ ìˆ˜')
    parser.add_argument('--force', action='store_true', 
                       help='ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°')
    
    args = parser.parse_args()
    
    print("ğŸ¤– MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë„êµ¬ (ì‹¤ì œ Hugging Face ê²½ë¡œ)")
    print("=" * 50)
    
    # ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    if args.list:
        print_model_list()
        return
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
    if args.check:
        check_installed_models()
        return
    
    # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ê²°ì •
    models_to_download = []
    
    if args.all:
        models_to_download = list(AI_MODELS.keys())
        print("ğŸ“¦ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëª¨ë“œ")
    elif args.essential:
        models_to_download = [key for key, model in AI_MODELS.items() if model.is_essential]
        print("ğŸ”¥ í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ ëª¨ë“œ")
    elif args.model:
        if args.model in AI_MODELS:
            models_to_download = [args.model]
            print(f"ğŸ¯ íŠ¹ì • ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {args.model}")
        else:
            print(f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {args.model}")
            print_model_list()
            return
    elif args.models:
        valid_models = [m for m in args.models if m in AI_MODELS]
        invalid_models = [m for m in args.models if m not in AI_MODELS]
        
        if invalid_models:
            print(f"âŒ ì˜ëª»ëœ ëª¨ë¸: {', '.join(invalid_models)}")
            print_model_list()
            return
        
        models_to_download = valid_models
        print(f"ğŸ¯ ì„ íƒëœ ëª¨ë¸ë“¤: {', '.join(models_to_download)}")
    else:
        # ê¸°ë³¸ê°’: í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        models_to_download = [key for key, model in AI_MODELS.items() if model.is_essential]
        print("ğŸ”¥ ê¸°ë³¸ ëª¨ë“œ: í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        print("ğŸ’¡ ëª¨ë“  ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´: python install_models.py --all")
    
    if not models_to_download:
        print("âŒ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•„ìš”í•œ ë””ìŠ¤í¬ ê³µê°„ ê³„ì‚°
    total_size = sum(AI_MODELS[key].size_mb for key in models_to_download)
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    if not check_disk_space(total_size * 1.2):  # 20% ì—¬ìœ ë¶„
        if not input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() == 'y':
            return
    
    # Hugging Face Hub í™•ì¸
    if not HF_HUB_AVAILABLE:
        print("âŒ huggingface_hub íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:")
        print("   pip install huggingface_hub")
        return
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    downloader = ModelDownloader()
    
    print(f"\nğŸš€ {len(models_to_download)}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {AI_MODELS_ROOT}")
    print(f"ğŸ’¾ ì˜ˆìƒ í¬ê¸°: {total_size:.1f}MB")
    
    if not args.force:
        if input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
            return
    
    success = downloader.download_models(
        models_to_download, 
        parallel=args.parallel, 
        max_workers=args.max_workers
    )
    
    # ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±
    create_model_config()
    
    if success:
        print("\nğŸ‰ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. conda activate mycloset-ai-clean")
        print("2. cd backend && python app/main.py")
        print("3. ë°±ì—”ë“œê°€ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤")
    else:
        print("\nâš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("í•˜ì§€ë§Œ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def check_installed_models():
    """ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸"""
    print("ğŸ“‹ ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸:")
    print("=" * 50)
    
    downloader = ModelDownloader()
    installed_count = 0
    total_size = 0.0
    
    for key, model_info in AI_MODELS.items():
        exists = downloader.check_model_exists(model_info)
        status = "âœ… ì„¤ì¹˜ë¨" if exists else "âŒ ì—†ìŒ"
        essential = "ğŸ”¥ í•„ìˆ˜" if model_info.is_essential else "ğŸ“¦ ì„ íƒì "
        
        print(f"{essential} {key}: {status}")
        print(f"   ì´ë¦„: {model_info.name}")
        print(f"   í¬ê¸°: {model_info.size_mb:.1f}MB")
        
        if exists:
            installed_count += 1
            total_size += model_info.size_mb
            
            # íŒŒì¼ ìƒì„¸ ì •ë³´
            if model_info.local_path:
                local_dir = AI_MODELS_ROOT / model_info.local_path
                print(f"   ê²½ë¡œ: {local_dir}")
                for file_name in model_info.files:
                    file_path = local_dir / Path(file_name).name
                    if file_path.exists():
                        file_size = file_path.stat().st_size / (1024*1024)
                        print(f"     âœ… {file_name} ({file_size:.1f}MB)")
                    else:
                        print(f"     âŒ {file_name}")
        print()
    
    print(f"ğŸ“Š ìš”ì•½:")
    print(f"   ì„¤ì¹˜ëœ ëª¨ë¸: {installed_count}/{len(AI_MODELS)}ê°œ")
    print(f"   ì´ í¬ê¸°: {total_size:.1f}MB")
    
    # í•„ìˆ˜ ëª¨ë¸ í™•ì¸
    essential_models = [key for key, model in AI_MODELS.items() if model.is_essential]
    installed_essential = [key for key in essential_models 
                          if downloader.check_model_exists(AI_MODELS[key])]
    
    if len(installed_essential) == len(essential_models):
        print("âœ… ëª¨ë“  í•„ìˆ˜ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        missing = [key for key in essential_models if key not in installed_essential]
        print(f"âš ï¸ ëˆ„ë½ëœ í•„ìˆ˜ ëª¨ë¸: {', '.join(missing)}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: python install_models.py --essential")

if __name__ == "__main__":
    main()