#!/usr/bin/env python3
"""
ğŸ§¹ MyCloset AI ìŠ¤ë§ˆíŠ¸ ë””ìŠ¤í¬ ì •ë¦¬ ì‹œìŠ¤í…œ v3.0
âœ… 500GB+ í”„ë¡œì íŠ¸ì—ì„œ í•„ìš”í•œ ê²ƒë§Œ ì•ˆì „í•˜ê²Œ ë³´ì¡´
âœ… ì¤‘ë³µ íŒŒì¼ ìë™ íƒì§€ ë° ì œê±°
âœ… ë°±ì—… íŒŒì¼ ë° ì„ì‹œ íŒŒì¼ ì •ë¦¬
âœ… AI ëª¨ë¸ ì¤‘ë³µ ì œê±° ë° ìµœì í™”
âœ… ì‚¬ìš©ì í™•ì¸ í›„ ì•ˆì „í•œ ì‚­ì œ
âœ… ìƒì„¸í•œ ê³µê°„ ì ˆì•½ ë³´ê³ ì„œ
"""

import os
import sys
import hashlib
import json
import shutil
import time
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import re

# ì§„í–‰ë¥  í‘œì‹œ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

class FileCategory(Enum):
    ESSENTIAL = "essential"        # ì ˆëŒ€ ì‚­ì œí•˜ë©´ ì•ˆë˜ëŠ” íŒŒì¼
    BACKUP = "backup"             # ë°±ì—… íŒŒì¼ (ì‚­ì œ ê°€ëŠ¥)
    DUPLICATE = "duplicate"       # ì¤‘ë³µ íŒŒì¼
    CACHE = "cache"              # ìºì‹œ íŒŒì¼
    TEMP = "temp"                # ì„ì‹œ íŒŒì¼
    LOG = "log"                  # ë¡œê·¸ íŒŒì¼
    MODEL_REDUNDANT = "model_redundant"  # ì¤‘ë³µ AI ëª¨ë¸
    LARGE_UNUSED = "large_unused"  # í° ë¯¸ì‚¬ìš© íŒŒì¼
    ARCHIVE = "archive"          # ì••ì¶• íŒŒì¼

@dataclass
class FileInfo:
    path: Path
    size_mb: float
    category: FileCategory
    priority: int = 1  # 1=ì•ˆì „ì‚­ì œ, 2=ì£¼ì˜, 3=ìœ„í—˜
    reason: str = ""
    duplicate_of: Optional[Path] = None
    last_accessed: Optional[float] = None

class SmartCleanupSystem:
    """ìŠ¤ë§ˆíŠ¸ ë””ìŠ¤í¬ ì •ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.scan_results: Dict[FileCategory, List[FileInfo]] = defaultdict(list)
        self.total_space_mb = 0
        self.potential_savings_mb = 0
        
        # í•„ìˆ˜ ë³´ì¡´ íŒ¨í„´ë“¤
        self.essential_patterns = {
            # í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
            r".*\.(py|js|ts|tsx|jsx|vue|svelte)$",
            r".*\.(html|css|scss|sass|less)$",
            r".*\.(json|yaml|yml|toml|ini|cfg)$",
            r".*\.(md|txt|rst)$",
            r".*/package\.json$",
            r".*/requirements\.txt$",
            r".*/pyproject\.toml$",
            r".*/Dockerfile$",
            r".*/docker-compose\.ya?ml$",
            r".*\.env\.example$",
            r".*/\.gitignore$",
            r".*/README\.*$",
            r".*/LICENSE$",
            r".*/Makefile$",
            
            # í•„ìˆ˜ AI ëª¨ë¸ (ìµœì‹  ë²„ì „ë§Œ)
            r".*/diffusion_pytorch_model\.safetensors$",
            r".*/pytorch_model\.bin$",
            r".*/config\.json$",
            r".*/model\.onnx$",
            r".*/(ootd|clip|stable-diffusion).*\.(safetensors|bin)$",
        }
        
        # ì•ˆì „í•˜ê²Œ ì‚­ì œ ê°€ëŠ¥í•œ íŒ¨í„´ë“¤
        self.safe_delete_patterns = {
            # ë°±ì—… íŒŒì¼ë“¤
            r".*\.backup.*$",
            r".*\.bak$",
            r".*\.old$",
            r".*_backup_\d+.*$",
            r".*\.backup\d*$",
            
            # ì„ì‹œ íŒŒì¼ë“¤
            r".*\.tmp$",
            r".*\.temp$",
            r".*/temp/.*$",
            r".*/tmp/.*$",
            r".*\.swp$",
            r".*\.swo$",
            r".*~$",
            
            # ìºì‹œ íŒŒì¼ë“¤
            r".*/__pycache__/.*$",
            r".*\.pyc$",
            r".*\.pyo$",
            r".*/\.cache/.*$",
            r".*/node_modules/.*$",
            r".*/\.next/.*$",
            r".*/dist/.*$",
            r".*/build/.*$",
            
            # ë¡œê·¸ íŒŒì¼ë“¤ (ì˜¤ë˜ëœ ê²ƒ)
            r".*\.log$",
            r".*\.log\.\d+$",
            r".*/logs/.*\.log$",
            
            # ì••ì¶•/ì•„ì¹´ì´ë¸Œ (ì†ŒìŠ¤ê°€ ìˆëŠ” ê²½ìš°)
            r".*\.zip$",
            r".*\.tar\.gz$",
            r".*\.tar\.bz2$",
            r".*\.7z$",
            r".*\.rar$",
            
            # Git ê´€ë ¨ (í° ê²ƒë“¤)
            r".*/\.git/objects/.*$",
            r".*\.bfg-report/.*$",
            
            # ì‹œìŠ¤í…œ íŒŒì¼
            r".*\.DS_Store$",
            r".*/Thumbs\.db$",
            
            # ì¤‘ë³µ ëª¨ë¸ë“¤ (íŒ¨í„´ ê¸°ë°˜)
            r".*/.*_v\d+\..*$",  # ë²„ì „ì´ ìˆëŠ” ì¤‘ë³µ
            r".*/.*_copy\..*$",   # ë³µì‚¬ë³¸
            r".*/.*_duplicate\..*$",  # ì¤‘ë³µ í‘œì‹œ
        }
        
        # ì£¼ì˜í•´ì„œ ì‚­ì œí•  íŒ¨í„´ë“¤
        self.caution_patterns = {
            r".*\.pth$",
            r".*\.ckpt$", 
            r".*\.h5$",
            r".*\.pkl$",
            r".*\.pickle$",
        }

    def scan_directory(self) -> Dict[str, float]:
        """ë””ë ‰í† ë¦¬ ì „ì²´ ìŠ¤ìº”"""
        print("ğŸ” í”„ë¡œì íŠ¸ ì „ì²´ ìŠ¤ìº” ì‹œì‘...")
        
        file_hashes = {}  # ì¤‘ë³µ íŒŒì¼ íƒì§€ìš©
        large_files = []  # 100MB ì´ìƒ íŒŒì¼ë“¤
        
        total_files = sum(1 for _ in self.project_root.rglob('*') if _.is_file())
        
        if TQDM_AVAILABLE:
            pbar = tqdm(total=total_files, desc="íŒŒì¼ ìŠ¤ìº”", unit="files")
        
        for file_path in self.project_root.rglob('*'):
            if not file_path.is_file():
                continue
                
            try:
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                self.total_space_mb += file_size_mb
                
                # 100MB ì´ìƒ íŒŒì¼ë“¤ ë³„ë„ ì¶”ì 
                if file_size_mb >= 100:
                    large_files.append((file_path, file_size_mb))
                
                # íŒŒì¼ ë¶„ë¥˜
                category = self._classify_file(file_path, file_size_mb)
                
                if category != FileCategory.ESSENTIAL:
                    # ì¤‘ë³µ íŒŒì¼ ì²´í¬ (1MB ì´ìƒë§Œ)
                    if file_size_mb >= 1:
                        file_hash = self._get_file_hash(file_path)
                        if file_hash in file_hashes:
                            # ì¤‘ë³µ ë°œê²¬
                            original_file = file_hashes[file_hash]
                            duplicate_info = FileInfo(
                                path=file_path,
                                size_mb=file_size_mb,
                                category=FileCategory.DUPLICATE,
                                priority=1,
                                reason=f"ì¤‘ë³µ íŒŒì¼ (ì›ë³¸: {original_file.name})",
                                duplicate_of=original_file
                            )
                            self.scan_results[FileCategory.DUPLICATE].append(duplicate_info)
                            self.potential_savings_mb += file_size_mb
                        else:
                            file_hashes[file_hash] = file_path
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
                    file_info = FileInfo(
                        path=file_path,
                        size_mb=file_size_mb,
                        category=category,
                        priority=self._get_priority(file_path, category),
                        reason=self._get_reason(file_path, category),
                        last_accessed=file_path.stat().st_atime
                    )
                    
                    self.scan_results[category].append(file_info)
                    
                    if category in [FileCategory.BACKUP, FileCategory.CACHE, 
                                  FileCategory.TEMP, FileCategory.LOG]:
                        self.potential_savings_mb += file_size_mb
                
                if TQDM_AVAILABLE:
                    pbar.update(1)
                    
            except (OSError, PermissionError):
                continue
        
        if TQDM_AVAILABLE:
            pbar.close()
        
        # í° íŒŒì¼ë“¤ ë¶„ì„
        self._analyze_large_files(large_files)
        
        return self._generate_scan_summary()

    def _classify_file(self, file_path: Path, size_mb: float) -> FileCategory:
        """íŒŒì¼ ë¶„ë¥˜"""
        path_str = str(file_path).lower()
        
        # í•„ìˆ˜ íŒŒì¼ ì²´í¬
        for pattern in self.essential_patterns:
            if re.match(pattern, path_str):
                return FileCategory.ESSENTIAL
        
        # ì•ˆì „ ì‚­ì œ ê°€ëŠ¥ íŒŒì¼ ì²´í¬
        for pattern in self.safe_delete_patterns:
            if re.match(pattern, path_str):
                if "backup" in pattern or ".bak" in pattern:
                    return FileCategory.BACKUP
                elif "cache" in pattern or "__pycache__" in pattern:
                    return FileCategory.CACHE
                elif "temp" in pattern or ".tmp" in pattern:
                    return FileCategory.TEMP
                elif ".log" in pattern:
                    return FileCategory.LOG
                elif any(ext in pattern for ext in [".zip", ".tar", ".7z"]):
                    return FileCategory.ARCHIVE
        
        # ì£¼ì˜ íŒŒì¼ ì²´í¬
        for pattern in self.caution_patterns:
            if re.match(pattern, path_str):
                # AI ëª¨ë¸ì¸ì§€ í™•ì¸
                if any(model in path_str for model in ["ootd", "diffusion", "clip", "stable"]):
                    return FileCategory.MODEL_REDUNDANT
        
        # í° íŒŒì¼ (500MB ì´ìƒ)ì´ë©´ì„œ ìµœê·¼ì— ì ‘ê·¼í•˜ì§€ ì•Šì€ íŒŒì¼
        if size_mb >= 500:
            last_access = file_path.stat().st_atime
            if time.time() - last_access > 30 * 24 * 3600:  # 30ì¼ ì´ìƒ
                return FileCategory.LARGE_UNUSED
        
        return FileCategory.ESSENTIAL

    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ íƒì§€ìš©)"""
        try:
            # í° íŒŒì¼ì€ ì²˜ìŒ 1MBë§Œ í•´ì‹œ
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                chunk = f.read(1024 * 1024)  # 1MB
                hasher.update(chunk)
            return hasher.hexdigest()
        except:
            return ""

    def _get_priority(self, file_path: Path, category: FileCategory) -> int:
        """ì‚­ì œ ìš°ì„ ìˆœìœ„ ë°˜í™˜"""
        if category == FileCategory.ESSENTIAL:
            return 3  # ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€
        elif category in [FileCategory.BACKUP, FileCategory.CACHE, FileCategory.TEMP]:
            return 1  # ì•ˆì „ ì‚­ì œ
        elif category == FileCategory.DUPLICATE:
            return 1  # ì•ˆì „ ì‚­ì œ
        elif category == FileCategory.LOG:
            return 1  # ì•ˆì „ ì‚­ì œ
        elif category == FileCategory.ARCHIVE:
            return 2  # ì£¼ì˜ ì‚­ì œ
        else:
            return 2  # ì£¼ì˜ ì‚­ì œ

    def _get_reason(self, file_path: Path, category: FileCategory) -> str:
        """ì‚­ì œ ì´ìœ  ë°˜í™˜"""
        reasons = {
            FileCategory.BACKUP: "ë°±ì—… íŒŒì¼",
            FileCategory.CACHE: "ìºì‹œ íŒŒì¼ (ì¬ìƒì„± ê°€ëŠ¥)",
            FileCategory.TEMP: "ì„ì‹œ íŒŒì¼",
            FileCategory.LOG: "ë¡œê·¸ íŒŒì¼",
            FileCategory.DUPLICATE: "ì¤‘ë³µ íŒŒì¼",
            FileCategory.ARCHIVE: "ì••ì¶• íŒŒì¼ (ì†ŒìŠ¤ ì¡´ì¬ì‹œ)",
            FileCategory.MODEL_REDUNDANT: "ì¤‘ë³µ AI ëª¨ë¸",
            FileCategory.LARGE_UNUSED: "í° ë¯¸ì‚¬ìš© íŒŒì¼ (30ì¼+ ë¯¸ì ‘ê·¼)"
        }
        return reasons.get(category, "ë¶„ë¥˜ë˜ì§€ ì•ŠìŒ")

    def _analyze_large_files(self, large_files: List[Tuple[Path, float]]):
        """í° íŒŒì¼ë“¤ ë¶„ì„"""
        print(f"\nğŸ“Š í° íŒŒì¼ ë¶„ì„ (100MB ì´ìƒ: {len(large_files)}ê°œ)")
        
        # í¬ê¸°ìˆœ ì •ë ¬
        large_files.sort(key=lambda x: x[1], reverse=True)
        
        for file_path, size_mb in large_files[:10]:  # ìƒìœ„ 10ê°œë§Œ
            category = self._classify_file(file_path, size_mb)
            if category != FileCategory.ESSENTIAL:
                print(f"  ğŸ“ {file_path.name}: {size_mb:.1f}MB ({category.value})")

    def _generate_scan_summary(self) -> Dict[str, float]:
        """ìŠ¤ìº” ê²°ê³¼ ìš”ì•½"""
        summary = {
            "total_space_gb": self.total_space_mb / 1024,
            "potential_savings_gb": self.potential_savings_mb / 1024,
            "savings_percentage": (self.potential_savings_mb / self.total_space_mb * 100) if self.total_space_mb > 0 else 0
        }
        
        print(f"\nğŸ“Š ìŠ¤ìº” ê²°ê³¼:")
        print(f"  ğŸ“ ì´ ìš©ëŸ‰: {summary['total_space_gb']:.1f}GB")
        print(f"  ğŸ’¾ ì ˆì•½ ê°€ëŠ¥: {summary['potential_savings_gb']:.1f}GB")
        print(f"  ğŸ“ˆ ì ˆì•½ë¥ : {summary['savings_percentage']:.1f}%")
        
        return summary

    def generate_cleanup_report(self) -> str:
        """ì •ë¦¬ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("ğŸ§¹ MyCloset AI ë””ìŠ¤í¬ ì •ë¦¬ ë³´ê³ ì„œ")
        report.append("=" * 50)
        report.append(f"ğŸ“… ìŠ¤ìº” ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ“ ìŠ¤ìº” ê²½ë¡œ: {self.project_root}")
        report.append(f"ğŸ’¾ ì´ ìš©ëŸ‰: {self.total_space_mb / 1024:.1f}GB")
        report.append(f"ğŸ¯ ì ˆì•½ ê°€ëŠ¥: {self.potential_savings_mb / 1024:.1f}GB")
        report.append("")
        
        for category, files in self.scan_results.items():
            if not files:
                continue
                
            total_size = sum(f.size_mb for f in files)
            report.append(f"ğŸ“‹ {category.value.upper()} ({len(files)}ê°œ íŒŒì¼, {total_size:.1f}MB)")
            
            # í¬ê¸°ìˆœ ì •ë ¬í•´ì„œ ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
            sorted_files = sorted(files, key=lambda x: x.size_mb, reverse=True)
            for file_info in sorted_files[:5]:
                report.append(f"  ğŸ“„ {file_info.path.name}: {file_info.size_mb:.1f}MB")
                report.append(f"      ì´ìœ : {file_info.reason}")
            
            if len(files) > 5:
                report.append(f"  ... ë° {len(files) - 5}ê°œ ë”")
            report.append("")
        
        return "\n".join(report)

    def execute_cleanup(self, categories: List[FileCategory], dry_run: bool = True) -> Dict[str, int]:
        """ì •ë¦¬ ì‹¤í–‰"""
        if dry_run:
            print("ğŸ§ª ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•ŠìŒ)")
        else:
            print("ğŸš¨ ì‹¤ì œ ì‚­ì œ ëª¨ë“œ")
        
        results = {"deleted_files": 0, "freed_space_mb": 0, "errors": 0}
        
        for category in categories:
            files = self.scan_results.get(category, [])
            if not files:
                continue
                
            print(f"\nğŸ—‚ï¸ {category.value} ì²˜ë¦¬ ì¤‘... ({len(files)}ê°œ íŒŒì¼)")
            
            if TQDM_AVAILABLE:
                pbar = tqdm(files, desc=f"ì‚­ì œ: {category.value}")
            else:
                pbar = files
                
            for file_info in pbar:
                try:
                    if not dry_run:
                        if file_info.path.is_file():
                            file_info.path.unlink()
                        elif file_info.path.is_dir():
                            shutil.rmtree(file_info.path)
                    
                    results["deleted_files"] += 1
                    results["freed_space_mb"] += file_info.size_mb
                    
                except Exception as e:
                    results["errors"] += 1
                    print(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {file_info.path} ({e})")
        
        print(f"\nâœ… ì •ë¦¬ ì™„ë£Œ:")
        print(f"  ğŸ—‘ï¸ ì‚­ì œëœ íŒŒì¼: {results['deleted_files']}ê°œ")
        print(f"  ğŸ’¾ í™•ë³´ëœ ê³µê°„: {results['freed_space_mb'] / 1024:.1f}GB")
        print(f"  âŒ ì˜¤ë¥˜: {results['errors']}ê°œ")
        
        return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§¹ MyCloset AI ìŠ¤ë§ˆíŠ¸ ë””ìŠ¤í¬ ì •ë¦¬ ì‹œìŠ¤í…œ v3.0")
    print("=" * 60)
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    current_dir = Path.cwd()
    project_candidates = [
        current_dir,
        current_dir / "mycloset-ai",
        current_dir.parent,
        Path("/Users/gimdudeul/MVP/mycloset-ai")
    ]
    
    project_root = None
    for candidate in project_candidates:
        if (candidate / "backend").exists() or candidate.name == "mycloset-ai":
            project_root = candidate
            break
    
    if not project_root:
        project_root = current_dir
        print(f"âš ï¸ MyCloset AI í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {project_root}")
    else:
        print(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    # ì •ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    cleanup_system = SmartCleanupSystem(project_root)
    
    # ìŠ¤ìº” ì‹¤í–‰
    summary = cleanup_system.scan_directory()
    
    # ë³´ê³ ì„œ ìƒì„±
    report = cleanup_system.generate_cleanup_report()
    
    # ë³´ê³ ì„œ ì €ì¥
    report_file = project_root / f"cleanup_report_{int(time.time())}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“‹ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\nğŸ¯ ì •ë¦¬ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ğŸš€ ì•ˆì „ ì •ë¦¬ (ë°±ì—…, ìºì‹œ, ì„ì‹œíŒŒì¼, ì¤‘ë³µíŒŒì¼)")
    print("2. âš¡ ì ê·¹ ì •ë¦¬ (+ ë¡œê·¸, ì••ì¶•íŒŒì¼)")
    print("3. ğŸ”¥ ì „ì²´ ì •ë¦¬ (+ í° ë¯¸ì‚¬ìš© íŒŒì¼)")
    print("4. ğŸ¯ ì»¤ìŠ¤í…€ ì„ íƒ")
    print("5. ğŸ“Š ë³´ê³ ì„œë§Œ ë³´ê¸°")
    print("0. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒ (0-5): ").strip()
    
    if choice == "1":
        categories = [FileCategory.BACKUP, FileCategory.CACHE, 
                     FileCategory.TEMP, FileCategory.DUPLICATE]
        mode = "ì•ˆì „ ì •ë¦¬"
    elif choice == "2":
        categories = [FileCategory.BACKUP, FileCategory.CACHE, FileCategory.TEMP, 
                     FileCategory.DUPLICATE, FileCategory.LOG, FileCategory.ARCHIVE]
        mode = "ì ê·¹ ì •ë¦¬"
    elif choice == "3":
        categories = list(FileCategory)
        categories.remove(FileCategory.ESSENTIAL)
        mode = "ì „ì²´ ì •ë¦¬"
    elif choice == "4":
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:")
        for i, cat in enumerate(FileCategory):
            if cat != FileCategory.ESSENTIAL:
                file_count = len(cleanup_system.scan_results.get(cat, []))
                total_size = sum(f.size_mb for f in cleanup_system.scan_results.get(cat, [])) / 1024
                print(f"{i+1}. {cat.value} ({file_count}ê°œ íŒŒì¼, {total_size:.1f}GB)")
        
        selected = input("ì„ íƒí•  ë²ˆí˜¸ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„): ").strip()
        try:
            indices = [int(x.strip()) - 1 for x in selected.split(",")]
            all_cats = [cat for cat in FileCategory if cat != FileCategory.ESSENTIAL]
            categories = [all_cats[i] for i in indices if 0 <= i < len(all_cats)]
            mode = "ì»¤ìŠ¤í…€ ì •ë¦¬"
        except:
            print("âŒ ì˜ëª»ëœ ì…ë ¥")
            return
    elif choice == "5":
        print("\n" + report)
        return
    else:
        print("ì •ë¦¬ ì·¨ì†Œë¨")
        return
    
    if not categories:
        print("ì„ íƒëœ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
    print(f"\nğŸ§ª {mode} ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰...")
    sim_results = cleanup_system.execute_cleanup(categories, dry_run=True)
    
    print(f"\nğŸ’¾ ì˜ˆìƒ ì ˆì•½ ê³µê°„: {sim_results['freed_space_mb'] / 1024:.1f}GB")
    
    # ì‹¤ì œ ì‚­ì œ í™•ì¸
    confirm = input("\nì •ë§ë¡œ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ").strip().lower()
    
    if confirm == "yes":
        print("\nğŸš¨ ì‹¤ì œ ì •ë¦¬ ì‹¤í–‰...")
        actual_results = cleanup_system.execute_cleanup(categories, dry_run=False)
        
        print(f"\nğŸ‰ ì •ë¦¬ ì™„ë£Œ! {actual_results['freed_space_mb'] / 1024:.1f}GB í™•ë³´")
        
        # ìµœì¢… ë³´ê³ ì„œ ì €ì¥
        final_report_file = project_root / f"cleanup_completed_{int(time.time())}.txt"
        with open(final_report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            f.write(f"\n\n=== ì‹¤í–‰ ê²°ê³¼ ===\n")
            f.write(f"ì‚­ì œëœ íŒŒì¼: {actual_results['deleted_files']}ê°œ\n")
            f.write(f"í™•ë³´ëœ ê³µê°„: {actual_results['freed_space_mb'] / 1024:.1f}GB\n")
            f.write(f"ì˜¤ë¥˜: {actual_results['errors']}ê°œ\n")
        
        print(f"ğŸ“‹ ìµœì¢… ë³´ê³ ì„œ: {final_report_file}")
    else:
        print("ì •ë¦¬ ì·¨ì†Œë¨")

if __name__ == "__main__":
    main()